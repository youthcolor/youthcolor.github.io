[{"title":"R使用keras包和lime包实现图像分类并添加解释","date":"2018-08-22T03:35:42.699Z","path":"2018/08/22/R使用keras包和lime包实现图像分类并添加解释/","text":"告诉大家一个好消息，我已经使用上keras和TensorFlow了，喜欢它简单直接的建模方式。 为不同类型的水果建立图像分类器本实验的目的是为不同类型的水果建立图像分类器，再次为它建模的快速且简单而惊讶到了，大约100行代码跑了不到一个小时时间！（主代码部分已经做了简单的注释以方便阅读） 这就是我为什么要分享使用keras的原因。 代码开始如果你之前还没有安装keras，请参照RStudio’s keras site 1library(keras) 本实验的数据集来自kaggle的fruit images dataset。我已经下载并解压了这个数据集，由于我不想对每个水果都建模，因此我只是选取了其中的16个种类的水果进行了建模。 为了尽可能简单，我一开始时定义了几个参数。 123456789101112131415161718# 要建模的水果列表fruit_list &lt;- c(\"Kiwi\", \"Banana\", \"Apricot\", \"Avocado\", \"Cocos\", \"Clementine\", \"Mandarine\", \"Orange\", \"Limes\", \"Lemon\", \"Peach\", \"Plum\", \"Raspberry\", \"Strawberry\", \"Pineapple\", \"Pomegranate\")# 输出水果类别的数量output_n &lt;- length(fruit_list)# 图像大小缩小至20*20像素 (原始图像尺寸为100*100像素)img_width &lt;- 20img_height &lt;- 20target_size &lt;- c(img_width, img_height)# RGB = 3 通道channels &lt;- 3# 图片文件夹路径train_image_files_path &lt;- \"C:/Users/Administrator/Desktop/fruits-360/Training/\"valid_image_files_path &lt;- \"C:/Users/Administrator/Desktop/fruits-360/Test/\" 加载图片12345678910111213141516# 可选择的数据参数train_data_gen = image_data_generator( rescale = 1/255 #, #rotation_range = 40, #width_shift_range = 0.2, #height_shift_range = 0.2, #shear_range = 0.2, #zoom_range = 0.2, #horizontal_flip = TRUE, #fill_mode = \"nearest\")# 验证数据不应该加入这些参数，除了尺寸上的缩放valid_data_gen &lt;- image_data_generator( rescale = 1/255 ) 现在将图片载入内存并调增大小123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172# 训练集图片train_image_array_gen &lt;- flow_images_from_directory(train_image_files_path, train_data_gen, target_size = target_size, class_mode = \"categorical\", classes = fruit_list, seed = 42)# 验证集图片valid_image_array_gen &lt;- flow_images_from_directory(valid_image_files_path, valid_data_gen, target_size = target_size, class_mode = \"categorical\", classes = fruit_list, seed = 42)cat(\"Number of images per class:\") ## Number of images per class:table(factor(train_image_array_gen$classes))## 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 ## 466 490 492 427 490 490 490 479 490 492 492 447 490 492 490 492cat(\"\\nClass label vs index mapping:\\n\")## Class label vs index mapping:train_image_array_gen$class_indices## $Lemon## [1] 9## ## $Peach## [1] 10## ## $Limes## [1] 8## ## $Apricot## [1] 2## ## $Plum## [1] 11## ## $Avocado## [1] 3## ## $Strawberry## [1] 13## ## $Pineapple## [1] 14## ## $Orange## [1] 7## ## $Mandarine## [1] 6## ## $Banana## [1] 1## ## $Clementine## [1] 5## ## $Kiwi## [1] 0## ## $Cocos## [1] 4## ## $Pomegranate## [1] 15## ## $Raspberry## [1] 12fruits_classes_indices &lt;- train_image_array_gen$class_indicessave(fruits_classes_indices, file = \"C:/Users/Administrator/Desktop/fruits-360/fruits_classes_indices.RData\") 定义模型接下来，我们定义keras模型。 1234567# 训练集样本的数量train_samples &lt;- train_image_array_gen$n# 验证集样本的数量valid_samples &lt;- valid_image_array_gen$n# 定义batch大小和epochs数量batch_size &lt;- 32epochs &lt;- 10 这里我使用的模型是一个简单序列的卷及神经网络，它的隐藏层包含2个卷积层，1个池化层，一个密集层。123456789101112131415161718192021222324252627282930313233# 初始化模型model &lt;- keras_model_sequential()# 添加层model %&gt;% layer_conv_2d(filter = 32, kernel_size = c(3,3), padding = \"same\", input_shape = c(img_width, img_height, channels)) %&gt;% layer_activation(\"relu\") %&gt;% # 第二个隐藏层 layer_conv_2d(filter = 16, kernel_size = c(3,3), padding = \"same\") %&gt;% layer_activation_leaky_relu(0.5) %&gt;% layer_batch_normalization() %&gt;% # 使用最大池化 layer_max_pooling_2d(pool_size = c(2,2)) %&gt;% layer_dropout(0.25) %&gt;% # 扁平最大化过滤输出特征向量并喂给密集层 layer_flatten() %&gt;% layer_dense(100) %&gt;% layer_activation(\"relu\") %&gt;% layer_dropout(0.5) %&gt;% # 再喂给输出层 layer_dense(output_n) %&gt;% layer_activation(\"softmax\")# 编译model %&gt;% compile( loss = \"categorical_crossentropy\", optimizer = optimizer_rmsprop(lr = 0.0001, decay = 1e-6), metrics = \"accuracy\") 拟合模型前面我已经使用了image_data_generator()和flow_images_from_directory()这两个函数，在此，我将使用fit_generator()函数跑训练集。 12345678910111213141516171819202122# 拟合hist &lt;- model %&gt;% fit_generator( # 训练数据 train_image_array_gen, # epochs steps_per_epoch = as.integer(train_samples / batch_size), epochs = epochs, # 验证数据 validation_data = valid_image_array_gen, validation_steps = as.integer(valid_samples / batch_size), # 打印执行进度 verbose = 2, callbacks = list( # 在每一个epoch之后保存最好的模型 callback_model_checkpoint(\"C:/Users/Administrator/Desktop/fruits-360/keras/fruits_checkpoints.h5\", save_best_only = TRUE), # 在TensorBoard中可视化需要 callback_tensorboard(log_dir = \"C:/Users/Administrator/Desktop/fruits-360/keras/logs\") )) 在RStudio的视图面板中我们能看到会输出一个交互式图形。 也可以用下面的命令绘制它： 1plot(hist) 我们可以看到，此模型在验证数据集上相当准确。但是，我们需要清醒认识到我们的图片都很正规，它们都有白色的背景，水果都在图片中央，而且没有其他的东西在图片上。所以，我们的模型对看似不一样的图片将几乎不起作用（这就是为什么我们能在如此小的神经网络实现如此高准确率的原因）。 最后，我想看一下在TensorBoard呈现的TensorFlow图。 1tensorboard(\"C:/Users/Administrator/Desktop/fruits-360/keras/logs\") 疑惑自己的台式机（Windows 7操作系统）是主频3.4GHz的四核八线程处理器，笔记本（Deepin操作系统）是主频2.6GHz的双核四线程处理器，两者都没有使用GPU加速，结果很费解的是笔记本模型拟合时要比台式机快太多！每个epoch时间：300+ s 对比 10+ s！难道是linux比Windows要快？？？速度对比如下图所示。 Windows 7 Deepin 添加解释上面做的不仅仅是怎样使用模型预测图片分类，其实单独实现预测是很无聊的，下面我将使用lime包为预测添加解释。 加载包和模型1234library(keras) # 调用神经网络library(lime) # 解释解释模型library(magick) # 预处理图片library(ggplot2) # 作图 加载预训练好的ImageNet模型。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293model &lt;- application_vgg16(weights = \"imagenet\", include_top = TRUE)model## Model______________________________________________________________________________________________________________Layer (type) Output Shape Param # ==============================================================================================================input_1 (InputLayer) (None, 224, 224, 3) 0 ______________________________________________________________________________________________________________block1_conv1 (Conv2D) (None, 224, 224, 64) 1792 ______________________________________________________________________________________________________________block1_conv2 (Conv2D) (None, 224, 224, 64) 36928 ______________________________________________________________________________________________________________block1_pool (MaxPooling2D) (None, 112, 112, 64) 0 ______________________________________________________________________________________________________________block2_conv1 (Conv2D) (None, 112, 112, 128) 73856 ______________________________________________________________________________________________________________block2_conv2 (Conv2D) (None, 112, 112, 128) 147584 ______________________________________________________________________________________________________________block2_pool (MaxPooling2D) (None, 56, 56, 128) 0 ______________________________________________________________________________________________________________block3_conv1 (Conv2D) (None, 56, 56, 256) 295168 ______________________________________________________________________________________________________________block3_conv2 (Conv2D) (None, 56, 56, 256) 590080 ______________________________________________________________________________________________________________block3_conv3 (Conv2D) (None, 56, 56, 256) 590080 ______________________________________________________________________________________________________________block3_pool (MaxPooling2D) (None, 28, 28, 256) 0 ______________________________________________________________________________________________________________block4_conv1 (Conv2D) (None, 28, 28, 512) 1180160 ______________________________________________________________________________________________________________block4_conv2 (Conv2D) (None, 28, 28, 512) 2359808 ______________________________________________________________________________________________________________block4_conv3 (Conv2D) (None, 28, 28, 512) 2359808 ______________________________________________________________________________________________________________block4_pool (MaxPooling2D) (None, 14, 14, 512) 0 ______________________________________________________________________________________________________________block5_conv1 (Conv2D) (None, 14, 14, 512) 2359808 ______________________________________________________________________________________________________________block5_conv2 (Conv2D) (None, 14, 14, 512) 2359808 ______________________________________________________________________________________________________________block5_conv3 (Conv2D) (None, 14, 14, 512) 2359808 ______________________________________________________________________________________________________________block5_pool (MaxPooling2D) (None, 7, 7, 512) 0 ______________________________________________________________________________________________________________flatten (Flatten) (None, 25088) 0 ______________________________________________________________________________________________________________fc1 (Dense) (None, 4096) 102764544 ______________________________________________________________________________________________________________fc2 (Dense) (None, 4096) 16781312 ______________________________________________________________________________________________________________predictions (Dense) (None, 1000) 4097000 ==============================================================================================================Total params: 138,357,544Trainable params: 138,357,544Non-trainable params: 0______________________________________________________________________________________________________________#加载自己的模型model2 &lt;- load_model_hdf5(filepath = \"/home/feng/Downloads/fruits-360/keras/fruits_checkpoints.h5\")model2## Model______________________________________________________________________________________________________________Layer (type) Output Shape Param # ==============================================================================================================conv2d_1 (Conv2D) (None, 20, 20, 32) 896 ______________________________________________________________________________________________________________activation_1 (Activation) (None, 20, 20, 32) 0 ______________________________________________________________________________________________________________conv2d_2 (Conv2D) (None, 20, 20, 16) 4624 ______________________________________________________________________________________________________________leaky_re_lu_1 (LeakyReLU) (None, 20, 20, 16) 0 ______________________________________________________________________________________________________________batch_normalization_1 (BatchNormalization) (None, 20, 20, 16) 64 ______________________________________________________________________________________________________________max_pooling2d_1 (MaxPooling2D) (None, 10, 10, 16) 0 ______________________________________________________________________________________________________________dropout_1 (Dropout) (None, 10, 10, 16) 0 ______________________________________________________________________________________________________________flatten_1 (Flatten) (None, 1600) 0 ______________________________________________________________________________________________________________dense_1 (Dense) (None, 100) 160100 ______________________________________________________________________________________________________________activation_2 (Activation) (None, 100) 0 ______________________________________________________________________________________________________________dropout_2 (Dropout) (None, 100) 0 ______________________________________________________________________________________________________________dense_2 (Dense) (None, 16) 1616 ______________________________________________________________________________________________________________activation_3 (Activation) (None, 16) 0 ==============================================================================================================Total params: 167,300Trainable params: 167,268Non-trainable params: 32______________________________________________________________________________________________________________ 加载准备图片 这里，我将加载并预处理两张水果图片。 香蕉 12345test_image_files_path &lt;- \"C:/Users/Administrator/Desktop/fruits-360/Test\"img &lt;- image_read('https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1532608344405&amp;di=1e838eec20a80076760818110dd2e697&amp;imgtype=0&amp;src=http%3A%2F%2Fwww.lp-gj.com%2Fupload%2Fimage%2F20171206%2F20171206094968436843.jpg')img_path &lt;- file.path(test_image_files_path, \"Banana\", 'banana.jpg')image_write(img, img_path)#plot(as.raster(img)) 橘子 1234img2 &lt;- image_read('https://timgsa.baidu.com/timg?image&amp;quality=80&amp;size=b9999_10000&amp;sec=1532615118205&amp;di=59abc0a58e9b9a724e3b82b12956a302&amp;imgtype=0&amp;src=http%3A%2F%2Fimgsrc.baidu.com%2Fimgad%2Fpic%2Fitem%2Ffc1f4134970a304ef95645d9dbc8a786c9175c99.jpg')img_path2 &lt;- file.path(test_image_files_path, \"Clementine\", 'clementine.jpg')image_write(img2, img_path2)#plot(as.raster(img2)) Superpixels 将图像分割成超像素是生成图像模型解释的重要一步。分割是正确的，并且在图中遵循有意义的模式，而且超级像素的大小/数量是适当的，这两者都很重要。如果图像中的重要特征被切成太多的片段，那么在几乎所有情况下，这些排列可能会破坏图像，从而导致一个糟糕的或失败的解释模型。当感兴趣的对象的大小发生变化时，不可能为超像素的数量设置硬性规则——对象相对于图像的大小越大，生成的超级像素就越少。使用plot_superpixels，在开始使用解释函数之前，可以对超像素参数进行评估。 1plot_superpixels(img_path, n_superpixels = 35, weight = 10) 1plot_superpixels(img_path2, n_superpixels = 50, weight = 20) 从超像素的图中我们可以看到，橘子的图像比香蕉图像的分辨率更高。 为Imagenet准备图片123456789image_prep &lt;- function(x) &#123; arrays &lt;- lapply(x, function(path) &#123; img &lt;- image_load(path, target_size = c(224,224)) x &lt;- image_to_array(img) x &lt;- array_reshape(x, c(1, dim(x))) x &lt;- imagenet_preprocess_input(x) &#125;) do.call(abind::abind, c(arrays, list(along = 1)))&#125; 测试预测结果 1234567891011121314151617res &lt;- predict(model, image_prep(c(img_path, img_path2)))imagenet_decode_predictions(res)[[1]] class_name class_description score1 n07753592 banana 0.9378491042 n03786901 mortar 0.0142855103 n03532672 hook 0.0077418724 n04579432 whistle 0.0042158405 n07747607 orange 0.002990912[[2]] class_name class_description score1 n07747607 orange 0.653720972 n07749582 lemon 0.070178343 n07717556 butternut_squash 0.059203894 n07720875 bell_pepper 0.030308835 n03937543 pill_bottle 0.02684177 加载标签和训练解释 12model_labels &lt;- readRDS(system.file('extdata', 'imagenet_labels.rds', package = 'lime'))explainer &lt;- lime(c(img_path, img_path2), as_classifier(model, model_labels), image_prep) 训练这个解释器需要相当长的时间，我自己模型中的较小图片比较大规模的Imagenet训练的时间要快很多。 1234explanation &lt;- explain(c(img_path, img_path2), explainer, n_labels = 2, n_features = 35, n_superpixels = 35, weight = 10, background = \"white\") plot_image_explanation()函数一次只支持一个实例 1plot_image_explanation(explanation) 12clementine &lt;- explanation[explanation$case == \"clementine.jpg\",]plot_image_explanation(clementine) 为我自己的模型准备图片测试预测结果(类似于训练和验证图片) 1234567891011121314151617181920212223242526272829303132333435363738394041test_datagen &lt;- image_data_generator(rescale = 1/255)test_generator &lt;- flow_images_from_directory( test_image_files_path, test_datagen, target_size = c(20, 20), class_mode = 'categorical')predictions &lt;- as.data.frame(predict_generator(model2, test_generator, steps = 1))load(\"/Users/shiringlander/Documents/Github/DL_AI/Tutti_Frutti/fruits-360/fruits_classes_indices.RData\")fruits_classes_indices_df &lt;- data.frame(indices = unlist(fruits_classes_indices))fruits_classes_indices_df &lt;- fruits_classes_indices_df[order(fruits_classes_indices_df$indices), , drop = FALSE]colnames(predictions) &lt;- rownames(fruits_classes_indices_df)t(round(predictions, digits = 2)) [,1] [,2]Kiwi 0.00 0Banana 0.06 1Apricot 0.02 0Avocado 0.00 0Cocos 0.00 0Clementine 0.86 0Mandarine 0.03 0Orange 0.00 0Limes 0.00 0Lemon 0.02 0Peach 0.00 0Plum 0.00 0Raspberry 0.00 0Strawberry 0.01 0Pineapple 0.00 0Pomegranate 0.00 0for (i in 1:nrow(predictions)) &#123; cat(i, \":\") print(unlist(which.max(predictions[i, ])))&#125;1 :Clementine 6 2 :Banana 2 尽管这似乎和lime不太符合,所以我准备了类似于Imagenet的图片。 123456789image_prep2 &lt;- function(x) &#123; arrays &lt;- lapply(x, function(path) &#123; img &lt;- image_load(path, target_size = c(20, 20)) x &lt;- image_to_array(img) x &lt;- reticulate::array_reshape(x, c(1, dim(x))) x &lt;- x / 255 &#125;) do.call(abind::abind, c(arrays, list(along = 1)))&#125; 准备标签 123456789fruits_classes_indices_l &lt;- rownames(fruits_classes_indices_df)names(fruits_classes_indices_l) &lt;- unlist(fruits_classes_indices)fruits_classes_indices_l 0 1 2 3 4 5 \"Kiwi\" \"Banana\" \"Apricot\" \"Avocado\" \"Cocos\" \"Clementine\" 6 7 8 9 10 11 \"Mandarine\" \"Orange\" \"Limes\" \"Lemon\" \"Peach\" \"Plum\" 12 13 14 15 \"Raspberry\" \"Strawberry\" \"Pineapple\" \"Pomegranate\" 训练解释器 12345explainer2 &lt;- lime(c(img_path, img_path2), as_classifier(model2, fruits_classes_indices_l), image_prep2)explanation2 &lt;- explain(c(img_path, img_path2), explainer2, n_labels = 1, n_features = 20, n_superpixels = 35, weight = 10, background = \"white\") 绘制特征权重图以寻找绘图的最佳的块阀值 (见下图) 1234explanation2 %&gt;% ggplot(aes(x = feature_weight)) + facet_wrap(~ case, scales = \"free\") + geom_density() 绘制预测 1plot_image_explanation(explanation2, display = 'block', threshold = 1e-09) 12clementine2 &lt;- explanation2[explanation2$case == \"clementine.jpg\",]plot_image_explanation(clementine2, display = 'block', threshold = 0.01) 123456789101112131415161718192021222324252627282930313233sessionInfo()R version 3.5.0 (2018-04-23)Platform: x86_64-w64-mingw32/x64 (64-bit)Running under: Windows 7 x64 (build 7601) Service Pack 1Matrix products: defaultlocale:[1] LC_COLLATE=Chinese (Simplified)_People's Republic of China.936 [2] LC_CTYPE=Chinese (Simplified)_People's Republic of China.936 [3] LC_MONETARY=Chinese (Simplified)_People's Republic of China.936[4] LC_NUMERIC=C [5] LC_TIME=Chinese (Simplified)_People's Republic of China.936 attached base packages:[1] stats graphics grDevices utils datasets methods base other attached packages:[1] ggplot2_2.2.1 magick_1.9 lime_0.4.0 keras_2.1.6 loaded via a namespace (and not attached): [1] Rcpp_0.12.16 pillar_1.2.2 compiler_3.5.0 later_0.7.3 [5] gower_0.1.2 plyr_1.8.4 base64enc_0.1-3 iterators_1.0.9 [9] tools_3.5.0 zeallot_0.1.0 digest_0.6.15 jsonlite_1.5 [13] tibble_1.4.2 gtable_0.2.0 lattice_0.20-35 rlang_0.2.0 [17] Matrix_1.2-14 foreach_1.4.4 shiny_1.1.0 curl_3.2 [21] parallel_3.5.0 knitr_1.20 htmlwidgets_1.2 grid_3.5.0 [25] glmnet_2.0-16 reticulate_1.9 R6_2.2.2 magrittr_1.5 [29] whisker_0.3-2 shinythemes_1.1.1 promises_1.0.1 scales_0.5.0 [33] codetools_0.2-15 tfruns_1.3 htmltools_0.3.6 abind_1.4-5 [37] stringdist_0.9.4.7 assertthat_0.2.0 xtable_1.8-2 mime_0.5 [41] colorspace_1.3-2 httpuv_1.4.5 labeling_0.3 tensorflow_1.8 [45] stringi_1.1.7 lazyeval_0.2.1 munsell_0.4.3","tags":[{"name":"keras","slug":"keras","permalink":"http://weibo.com/youthcolor/tags/keras/"},{"name":"图像分类","slug":"图像分类","permalink":"http://weibo.com/youthcolor/tags/图像分类/"},{"name":"lime","slug":"lime","permalink":"http://weibo.com/youthcolor/tags/lime/"},{"name":"tensorflow","slug":"tensorflow","permalink":"http://weibo.com/youthcolor/tags/tensorflow/"}]},{"title":"使用rvest抓取NBA历史比赛数据","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/使用rvest抓取NBA历史比赛数据/","text":"我们生活在互联网大数据时代，网上遍布各种数据，做数据分析必备的一步就是要能获取想要得到的数据，如果没有数据，做数据分析就如同“巧妇难为无米之炊”。网络爬虫技术为获取数据提供了途径，R提供的rvest包，为我们抓取网络数据提供了便利。 下面，以爬取NBA历史比赛数据为例，使用rvest进行简单的爬虫实践。NBA中文网站 http://www.stat-nba.com/gameList_simple.html 提供了十分全面的NBA历史比赛数据。 进入比赛数据详情页面，可以看到各种比赛数据记录非常详细。 爬虫的关键，是匹配数据所属html标签。那么，怎样才能比较容易获取到每个数据的html标签呢？虽然利用Chrome浏览器按F12键也分析出元素标签，但是，本人比价喜欢使用Pale Moon浏览器（火狐进化版本），因为用它获取元素的独一无二的识别标签十分方便，无疑就提高了工作效率。 选中要分析的元素后，右键下面的该元素标签，即可复制它的独特识别标签，如“div.text:nth-child(1) &gt; div:nth-child(1)”。 下面就有利于我们进一步的爬虫工作了，R代码如下。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234# install.packages('rvest')library('rvest')# 函数封装，返回dataframegetNBADataFrame &lt;- function(pageBegin,pageEnd) &#123; result &lt;- data.frame() for (index in pageBegin:pageEnd) &#123; # url url &lt;- paste(\"http://www.stat-nba.com/game/\",index,\".html\",sep=\"\",collapse=\"\") webpage &lt;- read_html(url,'uft-8') bodytext &lt;- webpage %&gt;% html_node('body') %&gt;% html_text # 判断该页面数据是否存在 if(!length(grep(\"from coach where id\", bodytext, value = FALSE)))&#123; # 出场 chuchang &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(2)') %&gt;% html_text %&gt;% sub(pattern = \"浜?\",replacement = \"\") %&gt;% as.numeric # 投篮 toulan &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(3)') %&gt;% html_text %&gt;% sub(pattern = \"%\",replacement = \"\") %&gt;% as.numeric # 投篮命中 mingzhong_t &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(4)') %&gt;% html_text %&gt;% as.numeric # 投篮出手 chushou_t &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(5)') %&gt;% html_text %&gt;% as.numeric # 三分 sanfen &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(6)') %&gt;% html_text %&gt;% sub(pattern = \"%\",replacement = \"\") %&gt;% as.numeric # 三分命中 mingzhong_s &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(7)') %&gt;% html_text %&gt;% as.numeric # 三分出手 chushou_s &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(8)') %&gt;% html_text %&gt;% as.numeric # 罚球 faqiu &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(9)') %&gt;% html_text %&gt;% sub(pattern = \"%\",replacement = \"\") %&gt;% as.numeric # 罚球命中 mingzhong_f &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(10)') %&gt;% html_text %&gt;% as.numeric # 罚球出手 chushou_f &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(11)') %&gt;% html_text %&gt;% as.numeric # 真实命中 mingzhong_z &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(12)') %&gt;% html_text %&gt;% sub(pattern = \"%\",replacement = \"\") %&gt;% as.numeric # 篮板 lanban &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(13)') %&gt;% html_text %&gt;% as.numeric # 前场 qianchang &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(14)') %&gt;% html_text %&gt;% as.numeric # 后场 houchang &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(15)') %&gt;% html_text %&gt;% as.numeric # 助攻 zhugong &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(16)') %&gt;% html_text %&gt;% as.numeric # 抢断 qiangduan &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(17)') %&gt;% html_text %&gt;% as.numeric # 盖帽 gaimao &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(18)') %&gt;% html_text %&gt;% as.numeric # 失误 shiwu &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(19)') %&gt;% html_text %&gt;% as.numeric # 犯规 fangui &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(20)') %&gt;% html_text %&gt;% as.numeric # 得分 defen &lt;- webpage %&gt;% html_nodes('tr.team_all_content &gt; td:nth-child(21)') %&gt;% html_text %&gt;% as.numeric date &lt;- webpage %&gt;% html_node('#background &gt; div:nth-child(7) &gt; div:nth-child(2)') %&gt;% html_text %&gt;% as.Date # 转换日期为北京时间 date &lt;- date + 1 title &lt;- webpage %&gt;% html_node('#background &gt; div:nth-child(7) &gt; div:nth-child(1)') %&gt;% html_text %&gt;% strsplit(\"\\n\") # 赛季 year &lt;- title[[1]][2] # 季前赛or常规赛or季后赛 type &lt;- title[[1]][3] # 客队 team_a &lt;- webpage %&gt;% html_node('div.team:nth-child(1) &gt; div:nth-child(1) &gt; div:nth-child(2) &gt; a:nth-child(1)') %&gt;% html_text # 主队 team_h &lt;- webpage %&gt;% html_node('div.team:nth-child(3) &gt; div:nth-child(1) &gt; div:nth-child(2) &gt; a:nth-child(1)') %&gt;% html_text # 客队第一节得分 defen_1_a &lt;- webpage %&gt;% html_node('div.table:nth-child(2) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(1) &gt; td:nth-child(2)') %&gt;% html_text %&gt;% as.numeric defen_2_a &lt;- webpage %&gt;% html_node('div.table:nth-child(2) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(2) &gt; td:nth-child(2)') %&gt;% html_text %&gt;% as.numeric defen_3_a &lt;- webpage %&gt;% html_node('div.table:nth-child(2) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(3) &gt; td:nth-child(2)') %&gt;% html_text %&gt;% as.numeric defen_4_a &lt;- webpage %&gt;% html_node('div.table:nth-child(2) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(4) &gt; td:nth-child(2)') %&gt;% html_text %&gt;% as.numeric # 主队第一节得分 defen_1_h &lt;- webpage %&gt;% html_node('div.table:nth-child(3) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(1) &gt; td:nth-child(1)') %&gt;% html_text %&gt;% as.numeric defen_2_h &lt;- webpage %&gt;% html_node('div.table:nth-child(3) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(2) &gt; td:nth-child(1)') %&gt;% html_text %&gt;% as.numeric defen_3_h &lt;- webpage %&gt;% html_node('div.table:nth-child(3) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(3) &gt; td:nth-child(1)') %&gt;% html_text %&gt;% as.numeric defen_4_h &lt;- webpage %&gt;% html_node('div.table:nth-child(3) &gt; table:nth-child(1) &gt; tbody:nth-child(1) &gt; tr:nth-child(4) &gt; td:nth-child(1)') %&gt;% html_text %&gt;% as.numeric shengfu &lt;- webpage %&gt;% html_node('div.team:nth-child(1) &gt; div:nth-child(1) &gt; div:nth-child(2) &gt; a:nth-child(3)') %&gt;% html_text %&gt;% strsplit(\" \") shengfu &lt;- shengfu[[1]][2] %&gt;% strsplit(\"胜\") # 客队胜场 sheng_a &lt;- shengfu[[1]][1] %&gt;% as.numeric shengfu &lt;- shengfu[[1]][2] %&gt;% strsplit(\"负\") # 客队负场 fu_a &lt;- shengfu[[1]][1] %&gt;% as.numeric shengfu &lt;- webpage %&gt;% html_node('div.team:nth-child(3) &gt; div:nth-child(1) &gt; div:nth-child(2) &gt; a:nth-child(3)') %&gt;% html_text %&gt;% strsplit(\" \") shengfu &lt;- shengfu[[1]][2] %&gt;% strsplit(\"胜\") # 主队胜场 sheng_h &lt;- shengfu[[1]][1] %&gt;% as.numeric shengfu &lt;- shengfu[[1]][2] %&gt;% strsplit(\"负\") # 主队负场 fu_h &lt;- shengfu[[1]][1] %&gt;% as.numeric # 组合成dataframe格式 datanew &lt;- data.frame(chuchang_a = chuchang[1], toulan_a = toulan[1], mingzhong_t_a = mingzhong_t[1], chushou_t_a = chushou_t[1], sanfen_a = sanfen[1], mingzhong_s_a = mingzhong_s[1], chushou_s_a = chushou_s[1], faqiu_a = faqiu[1], mingzhong_f_a = mingzhong_f[1], chushou_f_a = chushou_f[1], mingzhong_z_a = mingzhong_z[1], lanban_a = lanban[1], qianchang_a = qianchang[1], houchang_a = houchang[1], zhugong_a = zhugong[1], qiangduan_a = qiangduan[1], gaimao_a = qiangduan[1], shiwu_a = shiwu[1], fangui_a = fangui[1], defen_a = defen[1], chuchang_h = chuchang[2], toulan_h = toulan[2], mingzhong_t_h = mingzhong_t[2], chushou_t_h = chushou_t[2], sanfen_h = sanfen[2], mingzhong_s_h = mingzhong_s[2], chushou_s_h = chushou_s[2], faqiu_h = faqiu[2], mingzhong_f_h = mingzhong_f[2], chushou_f_h = chushou_f[2], mingzhong_z_h = mingzhong_z[2], lanban_h = lanban[2], qianchang_h = qianchang[2], houchang_h = houchang[2], zhugong_h = zhugong[2], qiangduan_h = qiangduan[2], gaimao_h = qiangduan[2], shiwu_h = shiwu[2], fangui_h = fangui[2], defen_h = defen[2], date = date, year = year, type = type, team_a = team_a, team_h = team_h, defen_1_a = defen_1_a, defen_2_a = defen_2_a, defen_3_a = defen_3_a, defen_4_a = defen_4_a, defen_1_h = defen_1_h, defen_2_h = defen_2_h, defen_3_h = defen_3_h, defen_4_h = defen_4_h, sheng_a = sheng_a, fu_a = fu_a, sheng_h = sheng_h, fu_h = fu_h, page = index ) result &lt;- rbind(result, datanew) print(index) print(dim(result)) &#125; &#125; return(result)&#125;# 保存写入文件（2010年至今所有比赛数据）data_nba &lt;- getNBADataFrame(27035,40711)save(data_nba,file=\"C:/Users/Administrator/Desktop/NBA_Data_Scraping/data_nba.rData\")write.csv(data_nba,file=\"C:/Users/Administrator/Desktop/NBA_Data_Scraping/data_nba.csv\", row.names = F,quote = F) 运行代码，获取到的数据格式如下","tags":[{"name":"网络爬虫","slug":"网络爬虫","permalink":"http://weibo.com/youthcolor/tags/网络爬虫/"}]},{"title":"R机器学习之股票预测初探","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/R机器学习之股票预测初探/","text":"如今机器学习在工业界和学术界都愈发火热，智能化的崭新时代正在向我们走来。在金融领域，是否也能应用机器学习的方法做股票涨跌的预测呢？下面就以R语言的相关机器学习包为例，简单做股票预测方面的初探。 数据准备昨天，我已经从同花顺中导出了“分众传媒”（股票代码：002027）的历史交易数据，该数据集包含了“时间,开盘,最高,最低,收盘,涨幅,振幅,总手,金额,换手%,成交次数”等维度的数据。 12345678910111213141516171819202122# 载入包library(quantmod);library(TTR);library(randomForest); library(caret);library(corrplot);library(pROC);library(FSelector)XYZQ &lt;- read.csv(\"002027.csv\", header = T, stringsAsFactors = F)names(XYZQ) &lt;- c(\"Index\", \"Open\", \"High\", \"Low\", \"Close\", \"zhangfu\", \"zhenfu\", \"Volume\", \"jine\", \"huanshou\", \"chengjiaocishu\")XYZQ &lt;- XYZQ[XYZQ$Volume != 0, ]tail(XYZQ) Index Open High Low Close zhangfu zhenfu Volume2979 2017-06-15,四 102.94 104.48 101.04 101.53 -1.77% 3.33% 34,873,4182980 2017-06-16,五 101.53 103.15 100.27 102.73 1.18% 2.83% 31,332,3362981 2017-06-19,一 92.55 97.11 92.55 92.76 -9.71% 4.44% 162,266,2502982 2017-06-20,二 93.11 95.57 93.11 94.23 1.58% 2.65% 44,149,7182983 2017-06-21,三 94.30 96.41 91.71 94.58 0.37% 4.99% 41,641,7572984 2017-06-22,四 94.09 97.95 93.88 96.48 2.01% 4.30% 23,065,242 jine huanshou chengjiaocishu2979 503,424,470 0.861 230562980 448,854,630 0.774 211372981 2,141,561,400 4.010 581422982 586,119,870 1.090 269752983 550,581,210 1.030 249892984 313,326,010 0.570 13919 数据预处理下面需要对导出的数据进行处理，使他符合自己期望的格式要求。1234567891011121314151617for(i in 1:nrow(XYZQ))&#123; # XYZQ[i,\"week\"] &lt;- strsplit(as.character(XYZQ[i,\"Index\"]), split = \",\")[[1]][2] # 提取日期 XYZQ[i,1] &lt;- strsplit(as.character(XYZQ[i,1]), split = \",\")[[1]][1] # 去除%号 XYZQ[i,\"zhangfu\"] &lt;- strsplit(as.character(XYZQ[i,\"zhangfu\"]), split = \"%\")[[1]][1] XYZQ[i,\"zhenfu\"] &lt;- strsplit(as.character(XYZQ[i,\"zhenfu\"]), split = \"%\")[[1]][1]&#125;# 类型转换XYZQ[,1] &lt;- as.Date(XYZQ[,1])# XYZQ$week &lt;- as.factor(XYZQ$week)XYZQ$zhangfu &lt;- as.numeric(XYZQ$zhangfu)XYZQ$zhenfu &lt;- as.numeric(XYZQ$zhenfu)XYZQ$Volume &lt;- as.numeric(gsub(\",\",\"\", XYZQ$Volume))XYZQ$jine &lt;- as.numeric(gsub(\",\",\"\", XYZQ$jine))# 时间序列xts类型转换XYZQ &lt;- xts(XYZQ[,-1], order.by = XYZQ[, 1]) 特征提取在R中提供了许多金融指标的TTR包，使用它可以很方便的提取出各种金融指标。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151# 换手率huanshou &lt;- XYZQ$huanshouhuanshou &lt;- as.data.frame(huanshou)$huanshouhuanshou &lt;- c(NA, huanshou)# 振幅zhenfu &lt;- XYZQ$zhenfuzhenfu &lt;- as.data.frame(zhenfu)$zhenfuzhenfu &lt;- c(NA, zhenfu)# 第二日涨跌信号Close &lt;- ifelse(XYZQ$zhangfu &gt; 0, 1, 0)Close &lt;- as.data.frame(Close)$zhangfuClose &lt;- c(Close,NA)# TTR指标myATR &lt;- ATR(HLC(XYZQ))[,'atr'] ; myATR &lt;- c(NA,myATR) ;mySMI &lt;- SMI(HLC(XYZQ))[,'SMI'] ; mySMI &lt;- c(NA,mySMI) ;myADX &lt;- ADX(HLC(XYZQ))[,'ADX'] ; myADX &lt;- c(NA,myADX) ;myAroon &lt;- aroon(XYZQ[,c('High','Low')])$oscillator ; myAroon &lt;- c(NA,myAroon) ;myBB &lt;- BBands(HLC(XYZQ))[,'pctB'] ; myBB &lt;- c(NA,myBB) ;myChaikinVol &lt;- Delt(chaikinVolatility(XYZQ[,c(\"High\",\"Low\")]))[,1] ; myChaikinVol &lt;- c(NA,myChaikinVol) ;myCLV &lt;- EMA(CLV(HLC(XYZQ)))[,1] ; myCLV &lt;- c(NA,myCLV) ;myEMV &lt;- EMV(XYZQ[,c('High','Low')],XYZQ[,'Volume'])[,2] ; myEMV &lt;- c(NA,myEMV) ;myMACD &lt;- MACD(Cl(XYZQ))[,2] ; myMACD &lt;- c(NA,myMACD) ;myMFI &lt;- MFI(XYZQ[,c(\"High\",\"Low\",\"Close\")], XYZQ[,\"Volume\"]) ; myMFI &lt;- c(NA,myMFI) ;mySAR &lt;- SAR(XYZQ[,c('High','Close')]) [,1] ; mySAR &lt;- c(NA,mySAR) ;myVolat &lt;- volatility(OHLC(XYZQ),calc=\"garman\")[,1] ; myVolat &lt;- c(NA,myVolat) ;myCMO &lt;- CMO(Cl(XYZQ)) ; myCMO &lt;- c(NA,myCMO) ;myEMA &lt;- EMA(Delt(Cl(XYZQ))) ; myEMA &lt;- c(NA,myEMA) ;forceindex &lt;- (XYZQ$Close - XYZQ$Open) * XYZQ$Volume ; forceindex &lt;- c(NA,forceindex) ;WillR5 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 5) ; WillR5 &lt;- c(NA,WillR5) ;WillR10 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 10) ; WillR10 &lt;- c(NA,WillR10) ;WillR15 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 15) ; WillR15 &lt;- c(NA,WillR15) ;WillR30 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 30) ; WillR30 &lt;- c(NA,WillR30) ;WillR45 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 45) ; WillR45 &lt;- c(NA,WillR45) ;WillR60 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 60) ; WillR60 &lt;- c(NA,WillR60) ;RSI5 &lt;- RSI(XYZQ$Close, n = 5,maType=\"WMA\") ;RSI5 &lt;- c(NA,RSI5) ;RSI10 &lt;- RSI(XYZQ$Close, n = 10,maType=\"WMA\") ;RSI10 &lt;- c(NA,RSI10) ;RSI15 &lt;- RSI(XYZQ$Close, n = 15,maType=\"WMA\") ;RSI15 &lt;- c(NA,RSI15) ;RSI30 &lt;- RSI(XYZQ$Close, n = 30,maType=\"WMA\") ;RSI30 &lt;- c(NA,RSI30) ;RSI45 &lt;- RSI(XYZQ$Close, n = 45,maType=\"WMA\") ;RSI45 &lt;- c(NA,RSI45) ;RSI60 &lt;- RSI(XYZQ$Close, n = 60,maType=\"WMA\") ;RSI60 &lt;- c(NA,RSI60) ;ROC5 &lt;- ROC(XYZQ$Close, n = 5,type =\"discrete\")*100 ; ROC5 &lt;- c(NA,ROC5) ;ROC10 &lt;- ROC(XYZQ$Close, n = 10,type =\"discrete\")*100 ; ROC10 &lt;- c(NA,ROC10) ;ROC15 &lt;- ROC(XYZQ$Close, n = 15,type =\"discrete\")*100 ; ROC15 &lt;- c(NA,ROC15) ;ROC30 &lt;- ROC(XYZQ$Close, n = 30,type =\"discrete\")*100 ; ROC30 &lt;- c(NA,ROC30) ;ROC45 &lt;- ROC(XYZQ$Close, n = 45,type =\"discrete\")*100 ; ROC45 &lt;- c(NA,ROC45) ;ROC60 &lt;- ROC(XYZQ$Close, n = 60,type =\"discrete\")*100 ; ROC60 &lt;- c(NA,ROC60) ;MOM5 &lt;- momentum(XYZQ$Close, n = 5, na.pad = TRUE) ; MOM5 &lt;- c(NA,MOM5) ;MOM10 &lt;- momentum(XYZQ$Close, n = 10, na.pad = TRUE) ; MOM10 &lt;- c(NA,MOM10) ;MOM15 &lt;- momentum(XYZQ$Close, n = 15, na.pad = TRUE) ; MOM15 &lt;- c(NA,MOM15) ;MOM30 &lt;- momentum(XYZQ$Close, n = 30, na.pad = TRUE) ; MOM30 &lt;- c(NA,MOM30) ;MOM45 &lt;- momentum(XYZQ$Close, n = 45, na.pad = TRUE) ; MOM45 &lt;- c(NA,MOM45) ;MOM60 &lt;- momentum(XYZQ$Close, n = 60, na.pad = TRUE) ; MOM60 &lt;- c(NA,MOM60) ;ATR5 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 5, maType=\"WMA\")[,1] ; ATR5 &lt;- c(NA,ATR5) ;ATR10 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 10, maType=\"WMA\")[,1]; ATR10 &lt;- c(NA,ATR10) ;ATR15 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 15, maType=\"WMA\")[,1]; ATR15 &lt;- c(NA,ATR15) ;ATR30 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 30, maType=\"WMA\")[,1] ; ATR30 &lt;- c(NA,ATR30) ;ATR45 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 45, maType=\"WMA\")[,1]; ATR45 &lt;- c(NA,ATR45) ;ATR60 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 60, maType=\"WMA\")[,1]; ATR60 &lt;- c(NA,ATR60) ;Mean5 &lt;- runMean(Cl(XYZQ), n = 5); Mean5 &lt;- c(NA,Mean5) ;Mean10 &lt;- runMean(Cl(XYZQ), n = 10); Mean10 &lt;- c(NA,Mean10) ;Mean15 &lt;- runMean(Cl(XYZQ), n = 15); Mean15 &lt;- c(NA,Mean15) ;Mean30 &lt;- runMean(Cl(XYZQ), n = 30); Mean30 &lt;- c(NA,Mean30) ;Mean45 &lt;- runMean(Cl(XYZQ), n = 45); Mean45 &lt;- c(NA,Mean45) ;Mean60 &lt;- runMean(Cl(XYZQ), n = 60); Mean60 &lt;- c(NA,Mean60) ;SD5 &lt;- runSD(Cl(XYZQ), n = 5); SD5 &lt;- c(NA,SD5) ;SD10 &lt;- runSD(Cl(XYZQ), n = 10); SD10 &lt;- c(NA,SD10) ;SD15 &lt;- runSD(Cl(XYZQ), n = 15); SD15 &lt;- c(NA,SD15) ;SD30 &lt;- runSD(Cl(XYZQ), n = 30); SD30 &lt;- c(NA,SD30) ;SD45 &lt;- runSD(Cl(XYZQ), n = 45); SD45 &lt;- c(NA,SD45) ;SD60 &lt;- runSD(Cl(XYZQ), n = 60); SD60 &lt;- c(NA,SD60) ;# 创建特征数据集mydataset &lt;- data.frame(Close,myATR,mySMI,myADX,myAroon,myBB,myChaikinVol, myCLV,myEMV,myMACD,myMFI,mySAR,myVolat,myCMO,myEMA, forceindex,WillR5,WillR10,WillR15,WillR30,WillR45,WillR60, RSI5,RSI10,RSI15,RSI30,RSI45,RSI60, ROC5,ROC10,ROC15,ROC30,ROC45,ROC60, MOM5,MOM10,MOM15,MOM30,MOM45,MOM60, ATR5,ATR10,ATR15,ATR30,ATR45,ATR60, Mean5,Mean10,Mean15,Mean30,Mean45,Mean60, SD5,SD10,SD15,SD30,SD45,SD60,zhenfu,huanshou)tail(mydataset)Close myATR mySMI myADX myAroon myBB myChaikinVol myCLV2980 1 3.541269 59.81687 28.90271 55 0.7459216 -1.1581303 -0.139450172981 0 3.494036 58.12533 28.46763 55 0.7118186 0.3881411 0.014692292982 1 3.971604 49.35320 27.41221 55 0.3746141 -3.0420371 -0.153050812983 1 3.888633 39.81658 26.43217 55 0.3783665 0.2083101 -0.141483552984 1 3.946588 31.02765 25.88484 55 0.3630058 -0.8824859 -0.075527162985 NA 3.955403 24.34497 24.82386 55 0.4283426 -6.2060752 -0.01131472 myEMV myMACD myMFI mySAR myVolat myCMO myEMA2980 6.188251e-04 3.383335 82.26928 98.32502 0.3562983 45.267176 0.00084080882981 4.658566e-04 3.476127 77.92001 99.45121 0.3493821 46.711260 0.00283687382982 2.671371e-04 3.360701 65.19533 107.71000 0.3824663 9.544950 -0.01532447062983 -3.034233e-05 3.136216 63.25320 107.71000 0.3857953 7.727144 -0.00965686722984 -4.895251e-04 2.853403 55.06603 107.11200 0.4081373 -4.286628 -0.00722574302985 -2.754052e-04 2.573196 52.62331 106.53792 0.3798513 -1.747815 -0.0022594600 forceindex WillR5 WillR10 WillR15 WillR30 WillR45 WillR602980 -49171519 0.9265367 0.4537445 0.2915094 0.2915094 0.2086428 0.20524742981 37598803 0.6693548 0.4055375 0.2580311 0.2349057 0.1681296 0.16539362982 34075913 0.9854772 0.9861478 0.7746114 0.7051887 0.5047265 0.49651282983 49447684 0.8772827 0.8891821 0.8067026 0.6358491 0.4550979 0.44769182984 11659692 0.7752545 0.8206250 0.7857570 0.6193396 0.4432816 0.43606782985 55125928 0.5830420 0.7018750 0.7018750 0.5297170 0.4060014 0.3755853 RSI5 RSI10 RSI15 RSI30 RSI45 RSI60 ROC5 ROC102980 21.975737 51.34959 62.96328 63.85533 63.19654 62.64153 -1.168111 5.8486242981 31.805274 54.59128 64.75479 65.85176 64.42324 63.65843 -3.367510 5.8635612982 8.578284 23.67412 36.13496 46.71875 50.26581 52.31439 -11.866983 -3.7859142983 19.109948 27.06896 38.44325 48.65479 51.67119 53.47462 -11.246115 -2.8957132984 24.012449 25.28028 37.25706 48.82718 51.85549 53.64726 -8.494582 -7.4287952985 45.294925 33.95349 41.28556 51.58082 53.73532 55.13925 -4.973899 -6.083909 ROC15 ROC30 ROC45 ROC60 MOM5 MOM10 MOM15 MOM30 MOM45 MOM602980 13.5809375 9.125107 19.78528 22.63558 -1.20 5.61 12.14 8.49 16.77 18.742981 14.5645143 12.890110 22.32674 23.14793 -3.58 5.69 13.06 11.73 18.75 19.312982 2.8837622 4.837251 11.01005 14.06788 -12.49 -3.65 2.60 4.28 9.20 11.442983 5.4970891 6.752011 12.67488 18.96225 -11.94 -2.81 4.91 5.96 10.60 15.022984 3.3661202 4.014077 13.28303 19.19345 -8.78 -7.59 3.08 3.65 11.09 15.232985 0.5838198 4.404285 18.94957 23.66060 -5.05 -6.25 0.56 4.07 15.37 18.46 ATR5 ATR10 ATR15 ATR30 ATR45 ATR60 Mean5 Mean10 Mean15 Mean30 Mean452980 3.44 3.44 3.44 3.44 3.44 3.44 104.524 101.801 98.30533 94.60333 92.102442981 2.88 2.88 2.88 2.88 2.88 2.88 103.808 102.370 99.17600 94.99433 92.519112982 10.18 10.18 10.18 10.18 10.18 10.18 101.310 102.005 99.34933 95.13700 92.723562983 2.81 2.81 2.81 2.81 2.81 2.81 98.922 101.724 99.67667 95.33567 92.959112984 4.70 4.70 4.70 4.70 4.70 4.70 97.166 100.965 99.88200 95.45733 93.205562985 4.07 4.07 4.07 4.07 4.07 4.07 96.156 100.340 99.91933 95.59300 93.54711 Mean60 SD5 SD10 SD15 SD30 SD45 SD60 zhenfu huanshou2980 89.86567 2.046040 3.789094 6.124109 5.798568 6.304757 6.834129 3.33 0.8612981 90.18750 1.884840 3.402179 5.724033 5.940947 6.374945 6.978591 2.83 0.7742982 90.37817 5.074185 4.212042 5.465018 5.829461 6.226899 6.887901 4.44 4.0102983 90.62850 5.024527 4.650943 4.943558 5.687187 6.073691 6.746620 2.65 1.0902984 90.88233 4.602253 5.161378 4.633946 5.628425 5.903346 6.600000 4.99 1.0302985 91.19000 3.906678 5.300442 4.601892 5.601426 5.625728 6.418002 4.30 0.570print(dim(mydataset))[1] 2985 60# data.test.tmp &lt;- mydataset[nrow(mydataset),]mydataset.tmp &lt;- na.omit(mydataset[1:(nrow(mydataset)-1),])print(dim(mydataset.tmp))[1] 2923 60y = mydataset.tmp$Closecbind(freq=table(y), percentage=prop.table(table(y))*100) freq percentage0 1384 47.348611 1539 52.65139# correlations = cor(mydataset.tmp[,c(2:58)])# print(head(correlations))# corrplot(correlations, method=\"circle\")set.seed(5)weights &lt;- random.forest.importance(Close~., mydataset.tmp, importance.type = 1)# 选取前十个权重较大的特征subset = cutoff.k(weights, 10)print(subset)[1] \"ROC45\" \"huanshou\" \"WillR60\" \"Mean60\" \"SD15\" \"myCLV\" \"zhenfu\" [8] \"MOM60\" \"SD60\" \"myATR\" 构建训练集、测试集用选取出来的十个权重较大的特征构建训练集测试集123456789101112131415dataset_rf = data.frame(Close,mydataset[,subset[1]],mydataset[,subset[2]], mydataset[,subset[3]],mydataset[,subset[4]], mydataset[,subset[5]],mydataset[,subset[6]], mydataset[,subset[7]],mydataset[,subset[8]], mydataset[,subset[9]],mydataset[,subset[10]])# write.csv(na.omit(dataset_rf),\"XYZQ.csv\", quote = F, row.names = F)# 测试集data.test &lt;- dataset_rf[nrow(dataset_rf),]# 训练集dataset_rf &lt;- na.omit(dataset_rf[1:(nrow(dataset_rf)-1),])dataset_rf$Close &lt;- factor(dataset_rf$Close)# 十次交叉验证trainControl = trainControl(method=\"cv\", number=10)metric = \"Accuracy\" 机器学习模型分别以nnet，naive bayes，rpart，pcaNNet，svmRadial，gbm，xgbTree七个模型进行训练测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# nnetfit.nnet = train(Close~., data=dataset_rf, method=\"nnet\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# naive bayesfit.nb = train(Close~., data=dataset_rf, method=\"nb\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# rpartfit.rpart = train(Close~., data=dataset_rf, method=\"rpart\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# pcaNNetfit.pcaNNet = train(Close~., data=dataset_rf, method=\"pcaNNet\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# svmRadialfit.svmRadial = train(Close~., data=dataset_rf, method=\"svmRadial\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# gbmfit.gbm = train(Close~., data=dataset_rf, method=\"gbm\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# xgbTreefit.xgbTree = train(Close~., data=dataset_rf, method=\"xgbTree\", metric=metric,preProc=c(\"range\"),trControl=trainControl)## Evaluating the algorithms using the \"Accuracy\" metricresults = resamples(list(nnet=fit.nnet,nb=fit.nb,rpart=fit.rpart, pcaNNet=fit.pcaNNet, svmRadial=fit.svmRadial, gbm=fit.gbm, xgbTree=fit.xgbTree))summary(results)Call:summary.resamples(object = results)Models: nnet, nb, rpart, pcaNNet, svmRadial, gbm, xgbTree Number of resamples: 10 Accuracy Min. 1st Qu. Median Mean 3rd Qu. Max. NA'snnet 0.5119454 0.5278005 0.5350531 0.5374879 0.5448303 0.5807560 0nb 0.4914676 0.5179795 0.5435738 0.5371219 0.5593500 0.5616438 0rpart 0.5034247 0.5239726 0.5299161 0.5289004 0.5376712 0.5426621 0pcaNNet 0.5034247 0.5226314 0.5273973 0.5299406 0.5439741 0.5582192 0svmRadial 0.5034247 0.5106918 0.5188356 0.5264996 0.5445205 0.5631399 0gbm 0.5017065 0.5393836 0.5462329 0.5473935 0.5571672 0.5890411 0xgbTree 0.5119454 0.5397722 0.5487166 0.5545761 0.5758279 0.5972696 0Kappa Min. 1st Qu. Median Mean 3rd Qu. Max. NA'snnet -0.018275937 0.019774954 0.03377084 0.03687602 0.05092740 0.12767212 0nb -0.054848141 -0.001926439 0.05667858 0.04044339 0.08712856 0.09197804 0rpart -0.019553073 0.018301156 0.03064388 0.02607515 0.03487674 0.06389776 0pcaNNet -0.037948617 0.001225074 0.02214420 0.02116072 0.04925753 0.07875171 0svmRadial -0.029068637 -0.010983769 0.00421168 0.01941413 0.05314826 0.09151688 0gbm -0.015573809 0.060255860 0.07418475 0.07746468 0.09846185 0.16428163 0xgbTree 0.004939796 0.062262588 0.07858693 0.09198844 0.13641100 0.17330592 0 dotplot(results) 预测涨跌首先根据每个模型的预测效果的平均数，计算出每个模型的权重，再根据这个权重计算出最终的涨跌概率。有点类似于7个常委进行民主投票进行最终的决策。12345678910111213141516171819202122232425262728293031323334353637383940pres.pcaNNet &lt;- predict(fit.pcaNNet, data.test)pres.nnet &lt;- predict(fit.nnet, data.test)pres.xgbTree &lt;- predict(fit.xgbTree, data.test)pres.svmRadial &lt;- predict(fit.svmRadial, data.test)pres.gbm &lt;- predict(fit.gbm, data.test)pres.nb &lt;- predict(fit.nb, data.test)pres.rpart &lt;- predict(fit.rpart, data.test)pres.pcaNNet &lt;- as.numeric(as.character(pres.pcaNNet))pres.nnet &lt;- as.numeric(as.character(pres.nnet))pres.xgbTree &lt;- as.numeric(as.character(pres.xgbTree))pres.svmRadial &lt;- as.numeric(as.character(pres.svmRadial))pres.gbm &lt;- as.numeric(as.character(pres.gbm))pres.nb &lt;- as.numeric(as.character(pres.nb))pres.rpart &lt;- as.numeric(as.character(pres.rpart))pres &lt;- data.frame(pres.pcaNNet,pres.nnet,pres.xgbTree,pres.svmRadial,pres.gbm,pres.nb,pres.rpart) prespres.pcaNNet pres.nnet pres.xgbTree pres.svmRadial pres.gbm pres.nb pres.rpart1 1 1 0 1 0 0 1# names(getModelInfo())results.acc &lt;- as.data.frame(summary(results)[3]$statistics$Accuracy)mean.sum &lt;- sum(results.acc$Mean)mean.pcaNNet &lt;- results.acc[\"pcaNNet\",\"Mean\"]mean.nnet &lt;- results.acc[\"nnet\",\"Mean\"]mean.xgbTree &lt;- results.acc[\"xgbTree\",\"Mean\"]mean.svmRadial &lt;- results.acc[\"svmRadial\",\"Mean\"]mean.gbm &lt;- results.acc[\"gbm\",\"Mean\"]mean.nb &lt;- results.acc[\"nb\",\"Mean\"]mean.rpart &lt;- results.acc[\"rpart\",\"Mean\"]# 加入权重计算涨跌概率q.pcaNNet &lt;- (mean.pcaNNet / mean.sum) * nrow(results.acc)q.nnet &lt;- (mean.nnet / mean.sum) * nrow(results.acc)q.xgbTree &lt;- (mean.xgbTree / mean.sum) * nrow(results.acc)q.svmRadial &lt;- (mean.svmRadial / mean.sum) * nrow(results.acc)q.gbm &lt;- (mean.gbm / mean.sum) * nrow(results.acc)q.nb &lt;- (mean.nb / mean.sum) * nrow(results.acc)q.rpart &lt;- (mean.rpart / mean.sum) * nrow(results.acc)# 最终的预测结果p.result &lt;- mean(c(pres.pcaNNet*q.pcaNNet,pres.nnet*q.nnet,pres.xgbTree*q.xgbTree,pres.svmRadial*q.svmRadial,pres.gbm*q.gbm,pres.nb*q.nb,pres.rpart*q.rpart))p.result[1] 0.5642939 最终计算出的今日涨跌概率为0.56，超出了0.5，那么可以认为，今天的分众传媒的预测结果为涨。实际上，分众传媒今天上涨了0.45%，这是巧合吗？有待进一步的探索。如设置止损点、挖掘更有效的特征以及配合蒙特卡洛方法。","tags":[{"name":"量化金融","slug":"量化金融","permalink":"http://weibo.com/youthcolor/tags/量化金融/"},{"name":"股票预测","slug":"股票预测","permalink":"http://weibo.com/youthcolor/tags/股票预测/"},{"name":"机器学习","slug":"机器学习","permalink":"http://weibo.com/youthcolor/tags/机器学习/"},{"name":"特征选取","slug":"特征选取","permalink":"http://weibo.com/youthcolor/tags/特征选取/"}]},{"title":"R中如何对量化策略进行回测","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/R中如何对量化策略进行回测/","text":"R中提供了极为便利的回测方法，如PerformanceAnalytics包，下面是简短实例。以A股福田汽车600166.SS为例。 第1步: 获取股票数据12setSymbolLookup(MYSTOCK=list(name='600166.SS',src='yahoo',from='2007-01-01'))getSymbols('MYSTOCK') 第2步: 建立指标12MYSTOCK &lt;- na.omit(MYSTOCK)dvi &lt;- DVI(Cl(MYSTOCK)) 第3步: 构建交易规则1sig &lt;- Lag(ifelse(dvi$dvi &lt; 0.5, 1, -1)) 第4步: 交易规则/资金曲线1234ret &lt;- ROC(Cl(MYSTOCK))*(sig-0.00025)ret &lt;- ret['2013-09-01/2014-06-01']eq &lt;- exp(cumsum(ret))plot(eq) 第5步: 评估量化策略的表现123456789101112131415161718192021222324table.Drawdowns(ret, top=10) From Trough To Depth Length To Trough Recovery1 2014-01-29 2014-02-18 2014-03-31 -0.1574 39 10 292 2013-09-02 2014-01-13 2014-01-28 -0.1246 99 88 113 2014-04-23 2014-05-09 2014-05-19 -0.0484 17 11 64 2014-04-01 2014-04-03 2014-04-08 -0.0206 5 3 25 2014-04-09 2014-04-14 2014-04-15 -0.0163 5 4 16 2014-05-22 2014-05-22 2014-05-26 -0.0061 3 1 27 2014-05-27 2014-05-27 2014-05-28 -0.0060 2 1 18 2014-05-20 2014-05-20 2014-05-21 -0.0041 2 1 1table.DownsideRisk(ret) 600166.SS.CloseSemi Deviation 0.0147Gain Deviation 0.0119Loss Deviation 0.0153Downside Deviation (MAR=210%) 0.0188Downside Deviation (Rf=0%) 0.0143Downside Deviation (0%) 0.0143Maximum Drawdown 0.1574Historical VaR (95%) -0.0268Historical ES (95%) -0.0466Modified VaR (95%) -0.0334Modified ES (95%) -0.0617charts.PerformanceSummary(ret)","tags":[{"name":"量化金融","slug":"量化金融","permalink":"http://weibo.com/youthcolor/tags/量化金融/"},{"name":"回测","slug":"回测","permalink":"http://weibo.com/youthcolor/tags/回测/"}]},{"title":"data.table() vs data.frame() – R大数据集处理","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/data.table() vs data.frame() – R大数据集处理/","text":"引文R用户(主要是初学者)在处理大型数据集时往往是比较无助的。他们会被重复的警告以及内存使用不足的错误信息所困扰。他们中的大多数人就会觉得，自己的机器配置不够强大。是时候升级内存，或者在新机器上工作了。你有这样想过吗?如果你认真研究过数据集，我相信你一定会有。甚至，当我参加黑色星期五的时候，我也做了同样的工作。数据集包含40多万行。我完全无可奈何。老实说，看到RStudio花费数小时执行一行代码是令人沮丧的。就像我们说的，“需求是发明之母”。我需要一个解决方案。 经过2个小时的互联网搜索研究，我发现了一些有趣的R包data.table和api，特别是在不影响执行速度的情况下使用大型数据集。 在本文中，我分享了一种智能方法，当您在大型数据集上工作时，您应该使用这种方法。当您向下滚动时，您将会遇到可以改进您的R编码的更改类型。是时候编写快速而简短的代码了。可以把data.table看作是一个关于数据的快速教程。 为什么你的机器无法处理大数据集理解影响您的R代码性能的因素是很重要的。很多时候，你的机器配置强弱直接影响你R代码所能处理的工作。下面是一些妨碍R在大数据集上的性能的例子: 使用read.csv加载大文件的csv函数。 使用谷歌chrome:在chrome中打开几个标签会消耗大量系统的内存。这可以在chrome浏览器中使用Shift+Esc键进行检查。(同样也适用于Mozilla web浏览器) 机器配置:R将整个数据集立即读取到RAM中。也就是说，R对象完全存在于内存中。如果您仍在使用2GB RAM机器，那么您在技术上是行不通的。有了2GB内存，就没有足够的空闲RAM空间可以无缝地处理大数据。因此，强烈建议使用至少4GB的机器。 在高温环境下工作:一旦机器升温，处理器速度就会减慢。特别是在夏季，它就会是一个比较严重的问题。 注:我的系统规格是Intel(R) i5-3320M CPU @2.60 GHz，双核，4个逻辑处理器，8GB RAM。 关于data.tabledata.table包是Matt Dowle在2008年编写的。 data.table作为一个高级版本的data.frame。它从data.frame继承而来，data.frame语法也都可以应用在data.table中。这个包可以与任何其他包接受data.frame的包一起使用。 data.table的语法与SQL非常相似。因此，如果您使用SQL，您将很快理解它。data.table语法的一般形式是:DT[i, j, by]，这里 DT表示数据表。 i &lt;=&gt; where: 表示行索引，这里，是行条件。 j &lt;=&gt; select: 表示列索引，这里，是列上放置条件(过滤，总结)。 by &lt;=&gt; group_by: 表示任何分类变量，这里，是分组执行的变量。 例如：123456789101112#creating a dummy data tableDT &lt;- data.table( ID = 1:50, Capacity = sample(100:1000, size = 50, replace = F), Code = sample(LETTERS[1:4], 50, replace = T), State = rep(c(\"Alabama\",\"Indiana\",\"Texas\",\"Nevada\"), 50))#simple data.table commandDT[Code == \"C\", mean(Capacity), State] State V11: Alabama 538.83332: Nevada 487.00003: Indiana 487.00004: Texas 538.8333 让我们看看这个命令是如何工作的。在创建了数据表之后，请求数据表对其Code列为C的行进行过滤，然后要求它计算每一个状态行的Capacity平均值。您不必总是提到语法的所有三个部分。试着在你的结尾执行以下命令:123DT[Code == \"D\"]DT[, mean(Capacity), by = State]DT[Code == \"A\", mean(Capacity)] 为什么要使用data.table而不是data.frame在我深入研究data.table之后，发现了优于data.frame包的几个方面。因此，我建议每个R初学者都可能多的使用data.table。它有很多值得探索的地方。你越早开始使用，你就会做得越好。您应该使用data.table,因为: 在加载数据时，它的速度极快。使用data.table中的fread函数，加载大型数据集只需几秒钟。例如，我使用包含439541行的数据集来检查加载时间。让我们看看fread有多快。12345678system.time(dt &lt;- read.csv(\"data.csv\"))user system elapsed 11.46 0.21 11.69system.time(dt &lt;- fread(\"data.csv\"))user system elapsed 0.66 0.00 0.66dim(dt)[1] 439541 18 如您所见，以fread加载数据的速度比基本函数read.csv快16倍。fread()比read.csv()更快，因为，read.csv()尝试首先将行读取到内存中，然后尝试将它们转换为整数，并作为数据类型进行转换。而fread()只是简单地把所有的数据都读成字符。 它甚至比流行的dplyr，plyr软件包还要快。data.table 为任务提供了足够的空间，如聚合、过滤、合并、分组和其他相关任务等。例如:123456system.time(dt %&gt;% group_by(Store_ID) %&gt;% filter(Gender == \"F\") %&gt;% summarise(sum(Transaction_Amount), mean(Var2))) #with dplyruser system elapsed 0.13 0.02 0.21system.time(dt[Gender == \"F\", .(sum(Transaction_Amount), mean(Var2)), by = Store_ID])user system elapsed 0.02 0.00 0.01 data.table处理这个任务的速度比dplyr快20倍。之所以发生这种情况，是因为它避免将内存分配给诸如过滤之类的中间步骤。另外，dplyr还创建了作为数据的整个数据帧的深度拷贝。data.table对数据框架进行了简单的复制。浅拷贝意味着数据不会在系统的内存中被物理地复制。它只是一个列指针(名称)的副本。深度拷贝将整个数据复制到内存中的另一个位置。因此，随着内存效率的提高，计算的速度也得到了提高。 不只是读取文件，将数据写入文件方面，data.table也比write.csv()快得多。这个包提供了fwrite()函数，具有并行快速写入能力。所以，下次你要写入100万行数据，试试这个函数。 在构建特征中，如自动索引、滚动连接、重叠范围连接，都进一步增强了用户在大数据集上的体验。 因此，您可以看到data.frame没有什么问题，它只是缺少像data.table一样更适用的特性和操作。 重要的数据操作命令本教程的目的是为您提供一些方便的命令，这些命令可以加速您的建模过程。实际上，在这个包中有很多要探索的地方，您可能会对从哪里开始、何时使用哪个命令以及何时使用特定命令感到困惑。在这里，我提供了一些最常见的问题，这些问题可能是您在进行数据探测/操作时经常遇到的。 下面使用的数据集可以从这里下载:下载。数据集包含1720673行12列。有趣的是，data.table会使这些数据多长时间被加载，是采取行动的时候了! 注意:数据集包含不均匀分布的观测值，即空列和NA值。获取这些数据的原因是为了检查data.table在处理大数据集的性能。12345#set working directorysetwd(\"C:/Users/Administrator/Desktop/新建文件夹/\")#load dataDT &lt;- fread(\"GB_full.csv\")Read 1720673 rows and 12 (of 12) columns from 0.191 GB file in 00:00:07 读取数据只用了7秒。 如何获取行和列的子集123456789101112131415161718#subsetting rowssub_rows &lt;- DT[V4 == \"England\" &amp; V3 == \"Beswick\"] V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 V121: GB YO25 Beswick England ENG East Riding of Yorkshire 11609011 NA NA 4#subsetting columnssub_columns &lt;- DT[,.(V2,V3,V4)] V2 V3 V4 1: AB10 Aberdeen Scotland 2: AB10 1AB Aberdeen Scotland 3: AB10 1AF Aberdeen Scotland 4: AB10 1AG Aberdeen Scotland 5: AB10 1AH Aberdeen Scotland --- 1720669: ZE3 9JU Shetland South Ward Scotland1720670: ZE3 9JW Shetland South Ward Scotland1720671: ZE3 9JX Shetland South Ward Scotland1720672: ZE3 9JY Shetland South Ward Scotland1720673: ZE3 9JZ Shetland South Ward Scotland 在data.table中，列被称为变量。因此，我们不需要将变量称为DT$列名，仅列名称就行了。如果你输入DT[,.c(V2，V3，V4)]，它会返回一个列值向量。使用.()符号，将变量封装在list()中，并返回数据表。实际上，每个数据表或数据框都是对长度和不同数据类型的列表的汇编。 设置键可以更快速地获取子集，键不过是更加有效的行名称。如下所示。 如何按升序或降序排列变量12#ordering columnsdt_order &lt;- DT[order(V4, -V8)] Order函数是数据表比基本函数order()快得多。原因是，在数据表中的顺序使用基数排序来提高速度。-以降序显示结果。 如何在数据集中添加/更新/删除列或值12#add a new columnDT[, V_New := V10 + V11] 我们没有将结果返回给DT，这是因为:=操作符通过引用修改输入对象。它会在R中产生较浅的拷贝，从而导致更好的性能，内存占用更少。其结果是不可见的返回。1234#update row valuesDT[V8 == \"Aberdeen City\", V8 := \"Abr City\"]#delete a columnDT [,c(\"V6\",\"V7\") := NULL ] 检查视图(DT)。我们可以看到，数据中包含了数据集中的空白列，可以使用上面的代码删除这些列。实际上，所有这三个步骤都可以在一个命令中完成。这就是命令的链接。1DT[V8 == \"Aberdeen City\", V8 := \"Abr City\"][, V_New := V10 + V11][,c(\"V6\",\"V7\") := NULL] 如何基于对一个列分组来计算变量的函数让我们计算一下V10变量在V4(显示国家)的基础上的平均值。1234#compute the averageDT[, .(average = mean(V1o)), by = V4]#compute the countDT[, .N, by = V4] .N是数据中的一个特殊变量。表用于计算变量的计数。如果您希望获得由选项指定的变量的顺序，您可以使用keyby替换by。keyby自动按升序顺序对分组变量进行排序。 如何为子集数据设置键值数据表中的键提供了难以置信的快速结果。我们通常在列名称上设置键，这些键可以是任何类型的，例如数字、因数、整数、字符。一旦一个键被设置为一个变量，它就会以递增的顺序重新排列列的观察值。设置一个键是有帮助的，特别是当您知道需要在一个变量上进行多个计算时。1234#setting a keysetkey(DT, V4) #subsetting England from V4DT[.(\"England\")] 一旦设置好了键，我们就不再需要一次又一次地提供列名称了。如果我们要在一个列中寻找多个值，我们可以把它写成:1DT[.(c(\"England\", \"Scotland\"))] 类似地，我们也可以设置多个键。这可以用:1setkey(DT, V3, V4) 我们可以再次使用这两列的子集值:1DT[.(\"Shetland South Ward\",\"Scotland\")] 还可以在上面演示的5个步骤中进行其他一些修改。上面介绍的这5个步骤将帮助您使用data.table执行基本的数据操作任务。为了了解更多，我建议您在每天的R工作中开始使用这个包。你会遇到各种各样的障碍，这就是你的学习曲线会加速的地方。 写在最后本文旨在为您提供一个可以轻松处理大型数据集的路径。你不再需要花钱升级你的机器了，现在是时候升级你处理这种情况的知识了。data.table还有其他几个用于并行计算的包。但是，一旦您精通了data.table，就不需要任何其他的数据操作包了。 在本文中，我讨论了在处理大型数据集时，R中每个初学者都必须知道的一些重要知识。在数据处理之后，接下来的障碍就是模型构建。有了大型数据集，像caret、随机森林、xgboost这样的包需要大量的时间来计算。 我计划下周在我的文章中提供一个有趣的解决方案！在处理大数据时，请告诉我您的痛处。你喜欢看这篇文章吗?在处理大数据集时，您使用了哪些其他的包?在评论中提出您的建议/意见。","tags":[{"name":"大数据","slug":"大数据","permalink":"http://weibo.com/youthcolor/tags/大数据/"}]},{"title":"R中的大数据分析","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/R中的大数据分析/","text":"尽管R语言是一门十分强大且健壮的统计型语言，但它最大的短板是对数据大小的限制，因为R需要将数据一次性先加载到内存。R只支持大约4G大小内存的数据量，一旦达到RAM的阀值，操作将无法继续。 但是，R中有一些包很好地支持了大数据分析，比如，bigmemory包被广泛用于大数据的统计与计算，还有biganalytics,bigtabulate,bigalgebra等包解决了大数据的管理与统计分析的问题。还一个与bigmemory相关联的ff包，它支持用户处理大型向量和矩阵，并能同时操作多个大数据文件，ff包的有一大好处就是它可以像操作原生的R向量一样进行操作，尽管数据不存储在内存中而是常驻在磁盘中。下面是几个简单的事例。 聚类载入大矩阵1234567891011install.packages(\"bigmemory\")install.packages(\"biganalytics\")library(bigmemory)library(biganalytics)x &lt;- read.big.matrix(\"FlightTicketData.csv\", type = 'integer', header = TRUE, backingfile=\"data.bin\", descriptorfile=\"data.desc\")head(x)class(x)xm &lt;- as.matrix(x)nrow(xm)[1] 3156925 聚类分析123456789res_bigkmeans &lt;- lapply(1:10, function(i)&#123; bigkmeans(x, centers = i, iter.max = 50, nstart = 1)&#125;)class(res_bigkmeans)lapply(res_bigkmeans, function(x) x$withinss)var &lt;- sapply(res_bigkmeans, function(x) sum(x$withinss))varplot(1:10, var, type = 'b', xlab = \"Number of clusters\", ylab = \"Percentage of variance explained\") 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950res_big &lt;- bigkmeans(x, centers = 3, iter.max = 50, nstart = 1)res_bigK-means clustering with 3 clusters of sizes 1120691, 919959, 1116275Cluster means: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8][1,] 2.757645 11040.08 1104010 30910.66 0.6813850 0.03740460 1.989817 2211.801[2,] 2.663235 12850.78 1285081 32097.61 0.6323662 0.03459393 2.084982 2305.836[3,] 2.744241 14513.19 1451322 32768.11 0.6545699 0.02660276 1.974971 2390.292 [,9][1,] 1.949151[2,] 1.929160[3,] 1.930394Clustering vector: [1] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 [40] 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 [79] 3 1 1 1 2 3 3 1 1 1 2 2 2 2 2 2 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [118] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [157] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [196] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 3 3 3 1 1 1 1 1 2 2 3 3 1 2 2 1 1 1 1 [235] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 [274] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 1 1 1 1 1 1 1 2 2 [313] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [352] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 [391] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 [430] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 [469] 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 2 3 3 3 3 3 3 3 3 1 1 2 2 1 1 2 2 [508] 2 2 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 [547] 3 3 3 3 3 3 2 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 [586] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [625] 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 [664] 3 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 3 3 3 3 3 3 2 3 1 2 2 2 1 2 1 1 1 1 1 1 1 [703] 1 1 1 1 1 1 1 1 1 1 3 3 1 2 1 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [742] 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 3 3 1 1 2 2 2 3 1 1 1 1 1 1 1 1 3 3 1 2 2 3 1 1 [781] 2 3 3 3 1 3 3 3 3 3 3 3 3 1 3 1 1 1 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [820] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [859] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [898] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [937] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [976] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 [ reached getOption(\"max.print\") -- omitted 3155925 entries ]Within cluster sum of squares by cluster:[1] 2.183142e+15 2.010160e+15 2.466224e+15Available components:[1] \"cluster\" \"centers\" \"withinss\" \"size\" 线性回归分析载入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253install.packages(\"ff\")install.packages(\"biglm\")library(ff)library(biglm)download.file(\"http://www.irs.gov/file_source/pub/irs-soi/12zpallagi.csv\", \"soi.csv\")x &lt;- read.csv.ffdf(file = \"soi.csv\", header = TRUE)class(x)[1] \"ffdf\"head(x)ffdf (all open) dim=c(73732,6), dimorder=c(1,2) row.names=NULLffdf virtual mapping PhysicalName VirtualVmode PhysicalVmode AsIs VirtualIsMatrixSTATEFIPS STATEFIPS integer integer FALSE FALSESTATE STATE integer integer FALSE FALSEzipcode zipcode integer integer FALSE FALSEAGI_STUB AGI_STUB integer integer FALSE FALSEN1 N1 double double FALSE FALSEMARS1 MARS1 double double FALSE FALSE PhysicalIsMatrix PhysicalElementNo PhysicalFirstCol PhysicalLastColSTATEFIPS FALSE 1 1 1STATE FALSE 2 1 1zipcode FALSE 3 1 1AGI_STUB FALSE 4 1 1N1 FALSE 5 1 1MARS1 FALSE 6 1 1 PhysicalIsOpenSTATEFIPS TRUESTATE TRUEzipcode TRUEAGI_STUB TRUEN1 TRUEMARS1 TRUEffdf data STATEFIPS STATE zipcode AGI_STUB N1 MARS11 1 AL 0 1 889920 4908502 1 AL 0 2 491150 1943703 1 AL 0 3 254280 681604 1 AL 0 4 160160 230205 1 AL 0 5 183320 158806 1 AL 0 6 44840 34207 1 AL 35004 1 1600 9908 1 AL 35004 2 1310 570: : : : : : :73725 27 MN 55412 1 4820 289073726 27 MN 55412 2 3020 171073727 27 MN 55412 3 1280 63073728 27 MN 55412 4 590 20073729 27 MN 55412 5 440 9073730 27 MN 55412 6 40 073731 27 MN 55413 1 2630 208073732 27 MN 5 NA NA NA 应用线性回归模型123456789101112131415require(biglm)mymodel &lt;- biglm(A02300 ~ A00200+AGI_STUB+NUMDEP+MARS2, data = x)summary(mymodel)Large data regression model: biglm(A02300 ~ A00200 + AGI_STUB + NUMDEP + MARS2, data = x)Sample size = 73731 Coef (95% CI) SE p(Intercept) -60.0777 -161.8333 41.6779 50.8778 0.2377A00200 -0.0014 -0.0015 -0.0014 0.0000 0.0000AGI_STUB -2.8467 -28.9734 23.2800 13.0634 0.8275NUMDEP 1.0457 1.0421 1.0493 0.0018 0.0000MARS2 -0.3549 -0.3679 -0.3419 0.0065 0.0000summary(mymodel)$rsq[1] 0.9424789 可覆盖解释了94.24789%的线性回归模型看起来相当不错！","tags":[{"name":"大数据","slug":"大数据","permalink":"http://weibo.com/youthcolor/tags/大数据/"},{"name":"聚类","slug":"聚类","permalink":"http://weibo.com/youthcolor/tags/聚类/"},{"name":"线性回归","slug":"线性回归","permalink":"http://weibo.com/youthcolor/tags/线性回归/"}]},{"title":"使用H2O和data.table构建R大数据集模型","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/使用H2O和data.table构建R大数据集模型/","text":"引文上周，我写了一篇关于data.table包的介绍性文章。它的目的是为您提供一个良好的开端，并熟悉它的独特和简短的语法。下一步是关注建模，我们将在今天的文章中做建模。 有了data.table，您不再需要担心您的机器没有足够的RAM。至少，在面对大型数据集时，我曾经认为自己是一个瘫痪的R用户。但不会有这种担心了，再次感谢Matt Dowle的做出的贡献。上周，我收到一封邮件说:“好的,我明白了。data.table使我们能够进行数据的探索和操作。但是，模型构建呢?我的RAM为8G。像随机森林(ntrees=1000)这样的算法需要花费太长时间来运行80万行数据集。” 我确信有很多的R用户被困在类似的情况下。为了克服这一艰难的障碍，我决定编写这篇文章，它演示了如何使用两个最强大的包，即H2O和data.table。 为了达到实用性的理解，我从一个实践问题中获取了数据，并尝试使用4种不同的机器学习算法(H2O)和特性工程(使用data.table)来提高分数。所以，在排行榜上准备好从排行第154位到排行第25位的旅程。 目录 H2O简介 为什么它如此快 实战开始 使用data.table和ggplot进行数据探索 使用data.table处理数据 使用H2O构建模型 回归 随机森林 GBM 深度学习 注意:将本文作为使用data.table和H2O进行数据模型构建的初学者指南。我还没有详细解释这些算法。相反，我们关注的焦点是如何使用H2O来实现这些算法。别担心，资源和链接都会提供的。 H2O简介H2O是一个开源机器学习平台，企业可以在大型数据集(无采样需求)上建立模型，并实现准确的预测。它的速度非常快，可伸缩，并且很容易实现。 简而言之，它们为企业提供了一个GUI驱动的平台，以便进行更快的数据计算。目前，他们的平台支持高级和基本的机器学习算法，如深度学习、提升、装袋、朴素贝叶斯、主成分分析、时间序列、k-均值、广义线性模型。 此外，H2O还为R、Python、Spark和Hadoop用户提供了api，让我们这样的人可以使用它在各个层次上构建模型。它可以自由使用，并加快计算速度。 为什么它如此快H2O有一个干净而清晰的特性，可以直接将工具(R或Python)与您的计算机的CPU连接起来。这样我们就可以获得更多的内存，处理更快速的计算工具的能力。这将允许计算以100%的CPU容量进行(如下所示)。它还可以在云平台的集群上进行计算。 此外，它还使用内存中的压缩来处理大型数据集，即使有一个小的集群。它还提供了并行分布式式网络训练的支持。 实战开始数据集:我从黑色星期五的实践问题中获取数据。数据集有两部分:训练集和测试集。训练数据集包含550068个观测数据。测试数据集包含233599个观察结果。下载数据并阅读问题声明:点击这里。需要登录。 理想情况下，模型构建的第一步是生成假设。在您阅读了问题陈述之后，但是没有看到数据，这一步是有必要的。 因为，这个指南并不是为了演示所有的预测建模步骤，所以我把它留给您。这里有一个很好的资源来更新你的基础知识:假设生成向导。如果你做了这一步，也许你最终会创造出比我更好的模型。尽你最大的努力。 H2O官网在R中安装H2O方法：12345678910111213# The following two commands remove any previously installed H2O packages for R.if (\"package:h2o\" %in% search()) &#123; detach(\"package:h2o\", unload=TRUE) &#125;if (\"h2o\" %in% rownames(installed.packages())) &#123; remove.packages(\"h2o\") &#125;# Next, we download packages that H2O depends on.pkgs &lt;- c(\"RCurl\",\"jsonlite\")for (pkg in pkgs) &#123; if (! (pkg %in% rownames(installed.packages()))) &#123; install.packages(pkg) &#125;&#125;# Now we download, install and initialize the H2O package for R.install.packages(\"h2o\", type=\"source\", repos=\"http://h2o-release.s3.amazonaws.com/h2o/rel-wheeler/1/R\")# Finally, let's load H2O and start up an H2O clusterlibrary(h2o)h2o.init() 开始在R中加载数据12345678path &lt;- \"D:\\\\用户目录\\\\下载\"setwd(path)#install and load the packageinstall.packages(\"data.table\")library(data.table)#load data using freadtrain &lt;- fread(\"train.csv\", stringsAsFactors = T)test &lt;- fread(\"test.csv\", stringsAsFactors = T) 在几秒钟内，fread将数据加载到r中，速度很快。参数stringsAsFactors确保将字符向量转换为因子。让我们快速检查一下数据集。1234567891011121314151617181920dim(train)[1] 550068 12#No. of rows and columns in Testdim(test)[1] 233599 11str(train)Classes ‘data.table’ and 'data.frame': 550068 obs. of 12 variables: $ User_ID : int 1000001 1000001 1000001 1000001 1000002 1000003 1000004 1000004 1000004 1000005 ... $ Product_ID : Factor w/ 3631 levels \"P00000142\",\"P00000242\",..: 673 2377 853 829 2735 1832 1746 3321 3605 2632 ... $ Gender : Factor w/ 2 levels \"F\",\"M\": 1 1 1 1 2 2 2 2 2 2 ... $ Age : Factor w/ 7 levels \"0-17\",\"18-25\",..: 1 1 1 1 7 3 5 5 5 3 ... $ Occupation : int 10 10 10 10 16 15 7 7 7 20 ... $ City_Category : Factor w/ 3 levels \"A\",\"B\",\"C\": 1 1 1 1 3 1 2 2 2 1 ... $ Stay_In_Current_City_Years: Factor w/ 5 levels \"0\",\"1\",\"2\",\"3\",..: 3 3 3 3 5 4 3 3 3 2 ... $ Marital_Status : int 0 0 0 0 0 0 1 1 1 1 ... $ Product_Category_1 : int 3 1 12 12 8 1 1 1 1 8 ... $ Product_Category_2 : int NA 6 NA 14 NA 2 8 15 16 NA ... $ Product_Category_3 : int NA 14 NA NA NA NA 17 NA NA NA ... $ Purchase : int 8370 15200 1422 1057 7969 15227 19215 15854 15686 7871 ... - attr(*, \".internal.selfref\")=&lt;externalptr&gt; 我们看到了什么?我们看到了12个变量，其中2个看起来有那么多的NAs。如果你读过问题描述和数据信息，我们会看到购买是依赖变量，剩下11个是独立变量。 在购买变量(连续)的性质上，我们可以推断这是一个回归问题。尽管比赛已经结束了，但是我们仍然可以检查我们的分数，并评估我们的表现有多好。让我们第一次提交看看结果如何。 有了所有的数据点，我们可以用均值来做第一组预测。这是因为，平均预测会给我们一个很好的预测误差的近似值。将此作为基线预测，我们的模型不会比这更糟。123#first prediction using meansub_mean &lt;- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = mean(train$Purchase))write.csv(sub_mean, file = \"first_sub.csv\", row.names = F) 这个比较简单。现在，我将上传结果文件并检查我的分数和等级。别忘了转换csv。上传之前的压缩格式。你可以在竞争页面上上传和检查你的解决方案。 我们的平均预测给了我们一个均值的平方误差4982.3199。但是，它有多好呢?让我们来看看我在排行榜上的排名。 谢天谢地，我不是最后一个。所以，平均预测得到了154/162。让我们改进这一分数，并试图提升排行榜的排名。 在开始使用单变量分析之前，让我们快速地总结一下文件(训练集和测试集)和解释，如果存在任何差异的话。 仔细看(在结尾)，你看到他们的输出有什么不同吗?事实上,我发现了一个。如果你仔细比较产品类别1、产品类别2和产品类别的测试和训练数据，你就会在最大的价值上存在差异。产品分类的最大值是20，而其他的是18。这些额外的类别似乎是噪声。请注意这一点。我们需要移除它们。 让我们把数据集合并起来，我使用了来自data.table的rbindlist函数，因为它比rbind函数速度快。1234#combine data settest[,Purchase := mean(train$Purchase)]c &lt;- list(train, test)combin &lt;- rbindlist(c) 在上面的代码中，我们首先在测试集中添加了购买变量，这样两个数据集都有相同数量的列。现在，我们来做一些数据探索。 使用data.table和ggplot进行数据探索在本节中，我们将做一些单变量和变量对分析，并尝试理解给定变量之间的关系。让我们先从单变量开始。123456789101112131415161718192021222324252627282930313233#analyzing gender variablecombin[,prop.table(table(Gender))]Gender F M 0.2470896 0.7529104 #Age Variablecombin[,prop.table(table(Age))]Age 0-17 18-25 26-35 36-45 46-50 51-55 55+ 0.02722330 0.18113944 0.39942348 0.19998801 0.08329814 0.06990724 0.03902040 #City Category Variablecombin[,prop.table(table(City_Category))]City_Category A B C 0.2682823 0.4207642 0.3109535 #Stay in Current Years Variablecombin[,prop.table(table(Stay_In_Current_City_Years))]Stay_In_Current_City_Years 0 1 2 3 4+ 0.1348991 0.3527327 0.1855724 0.1728132 0.1539825 #unique values in ID variableslength(unique(combin$Product_ID))[1] 3677length(unique(combin$User_ID))[1] 5891#missing valuescolSums(is.na(combin)) User_ID Product_ID Gender Age 0 0 0 0 Occupation City_Category Stay_In_Current_City_Years Marital_Status 0 0 0 0 Product_Category_1 Product_Category_2 Product_Category_3 Purchase 0 245982 545809 0 以下是我们可以从单变量分析中得出的推论: 我们需要将性别变量编码为0和1。 我们还需要分箱重新编码这个年龄变量。 由于在city category中有三个级别，所以我们可以进行一种热编码。 目前的“4+”水平需要重新评估。 数据集不包含所有惟一的id。这给了我们足够的特性工程的提示。 只有两个变量有缺失值。事实上，很多缺失值，可能是一种隐藏的趋势。我们需要以不同的方式对待他们。 我们已经从单变量分析中得到了足够的提示。让我们快速地进行二元分析。通过添加更多参数，您可以使这些图表看起来很漂亮。这里有一个快速的学习ggplots的指南。123library(ggplot2)#Age vs Genderggplot(combin, aes(Age, fill = Gender)) + geom_bar() 12#Age vs City_Categoryggplot(combin, aes(Age, fill = City_Category)) + geom_bar() 我们还可以创建交叉表来分析分类变量。为了创建交叉表，我们将使用创建全面交叉表的包gmodel。12library(gmodels)CrossTable(combin$Occupation, combin$City_Category) 有了这两个变量，您就可以获得一个长时间的全面交叉表。类似地，您可以在最后分析其他变量。我们的二元分析并没有给我们提供很多可操作的见解。不管怎样，我们现在要进行数据处理。 使用data.table处理数据在这一部分中，我们将创建新的变量，重新评估现有变量，并处理缺失的值。简而言之，我们将为建模阶段准备好数据。让我们从缺少的值开始。我们看到产品类别和产品类别有很多缺失值。对我来说，这暗示了一个隐藏的趋势，它可以通过创建一个新的变量来映射。因此，我们将创建一个新的变量，将NAs作为1和非NAs，在变量product类别2和product类别3中捕获。123#create a new variable for missing valuescombin[,Product_Category_2_NA := ifelse(sapply(combin$Product_Category_2, is.na) == TRUE,1,0)]combin[,Product_Category_3_NA := ifelse(sapply(combin$Product_Category_3, is.na) == TRUE,1,0)] 现在让我们用任意的数字来估算缺失的值。让我们取-999123#impute missing valuescombin[,Product_Category_2 := ifelse(is.na(Product_Category_2) == TRUE, \"-999\", Product_Category_2)]combin[,Product_Category_3 := ifelse(is.na(Product_Category_3) == TRUE, \"-999\", Product_Category_3)] 在继续进行特性工程之前，我们将从单变量分析中重新评估变量的值。1234567891011121314#set column levellevels(combin$Stay_In_Current_City_Years)[levels(combin$Stay_In_Current_City_Years) == \"4+\"] &lt;- \"4\"#recoding age groupslevels(combin$Age)[levels(combin$Age) == \"0-17\"] &lt;- 0levels(combin$Age)[levels(combin$Age) == \"18-25\"] &lt;- 1levels(combin$Age)[levels(combin$Age) == \"26-35\"] &lt;- 2levels(combin$Age)[levels(combin$Age) == \"36-45\"] &lt;- 3levels(combin$Age)[levels(combin$Age) == \"46-50\"] &lt;- 4levels(combin$Age)[levels(combin$Age) == \"51-55\"] &lt;- 5levels(combin$Age)[levels(combin$Age) == \"55+\"] &lt;- 6#convert age to numericcombin$Age &lt;- as.numeric(combin$Age)#convert Gender into numericcombin[, Gender := as.numeric(as.factor(Gender)) - 1] 为了建模目的，将因子变量转换为数值或整数是明智的。 现在让我们一步一步来，创建更多新的变量a.k.a特性工程。 在单变量分析中，我们发现ID变量的惟一值与数据集中的总观察值相比要小一些，这意味着用户ID或productid必须多次出现在这个数据集中。 让我们创建一个新变量来捕获这些ID变量的计数。较高的用户数量表明，某个特定用户已经多次购买产品。高产品数量表明，一款产品已经被多次购买，这表明它的受欢迎程度。1234#User Countcombin[, User_Count := .N, by = User_ID]#Product Countcombin[, Product_Count := .N, by = Product_ID] 同样，我们可以计算一个产品的平均购买价格。因为，购买价格越低，就越有可能被买到，反之亦然。类似地，我们可以创建另一个变量，该变量将用户的平均购买价格映射到用户的购买价格(平均)。1234#Mean Purchase of Productcombin[, Mean_Purchase_Product := mean(Purchase), by = Product_ID]#Mean Purchase of Usercombin[, Mean_Purchase_User := mean(Purchase), by = User_ID] 现在，我们只剩下一个关于city_category变量的热编码。这可以用dummies库来完成。12library(dummies)combin &lt;- dummy.data.frame(combin, names = c(\"City_Category\"), sep = \"_\") 在继续进行建模阶段之前，让我们检查一下变量的数据类型，并在必要时进行必要的更改。12345#check classes of all variablessapply(combin, class)#converting Product Category 2 &amp; 3combin$Product_Category_2 &lt;- as.integer(combin$Product_Category_2)combin$Product_Category_3 &lt;- as.integer(combin$Product_Category_3) 使用H2O建模在这一节中，我们将探讨在H2O中不同机器学习算法的力量。我们将构建带有回归、随机森林、GBM和深度学习的模型。 确保你不会像一个黑盒子那样使用这些算法。最好知道它们是如何工作的。这将帮助您理解构建这些模型所使用的参数。以下是学习这些算法的一些有用的资源: 回归 随机森林，GBM 深度学习 但是,先做重要的事。让我们把数据集分成测试集和训练集。123#Divide into train and testc.train &lt;- combin[1:nrow(train),]c.test &lt;- combin[-(1:nrow(train)),] 正如在开始时所发现的，在火车上的变量产品类别有一些噪声。让我们通过在产品类别1-18中选择所有行来删除它，从而删除类别为19和20的水平。1c.train &lt;- c.train[c.train$Product_Category_1 &lt;= 18,] 现在，我们的数据集已经准备好进行建模了。1localH2O &lt;- h2o.init(nthreads = -1) 这个命令告诉H2O要使用机器上的所有cpu，这是推荐的。对于较大的数据集(比如1,000,000行)，H2O建议在具有高内存的服务器上运行集群，以获得最佳性能。一旦实例成功启动，您还可以使用下面的代码查看他的状态:1h2o.init() 现在让我们将数据从R转移到H2O实例。它可以用as来完成H2O的命令。123#data to h2o clustertrain.h2o &lt;- as.h2o(c.train)test.h2o &lt;- as.h2o(c.test) 使用列索引，我们需要识别在建模中使用的变量:123456789#check column index numbercolnames(train.h2o) [1] \"User_ID\" \"Product_ID\" \"Gender\" [4] \"Age\" \"Occupation\" \"City_Category_A\" [7] \"City_Category_B\" \"City_Category_C\" \"Stay_In_Current_City_Years\"[10] \"Marital_Status\" \"Product_Category_1\" \"Product_Category_2\" [13] \"Product_Category_3\" \"Purchase\" \"Product_Category_2_NA\" [16] \"Product_Category_3_NA\" \"User_Count\" \"Product_Count\" [19] \"Mean_Purchase_Product\" \"Mean_Purchase_User\" 让我们从多元线性回归模型开始。 H2O中的多元线性回归12345678910111213141516regression.model &lt;- h2o.glm( y = y.dep, x = x.indep, training_frame = train.h2o, family = \"gaussian\") |==================================================================================================| 100%h2o.performance(regression.model)H2ORegressionMetrics: glm** Reported on training data. **MSE: 16710563RMSE: 4087.856MAE: 3219.644RMSLE: 0.5782911Mean Residual Deviance : 16710563R^2 : 0.3261543Null Deviance :1.353804e+13Null D.o.F. :545914Residual Deviance :9.122547e+12Residual D.o.F. :545898AIC :10628689 H2O的GLM算法适用于各种类型的回归，如lasso、bridge、逻辑、线性等。用户只需要对family参数进行相应的修改。例如:要做逻辑回归，你可以改写成family=“binomial”。 因此，在我们打印了模型结果之后，我们看到，回归给出了一个可怜的R值，即0.326。它的意思是，依赖变量中只有32.6%的变量是由独立变量来解释的，而剩下的是无法解释的。这表明，回归模型无法捕获非线性关系。 出于好奇，让我们看看这个模型的预测。它会比平均预测更糟糕吗?1234#make predictionspredict.reg &lt;- as.data.frame(h2o.predict(regression.model, test.h2o))sub_reg &lt;- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.reg$predict)write.csv(sub_reg, file = \"sub_reg.csv\", row.names = F) 让我们上传解决方案文件(压缩格式)，检查我们是否有一些改进。 哇!我们的预测分数有所提高。我们从4982.31开始，随着回归，我们比以前的分数有了进步。在排行榜上，这份报告让我排到了第129位。 如果我们选择一种能很好地映射非线性关系的算法，我们就能做得很好。随机森林是我们的下一个赌注。 H2O中的随机森林1234#Random Forestsystem.time( rforest.model &lt;- h2o.randomForest(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, mtries = 3, max_depth = 4, seed = 1122)) 在1000棵树的情况下，随机森林模型大约需要104秒的运行时间。它以100%的CPU容量运行，可以在任务管理器中看到。123456789101112131415161718192021222324252627282930h2o.performance(rforest.model)H2ORegressionMetrics: drf** Reported on training data. **** Metrics reported on Out-Of-Bag training samples **MSE: 10414919RMSE: 3227.215MAE: 2486.118RMSLE: 0.5007453Mean Residual Deviance : 10414919#check variable importanceh2o.varimp(rforest.model)Variable Importances: variable relative_importance scaled_importance percentage1 Mean_Purchase_Product 2720452686381056.000000 1.000000 0.5737972 Product_Category_1 1005997304840192.000000 0.369790 0.2121853 Product_Count 252741091852288.000000 0.092904 0.0533084 Product_Category_3 231408274505728.000000 0.085062 0.0488095 Product_Category_3_NA 194243133964288.000000 0.071401 0.0409706 Mean_Purchase_User 174858721820672.000000 0.064276 0.0368817 Product_Category_2 84932466573312.000000 0.031220 0.0179148 Product_Category_2_NA 54471002423296.000000 0.020023 0.0114899 User_Count 12314694647808.000000 0.004527 0.00259710 City_Category_C 5007590031360.000000 0.001841 0.00105611 Gender 2175469223936.000000 0.000800 0.00045912 City_Category_A 1162100736000.000000 0.000427 0.00024513 Age 613729370112.000000 0.000226 0.00012914 Occupation 478127718400.000000 0.000176 0.00010115 City_Category_B 234770481152.000000 0.000086 0.00005016 Stay_In_Current_City_Years 32139771904.000000 0.000012 0.00000717 Marital_Status 17185155072.000000 0.000006 0.000004 让我们通过做预测来检查这个模型的排行榜。你认为我们的分数会提高吗?不过，我还是报点希望。12345678#making predictions on unseen datasystem.time(predict.rforest &lt;- as.data.frame(h2o.predict(rforest.model, test.h2o))) |==================================================================================================| 100% 用户 系统 流逝 0.56 0.06 22.17 #writing submission filesub_rf &lt;- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.rforest$predict)write.csv(sub_rf, file = \"sub_rf.csv\", row.names = F) 做出预测需要22秒。现在是上传提交文件并检查结果的时候了。 这在排行榜上略有改善，但没有预期的那么好。可以再试试GBM，一种增强算法也许可以有所帮助。 H2O中的GBM如果您是GBM的新手，我建议您检查本节开始时给出的参考资料。我们可以在H2O中使用简单的代码行来实现GBM:1234567#GBMsystem.time( gbm.model &lt;- h2o.gbm(y=y.dep, x=x.indep, training_frame = train.h2o, ntrees = 1000, max_depth = 4, learn_rate = 0.01, seed = 1122))|==================================================================================================| 100% 用户 系统 流逝 3.33 0.15 298.91 使用相同数量的树，GBM花费的时间比随机的森林多。花了299秒。您可以使用这个模型来检查这个模型的性能:12345678h2o.performance (gbm.model)H2ORegressionMetrics: gbm** Reported on training data. **MSE: 6321280RMSE: 2514.216MAE: 1859.895RMSLE: NaNMean Residual Deviance : 6321280 正如您所看到的，与前两个模型相比，我们的R方已经有了很大的改进。这显示了一个强大的模式的迹象。让我们做个预测，看看这个模型是否能给我们带来一些改进。1234#making prediction and writing submission filepredict.gbm &lt;- as.data.frame(h2o.predict(gbm.model, test.h2o))sub_gbm &lt;- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.gbm$predict)write.csv(sub_gbm, file = \"sub_gbm.csv\", row.names = F) 我们已经创建了提交文件。让我们上传它，看看我们是否有任何改进。 我从来没有怀疑过GBM。如果做得好，增加算法通常会得到很好的回报。现在，我将会很有趣地看到我的排行榜: 这是一个巨大的排行榜的飞跃！它就像一个自由落体但安全着陆从第122位到第25位。我们能做得更好吗?也许,我们可以。现在让我们在H2O中使用深度学习算法来提高这个分数。 H2O中的深度学习让我简要地概述一下深度学习。在深度学习算法中，存在3层，即输入层、隐藏层和输出层。它的工作原理如下: 我们将数据输入到输入层。 然后，它将数据传输到隐藏层。这些隐藏的层由神经元组成。这些神经元使用一些功能，并协助在变量之间绘制非线性关系。隐藏的层是用户指定的。 最后，这些隐藏层将输出到输出层，然后输出结果。 现在我们来实现这个算法。1234567891011121314#deep learning modelssystem.time( dlearning.model &lt;- h2o.deeplearning(y = y.dep, x = x.indep, training_frame = train.h2o, epoch = 60, hidden = c(100,100), activation = \"Rectifier\", seed = 1122 ))|==================================================================================================| 100% 用户 系统 流逝 1.06 0.07 167.21 它的执行速度比GBM模型要快。GBM用了约 167秒。隐藏的参数指示这些算法创建每一个隐藏的100个神经元的隐藏层。epoch 负责训练数据的传递的数量。Activation 指的是在整个网络中使用的激活函数。不管怎样，让我们检查一下它的性能。123456789&gt; h2o.performance(dlearning.model)H2ORegressionMetrics: deeplearning** Reported on training data. **** Metrics reported on temporary training frame with 9881 samples **MSE: 6171467RMSE: 2484.244MAE: 1832.876RMSLE: NaNMean Residual Deviance : 6171467 与GBM模型相比，我们可以看到R方度量的进一步改进。这表明，深度学习模型已经成功地捕获了模型中大量无法解释的差异。我们来做一下预测，看看最终的分数。12345#making predictionspredict.dl2 &lt;- as.data.frame(h2o.predict(dlearning.model, test.h2o))#create a data frame and writing submission filesub_dlearning &lt;- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase = predict.dl2$predict)write.csv(sub_dlearning, file = \"sub_dlearning_new.csv\", row.names = F) 让我们上传最后的提交，并检查分数。 虽然我的分数有所提高，但排名没有。最后，我们通过使用一些特性工程和机器学习算法，最终达到了第25位。我希望你从第154位到第25位的旅程很愉快。如果你一直跟着我到这里，我想你已经准备好再走一步了。 下一步怎样改进模型实际上，你可以做很多事情。在这里，我把它们列出来: 在GBM、深度学习和随机森林中进行参数调优。 使用grid搜索进行参数调优。H2O有一个很好的H2O。完成这个任务的grid 考虑创建更多的特性，这些特性可以为模型带来新的信息。 最后，综合所有的结果以获得一个更好的模型。 结束语我希望你能享受这次使用data.table和H2O的数据探索经历。一旦你熟练使用这两个包，你就可以避免由于内存问题而产生的许多障碍。在本文中，我讨论了使用data.table和H2O实现模型构建的步骤(使用R代码)。虽然，H2O本身可以进行数据处理任务，但我相信data.table是一个非常容易使用的(语法)选项。 在本文中，我的目的是让您开始使用data.table和H2O构建模型。我相信，在这种建模实践之后，您将会变得很有兴趣进一步了解这些包。 这篇文章让你学到了新的东西吗?请在评论中写下你的建议、经验或任何反馈，这些都能让我以更好的方式帮助你。 译自：https://www.analyticsvidhya.com/blog/2016/05/h2o-data-table-build-models-large-data-sets/","tags":[{"name":"大数据","slug":"大数据","permalink":"http://weibo.com/youthcolor/tags/大数据/"},{"name":"H2O","slug":"H2O","permalink":"http://weibo.com/youthcolor/tags/H2O/"}]},{"title":"侦测欺诈交易","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/侦测欺诈交易/","text":"本案例来自葡萄牙作者Luis Torgo所著的《数据挖掘与Ｒ语言》一书第４章：侦测欺诈交易。 本章介绍了一类数据挖掘问题：离群值排序。特别是采用了一个从不同的方面处理这个任务的数据集。即应用了有监督的、无监督的和半监督的方法。对于应用有限的资源来找到一个现象的异常观测值这一常见问题，本章的应用可以看做是它的一个实例。多个实际问题可以映射到这个通用框架中，例如信用卡、电信和税收的欺诈侦测等。在证券领域，也有这种欺诈侦测一般概念的多个应用。 加载数据123456789# 侦测欺骗交易setwd(\"~/Downloads\")###################################################### Loading the data into R###################################################load('sales.Rdata')library(DMwR)data(sales)head(sales) 探索数据集12345678910111213###################################################### Exploring the dataset###################################################summary(sales)nlevels(sales$ID)nlevels(sales$Prod)length(which(is.na(sales$Quant) &amp; is.na(sales$Val)))sum(is.na(sales$Quant) &amp; is.na(sales$Val))table(sales$Insp)/nrow(sales)*100totS &lt;- table(sales$ID)totP &lt;- table(sales$Prod)barplot(totS,main='Transactions per salespeople',names.arg='',xlab='Salespeople', ylab='Amount') 12barplot(totP,main='Transactions per product',names.arg='',xlab='Products', ylab='Amount') 1234567891011sales$Uprice &lt;- sales$Val/sales$Quantsummary(sales$Uprice)attach(sales)upp &lt;- aggregate(Uprice,list(Prod),median,na.rm=T)topP &lt;- sapply(c(T,F),function(o) upp[order(upp[,2],decreasing=o)[1:5],1])colnames(topP) &lt;- c('Expensive','Cheap')topPtops &lt;- sales[Prod %in% topP[1,],c('Prod','Uprice')]tops$Prod &lt;- factor(tops$Prod)boxplot(Uprice ~ Prod,data=tops,ylab='Uprice',log=\"y\") 123456789101112131415161718192021vs &lt;- aggregate(Val,list(ID),sum,na.rm=T)scoresSs &lt;- sapply(c(T,F),function(o) vs[order(vs$x,decreasing=o)[1:5],1])colnames(scoresSs) &lt;- c('Most','Least')scoresSssum(vs[order(vs$x,decreasing=T)[1:100],2])/sum(Val,na.rm=T)*100sum(vs[order(vs$x,decreasing=F)[1:2000],2])/sum(Val,na.rm=T)*100qs &lt;- aggregate(Quant,list(Prod),sum,na.rm=T)scoresPs &lt;- sapply(c(T,F),function(o) qs[order(qs$x,decreasing=o)[1:5],1])colnames(scoresPs) &lt;- c('Most','Least')scoresPssum(as.double(qs[order(qs$x,decreasing=T)[1:100],2]))/ sum(as.double(Quant),na.rm=T)*100sum(as.double(qs[order(qs$x,decreasing=F)[1:4000],2]))/ sum(as.double(Quant),na.rm=T)*100out &lt;- tapply(Uprice,list(Prod=Prod), function(x) length(boxplot.stats(x)$out))out[order(out,decreasing=T)[1:10]]sum(out)sum(out)/nrow(sales)*100 数据问题1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556###################################################### Data problems###################################################totS &lt;- table(ID)totP &lt;- table(Prod)nas &lt;- sales[which(is.na(Quant) &amp; is.na(Val)),c('ID','Prod')]propS &lt;- 100*table(nas$ID)/totSpropS[order(propS,decreasing=T)[1:10]]propP &lt;- 100*table(nas$Prod)/totPpropP[order(propP,decreasing=T)[1:10]]detach(sales)sales &lt;- sales[-which(is.na(sales$Quant) &amp; is.na(sales$Val)),]nnasQp &lt;- tapply(sales$Quant,list(sales$Prod), function(x) sum(is.na(x)))propNAsQp &lt;- nnasQp/table(sales$Prod)propNAsQp[order(propNAsQp,decreasing=T)[1:10]]sales &lt;- sales[!sales$Prod %in% c('p2442','p2443'),]nlevels(sales$Prod)sales$Prod &lt;- factor(sales$Prod)nlevels(sales$Prod)nnasQs &lt;- tapply(sales$Quant,list(sales$ID),function(x) sum(is.na(x)))propNAsQs &lt;- nnasQs/table(sales$ID)propNAsQs[order(propNAsQs,decreasing=T)[1:10]]nnasVp &lt;- tapply(sales$Val,list(sales$Prod), function(x) sum(is.na(x)))propNAsVp &lt;- nnasVp/table(sales$Prod)propNAsVp[order(propNAsVp,decreasing=T)[1:10]]nnasVs &lt;- tapply(sales$Val,list(sales$ID),function(x) sum(is.na(x)))propNAsVs &lt;- nnasVs/table(sales$ID)propNAsVs[order(propNAsVs,decreasing=T)[1:10]]tPrice &lt;- tapply(sales[sales$Insp != 'fraud','Uprice'], list(sales[sales$Insp != 'fraud','Prod']), median,na.rm=T)noQuant &lt;- which(is.na(sales$Quant))sales[noQuant,'Quant'] &lt;- ceiling(sales[noQuant,'Val'] / tPrice[sales[noQuant,'Prod']])noVal &lt;- which(is.na(sales$Val))sales[noVal,'Val'] &lt;- sales[noVal,'Quant'] * tPrice[sales[noVal,'Prod']]sales$Uprice &lt;- sales$Val/sales$Quantsave(sales,file='salesClean.Rdata')attach(sales)notF &lt;- which(Insp != 'fraud')ms &lt;- tapply(Uprice[notF],list(Prod=Prod[notF]),function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2])&#125;)ms &lt;- matrix(unlist(ms), length(ms),2, byrow=T,dimnames=list(names(ms),c('median','iqr')))head(ms)par(mfrow=c(1,2))plot(ms[,1],ms[,2],xlab='Median',ylab='IQR',main='')plot(ms[,1],ms[,2],xlab='Median',ylab='IQR',main='',col='grey',log=\"xy\")smalls &lt;- which(table(Prod) &lt; 20)points(log(ms[smalls,1]),log(ms[smalls,2]),pch='+') 123456789101112131415161718dms &lt;- scale(ms)smalls &lt;- which(table(Prod) &lt; 20)prods &lt;- tapply(sales$Uprice,sales$Prod,list)similar &lt;- matrix(NA,length(smalls),7,dimnames=list(names(smalls), c('Simil','ks.stat','ks.p','medP','iqrP','medS','iqrS')))for(i in seq(along=smalls)) &#123; d &lt;- scale(dms,dms[smalls[i],],FALSE) d &lt;- sqrt(drop(d^2 %*% rep(1,ncol(d)))) stat &lt;- ks.test(prods[[smalls[i]]],prods[[order(d)[2]]]) similar[i,] &lt;- c(order(d)[2],stat$statistic,stat$p.value,ms[smalls[i],], ms[order(d)[2],])&#125;head(similar)levels(Prod)[similar[1,1]]nrow(similar[similar[,'ks.p'] &gt;= 0.9,])sum(similar[,'ks.p'] &gt;= 0.9)save(similar,file='similarProducts.Rdata') 测试标准12345678910111213141516###################################################### Evaluation criteria###################################################library(ROCR)data(ROCR.simple)pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )perf &lt;- performance(pred,'prec','rec')plot(perf)PRcurve &lt;- function(preds,trues,...) &#123; require(ROCR,quietly=T) pd &lt;- prediction(preds,trues) pf &lt;- performance(pd,'prec','rec') pf@y.values &lt;- lapply(pf@y.values,function(x) rev(cummax(rev(x)))) plot(pf,...)&#125;PRcurve(ROCR.simple$predictions, ROCR.simple$labels ) 1234567891011pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )perf &lt;- performance(pred,'lift','rpp')plot(perf,main='Lift Chart')CRchart &lt;- function(preds,trues,...) &#123; require(ROCR,quietly=T) pd &lt;- prediction(preds,trues) pf &lt;- performance(pd,'rec','rpp') plot(pf,...)&#125; CRchart(ROCR.simple$predictions, ROCR.simple$labels, main='Cumulative Recall Chart') 12345678910111213141516171819202122avgNDTP &lt;- function(toInsp,train,stats) &#123; if (missing(train) &amp;&amp; missing(stats)) stop('Provide either the training data or the product stats') if (missing(stats)) &#123; notF &lt;- which(train$Insp != 'fraud') stats &lt;- tapply(train$Uprice[notF], list(Prod=train$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;) stats &lt;- matrix(unlist(stats), length(stats),2,byrow=T, dimnames=list(names(stats),c('median','iqr'))) stats[which(stats[,'iqr']==0),'iqr'] &lt;- stats[which(stats[,'iqr']==0),'median'] &#125; mdtp &lt;- mean(abs(toInsp$Uprice-stats[toInsp$Prod,'median']) / stats[toInsp$Prod,'iqr']) return(mdtp)&#125; 实验方法12345678910111213###################################################### Experimental Methodology###################################################evalOutlierRanking &lt;- function(testSet,rankOrder,Threshold,statsProds) &#123; ordTS &lt;- testSet[rankOrder,] N &lt;- nrow(testSet) nF &lt;- if (Threshold &lt; 1) as.integer(Threshold*N) else Threshold cm &lt;- table(c(rep('fraud',nF),rep('ok',N-nF)),ordTS$Insp) prec &lt;- cm['fraud','fraud']/sum(cm['fraud',]) rec &lt;- cm['fraud','fraud']/sum(cm[,'fraud']) AVGndtp &lt;- avgNDTP(ordTS[nF,],stats=statsProds) return(c(Precision=prec,Recall=rec,avgNDTP=AVGndtp))&#125; 修改的箱图规则12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455###################################################### The modified box plot rule###################################################BPrule &lt;- function(train,test) &#123; notF &lt;- which(train$Insp != 'fraud') ms &lt;- tapply(train$Uprice[notF],list(Prod=train$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;) ms &lt;- matrix(unlist(ms),length(ms),2,byrow=T, dimnames=list(names(ms),c('median','iqr'))) ms[which(ms[,'iqr']==0),'iqr'] &lt;- ms[which(ms[,'iqr']==0),'median'] ORscore &lt;- abs(test$Uprice-ms[test$Prod,'median']) / ms[test$Prod,'iqr'] return(list(rankOrder=order(ORscore,decreasing=T), rankScore=ORscore))&#125;notF &lt;- which(sales$Insp != 'fraud')globalStats &lt;- tapply(sales$Uprice[notF], list(Prod=sales$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;)globalStats &lt;- matrix(unlist(globalStats), length(globalStats),2,byrow=T, dimnames=list(names(globalStats),c('median','iqr')))globalStats[which(globalStats[,'iqr']==0),'iqr'] &lt;- globalStats[which(globalStats[,'iqr']==0),'median']ho.BPrule &lt;- function(form, train, test, ...) &#123; res &lt;- BPrule(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;bp.res &lt;- holdOut(learner('ho.BPrule', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(bp.res)par(mfrow=c(1,2))info &lt;- attr(bp.res,'itsInfo')PTs.bp &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',avg='vertical')CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',avg='vertical') 局部离群值因子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051###################################################### Local outlier factors (LOF)###################################################ho.LOF &lt;- function(form, train, test, k, ...) &#123; ntr &lt;- nrow(train) all &lt;- rbind(train,test) N &lt;- nrow(all) ups &lt;- split(all$Uprice,all$Prod) r &lt;- list(length=ups) for(u in seq(along=ups)) r[[u]] &lt;- if (NROW(ups[[u]]) &gt; 3) lofactor(ups[[u]],min(k,NROW(ups[[u]]) %/% 2)) else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) else NULL all$lof &lt;- vector(length=N) split(all$lof,all$Prod) &lt;- r all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] &lt;- SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))]) structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'], decreasing=T),...), itInfo=list(preds=all[(ntr+1):N,'lof'], trues=ifelse(test$Insp=='fraud',1,0)) )&#125;lof.res &lt;- holdOut(learner('ho.LOF', pars=list(k=7,Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(lof.res)par(mfrow=c(1,2))info &lt;- attr(lof.res,'itsInfo')PTs.lof &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')legend('topright',c('BPrule','LOF'),lty=c(1,2))CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')legend('bottomright',c('BPrule','LOF'),lty=c(1,2)) 基于聚类的离群值排名123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960###################################################### Clustering-based outlier rankings (OR_h)###################################################ho.ORh &lt;- function(form, train, test, ...) &#123; require(dprep,quietly=T) ntr &lt;- nrow(train) all &lt;- rbind(train,test) N &lt;- nrow(all) ups &lt;- split(all$Uprice,all$Prod) r &lt;- list(length=ups) for(u in seq(along=ups)) r[[u]] &lt;- if (NROW(ups[[u]]) &gt; 3) outliers.ranking(ups[[u]])$prob.outliers else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) else NULL all$lof &lt;- vector(length=N) split(all$lof,all$Prod) &lt;- r all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] &lt;- SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))]) structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'], decreasing=T),...), itInfo=list(preds=all[(ntr+1):N,'lof'], trues=ifelse(test$Insp=='fraud',1,0)) )&#125;orh.res &lt;- holdOut(learner('ho.ORh', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(orh.res)par(mfrow=c(1,2))info &lt;- attr(orh.res,'itsInfo')PTs.orh &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('BPrule','LOF','ORh'), lty=c(1,2,1),col=c('black','black','grey'))CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical')legend('bottomright',c('BPrule','LOF','ORh'), lty=c(1,2,1),col=c('black','black','grey')) 类失衡问题1234567891011###################################################### The class imbalance problem###################################################data(iris)data &lt;- iris[,c(1,2,5)]data$Species &lt;- factor(ifelse(data$Species == 'setosa','rare','common'))newData &lt;- SMOTE(Species ~ .,data,perc.over=600)table(newData$Species)par(mfrow=c(1,2))plot(data[,1],data[,2],pch=19+as.integer(data[,3]),main='Original Data')plot(newData[,1],newData[,2],pch=19+as.integer(newData[,3]),main=\"SMOTE'd Data\") 简单贝叶斯方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051###################################################### Naive Bayes###################################################nb &lt;- function(train,test) &#123; require(e1071,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) model &lt;- naiveBayes(Insp ~ .,data) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nb &lt;- function(form, train, test, ...) &#123; res &lt;- nb(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nb.res &lt;- holdOut(learner('ho.nb', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nb.res)par(mfrow=c(1,2))info &lt;- attr(nb.res,'itsInfo')PTs.nb &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('NaiveBayes','ORh'), lty=1,col=c('black','grey'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('bottomright',c('NaiveBayes','ORh'), lty=1,col=c('black','grey')) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455nb.s &lt;- function(train,test) &#123; require(e1071,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) newData &lt;- SMOTE(Insp ~ .,data,perc.over=700) model &lt;- naiveBayes(Insp ~ .,newData) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nbs &lt;- function(form, train, test, ...) &#123; res &lt;- nb.s(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nbs.res &lt;- holdOut(learner('ho.nbs', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nbs.res)par(mfrow=c(1,2))info &lt;- attr(nbs.res,'itsInfo')PTs.nbs &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.nbs[,,1],PTs.nbs[,,2], add=T,lty=2, avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('NaiveBayes','smoteNaiveBayes','ORh'), lty=c(1,2,1),col=c('black','black','grey'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.nbs[,,1],PTs.nbs[,,2], add=T,lty=2, avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('bottomright',c('NaiveBayes','smoteNaiveBayes','ORh'), lty=c(1,2,1),col=c('black','black','grey')) AdaBoost方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970###################################################### AdaBoost###################################################library(RWeka)WOW(AdaBoostM1)data(iris)idx &lt;- sample(150,100)model &lt;- AdaBoostM1(Species ~ .,iris[idx,], control=Weka_control(I=100))preds &lt;- predict(model,iris[-idx,])head(preds)table(preds,iris[-idx,'Species'])prob.preds &lt;- predict(model,iris[-idx,],type='probability')head(prob.preds)ab &lt;- function(train,test) &#123; require(RWeka,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) model &lt;- AdaBoostM1(Insp ~ .,data, control=Weka_control(I=100)) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='probability') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.ab &lt;- function(form, train, test, ...) &#123; res &lt;- ab(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;ab.res &lt;- holdOut(learner('ho.ab', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(ab.res)par(mfrow=c(1,2))info &lt;- attr(ab.res,'itsInfo')PTs.ab &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.ab[,,1],PTs.ab[,,2], add=T,lty=2, avg='vertical') legend('topright',c('NaiveBayes','ORh','AdaBoostM1'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.ab[,,1],PTs.ab[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('NaiveBayes','ORh','AdaBoostM1'), lty=c(1,1,2),col=c('black','grey','black')) 半监督方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485###################################################### Semi-supervised approaches###################################################set.seed(1234) # Just to ensrure you get the same results as in the booklibrary(DMwR)library(e1071)data(iris)idx &lt;- sample(150,100)tr &lt;- iris[idx,]ts &lt;- iris[-idx,]nb &lt;- naiveBayes(Species ~ .,tr)table(predict(nb,ts),ts$Species)trST &lt;- trnas &lt;- sample(100,90)trST[nas,'Species'] &lt;- NAfunc &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='raw') data.frame(cl=colnames(p)[apply(p,1,which.max)],p=apply(p,1,max))&#125;nbSTbase &lt;- naiveBayes(Species ~ .,trST[-nas,])table(predict(nbSTbase,ts),ts$Species)nbST &lt;- SelfTrain(Species ~ .,trST,learner('naiveBayes',list()),'func')table(predict(nbST,ts),ts$Species)pred.nb &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='raw') data.frame(cl=colnames(p)[apply(p,1,which.max)], p=apply(p,1,max) )&#125;nb.st &lt;- function(train,test) &#123; require(e1071,quietly=T) train &lt;- train[,c('ID','Prod','Uprice','Insp')] train[which(train$Insp == 'unkn'),'Insp'] &lt;- NA train$Insp &lt;- factor(train$Insp,levels=c('ok','fraud')) model &lt;- SelfTrain(Insp ~ .,train, learner('naiveBayes',list()),'pred.nb') preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nb.st &lt;- function(form, train, test, ...) &#123; res &lt;- nb.st(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nb.st.res &lt;- holdOut(learner('ho.nb.st', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nb.st.res)par(mfrow=c(1,2))info &lt;- attr(nb.st.res,'itsInfo')PTs.nb.st &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.nb.st[,,1],PTs.nb.st[,,2], add=T,lty=2, avg='vertical') legend('topright',c('NaiveBayes','ORh','NaiveBayes-ST'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.nb.st[,,1],PTs.nb.st[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('NaiveBayes','ORh','NaiveBayes-ST'), lty=c(1,1,2),col=c('black','grey','black')) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364pred.ada &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='probability') data.frame(cl=colnames(p)[apply(p,1,which.max)], p=apply(p,1,max) )&#125;ab.st &lt;- function(train,test) &#123; require(RWeka,quietly=T) train &lt;- train[,c('ID','Prod','Uprice','Insp')] train[which(train$Insp == 'unkn'),'Insp'] &lt;- NA train$Insp &lt;- factor(train$Insp,levels=c('ok','fraud')) model &lt;- SelfTrain(Insp ~ .,train, learner('AdaBoostM1', list(control=Weka_control(I=100))), 'pred.ada') preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='probability') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.ab.st &lt;- function(form, train, test, ...) &#123; res &lt;- ab.st(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;ab.st.res &lt;- holdOut(learner('ho.ab.st', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(ab.st.res)par(mfrow=c(1,2))info &lt;- attr(ab.st.res,'itsInfo')PTs.ab.st &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.ab[,,1],PTs.ab[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.ab.st[,,1],PTs.ab.st[,,2], add=T,lty=2, avg='vertical') legend('topright',c('AdaBoostM1','ORh','AdaBoostM1-ST'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.ab[,,1],PTs.ab[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.ab.st[,,1],PTs.ab.st[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('AdaBoostM1','ORh','AdaBoostM1-ST'), lty=c(1,1,2),col=c('black','grey','black'))","tags":[{"name":"聚类","slug":"聚类","permalink":"http://weibo.com/youthcolor/tags/聚类/"},{"name":"欺诈侦测","slug":"欺诈侦测","permalink":"http://weibo.com/youthcolor/tags/欺诈侦测/"},{"name":"半监督分类","slug":"半监督分类","permalink":"http://weibo.com/youthcolor/tags/半监督分类/"},{"name":"类失衡","slug":"类失衡","permalink":"http://weibo.com/youthcolor/tags/类失衡/"},{"name":"贝叶斯分类器","slug":"贝叶斯分类器","permalink":"http://weibo.com/youthcolor/tags/贝叶斯分类器/"},{"name":"AdaBoost分类器","slug":"AdaBoost分类器","permalink":"http://weibo.com/youthcolor/tags/AdaBoost分类器/"},{"name":"决策精确度","slug":"决策精确度","permalink":"http://weibo.com/youthcolor/tags/决策精确度/"},{"name":"回溯精确度","slug":"回溯精确度","permalink":"http://weibo.com/youthcolor/tags/回溯精确度/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://weibo.com/youthcolor/tags/交叉验证/"},{"name":"离群值侦测排序","slug":"离群值侦测排序","permalink":"http://weibo.com/youthcolor/tags/离群值侦测排序/"}]},{"title":"实战Caret包对借贷数据进行机器学习预测","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/实战Caret包对借贷数据进行机器学习预测/","text":"机器学习面临的最大挑战之一该选择什么样的学习算法。在R中，不同的算法会有不同的语法、不同的参数调优和不同的数据格式，因此，对于初学者来说，显得比较吃力。 那么，你如何从一个初学者转变为一个数据科学家，建立数百个模型并把它们堆在一起?当然没有捷径可走，但我想不见得非得把数百个机器学习模型都应用一遍，记住每个算法的不同包名，应用每个算法的语法，为每个算法优化参数等。 Caret(分类和回归训练)包，这可能是R中最大的项目。这个包是我们需要知道的，就像要知道Python机器学习中的Scikit-Learn包一样，它几乎可以解决任何一个受监督的机器学习问题。它为几种机器学习算法提供了统一的接口，并标准化了各种各样的任务，如数据分割、预处理、特性选择、变量重要性评估等。今天，我们将探索研究借贷数据预测问题，展示下Caret包的强大功能。PS：尽管Caret确实简化了很多机器学习工作，但是也没必要在它身上投入太多精力。 目录 加载数据 数据预处理 数据分割 特征选择 训练模型 参数调优 参数重要性评估 结果预测 1. 加载数据1234567891011121314151617181920212223# install.packages(\"caret\", dependencies = c(\"Depends\", \"Suggests\"))#Loading caret packagelibrary(\"caret\")#Loading training datatrain&lt;-read.csv(\"C:\\\\Users\\\\Administrator\\\\Desktop\\\\loan\\\\train_u6lujuX_CVtuZ9i.csv\", stringsAsFactors = T)#Looking at the structure of caret package.str(train)head(rain)Loan_ID Gender Married Dependents Education Self_Employed ApplicantIncome CoapplicantIncome1 LP001002 Male No 0 Graduate No 5849 02 LP001003 Male Yes 1 Graduate No 4583 15083 LP001005 Male Yes 0 Graduate Yes 3000 04 LP001006 Male Yes 0 Not Graduate No 2583 23585 LP001008 Male No 0 Graduate No 6000 06 LP001011 Male Yes 2 Graduate Yes 5417 4196 LoanAmount Loan_Amount_Term Credit_History Property_Area Loan_Status1 NA 360 1 Urban Y2 128 360 1 Rural N3 66 360 1 Urban Y4 120 360 1 Urban Y5 141 360 1 Urban Y6 267 360 1 Urban Y 2. 数据预处理在建模之前，通常有个数据预处理，先来看看有没有缺失值。12sum(is.na(train))[1] 86 接下来，我们用KNN算法给数据中的缺失值预测并赋值，在这个过程中，对数据进行了标准化操作。123456#Imputing missing values using KNN.Also centering and scaling numerical columnspreProcValues &lt;- preProcess(train, method = c(\"knnImpute\",\"center\",\"scale\"))library('RANN')train_processed &lt;- predict(preProcValues, train)sum(is.na(train_processed))[1] 0 使用dummyVars函数将分类变量编码为哑变量。123456789101112131415161718192021222324252627282930#Converting outcome variable to numerictrain_processed$Loan_Status&lt;-ifelse(train_processed$Loan_Status=='N',0,1)id&lt;-train_processed$Loan_IDtrain_processed$Loan_ID&lt;-NULL#Converting every categorical variable to numerical using dummy variablesdmy &lt;- dummyVars(\" ~ .\", data = train_processed,fullRank = T)train_transformed &lt;- data.frame(predict(dmy, newdata = train_processed))str(train_transformed)'data.frame': 614 obs. of 19 variables: $ Gender.Female : num 0 0 0 0 0 0 0 0 0 0 ... $ Gender.Male : num 1 1 1 1 1 1 1 1 1 1 ... $ Married.No : num 1 0 0 0 1 0 0 0 0 0 ... $ Married.Yes : num 0 1 1 1 0 1 1 1 1 1 ... $ Dependents.0 : num 1 0 1 1 1 0 1 0 0 0 ... $ Dependents.1 : num 0 1 0 0 0 0 0 0 0 1 ... $ Dependents.2 : num 0 0 0 0 0 1 0 0 1 0 ... $ Dependents.3. : num 0 0 0 0 0 0 0 1 0 0 ... $ Education.Not.Graduate : num 0 0 0 1 0 0 1 0 0 0 ... $ Self_Employed.No : num 1 1 0 1 1 0 1 1 1 1 ... $ Self_Employed.Yes : num 0 0 1 0 0 1 0 0 0 0 ... $ ApplicantIncome : num 0.0729 -0.1343 -0.3934 -0.4617 0.0976 ... $ CoapplicantIncome : num -0.554 -0.0387 -0.554 0.2518 -0.554 ... $ LoanAmount : num 0.0162 -0.2151 -0.9395 -0.3086 -0.0632 ... $ Loan_Amount_Term : num 0.276 0.276 0.276 0.276 0.276 ... $ Credit_History : num 0.432 0.432 0.432 0.432 0.432 ... $ Property_Area.Semiurban: num 0 0 0 0 0 0 0 1 0 1 ... $ Property_Area.Urban : num 1 0 1 1 1 1 1 0 1 0 ... $ Loan_Status : num 1 0 1 1 1 1 1 0 1 0 ...#Converting the dependent variable back to categoricaltrain_transformed$Loan_Status&lt;-as.factor(train_transformed$Loan_Status) 3. 数据分割为了防止过度拟合，我们以因变量为准将数据分成两份，便于后续的交叉验证。123456#Spliting training set into two parts based on outcome: 75% and 25%index &lt;- createDataPartition(train_transformed$Loan_Status, p=0.75, list=FALSE)trainSet &lt;- train_transformed[ index,]testSet &lt;- train_transformed[-index,]#Checking the structure of trainSetstr(trainSet) 4. 特征选择特性选择是建模的极其重要部分，我们将使用递归特性消除方法，以找到用于建模的最佳特性子集。12345678910111213141516171819202122#Feature selection using rfe in caretcontrol &lt;- rfeControl(functions = rfFuncs, method = \"repeatedcv\", repeats = 3, verbose = FALSE)outcomeName&lt;-'Loan_Status'predictors&lt;-names(trainSet)[!names(trainSet) %in% outcomeName]Loan_Pred_Profile &lt;- rfe(trainSet[,predictors], trainSet[,outcomeName], rfeControl = control)Loan_Pred_ProfileRecursive feature selectionOuter resampling method: Cross-Validated (10 fold, repeated 3 times) Resampling performance over subset size: Variables Accuracy Kappa AccuracySD KappaSD Selected 4 0.8027 0.4843 0.04251 0.1198 8 0.8122 0.5021 0.04127 0.1210 16 0.8106 0.5029 0.03919 0.1147 18 0.8142 0.5097 0.03693 0.1098 *The top 5 variables (out of 18): Credit_History, Property_Area.Semiurban, CoapplicantIncome, ApplicantIncome, LoanAmount#Taking only the top 5 predictorspredictors&lt;-c(\"Credit_History\", \"LoanAmount\", \"Loan_Amount_Term\", \"ApplicantIncome\", \"CoapplicantIncome\") 5. 训练模型这可能是Caret从其他任何可用的方案中能脱颖而出的原因，它提供了使用一致的语法实现200多个机器学习算法的能力。如果想查看Caret支持的所有算法的列表，你可以使用names(getModelInfo())。12345678910names(getModelInfo())[1] \"ada\" \"AdaBag\" \"AdaBoost.M1\" \"adaboost\" [5] \"amdai\" \"ANFIS\" \"avNNet\" \"awnb\" [9] \"awtan\" \"bag\" \"bagEarth\" \"bagEarthGCV\" [13] \"bagFDA\" \"bagFDAGCV\" \"bam\" \"bartMachine\" ... [225] \"svmSpectrumString\" \"tan\" \"tanSearch\" \"treebag\" [229] \"vbmpRadial\" \"vglmAdjCat\" \"vglmContRatio\" \"vglmCumulative\" [233] \"widekernelpls\" \"WM\" \"wsrf\" \"xgbLinear\" [237] \"xgbTree\" \"xyf\" 我们可以简单地应用大量具有相似语法的算法。例如，应用GBM、随机森林、神经网络、逻辑回归贝叶斯、XGBTree和支持向量机。1234567model_gbm&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm')model_rf&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='rf')model_nnet&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='nnet')model_glm&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='glm')model_nb&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='nb')model_xgbTree&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='xgbTree')model_svmRadial&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='svmRadial') 您可以使用参数调优技术进一步优化所有这些算法中的参数。 6. 参数调优使用Caret优化参数是非常容易的。通常，Caret中的参数调优工作如下: 几乎调优过程中的每一个步骤都是可以定制的。在默认情况下使用一组参数来评估模型性能的重新采样技术是引导程序，但是它提供了使用k-交叉、重复k-交叉以及指定可选交叉验证(LOOCV)的替代方法。在本例中，我们将使用5倍的交叉验证重复5次。1234fitControl &lt;- trainControl( method = \"repeatedcv\", number = 5, repeats = 5) 如果没有定义参数的搜索空间，那么Caret将使用每个可调参数的3个随机值，并使用交叉验证结果来查找该算法的最佳参数集。除此之外，还有两种调优参数的方法： 6.1 使用tuneGrid要找到可以调优的模型的参数，您可以使用1234567891011121314151617181920212223242526272829303132333435modelLookup(model='gbm')model parameter label forReg forClass probModel1 gbm n.trees # Boosting Iterations TRUE TRUE TRUE2 gbm interaction.depth Max Tree Depth TRUE TRUE TRUE3 gbm shrinkage Shrinkage TRUE TRUE TRUE4 gbm n.minobsinnode Min. Terminal Node Size TRUE TRUE TRUE#Creating gridgrid &lt;- expand.grid(n.trees=c(10,20,50,100,500,1000),shrinkage=c(0.01,0.05,0.1,0.5),n.minobsinnode = c(3,5,10),interaction.depth=c(1,5,10))# training the modelmodel_gbm&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneGrid=grid)# summarizing the modelprint(model_gbm)Stochastic Gradient Boosting 461 samples 5 predictor 2 classes: '0', '1' No pre-processingResampling: Cross-Validated (5 fold, repeated 5 times) Summary of sample sizes: 368, 369, 370, 368, 369, 369, ... Resampling results across tuning parameters: shrinkage interaction.depth n.minobsinnode n.trees Accuracy 0.01 1 3 10 0.6876416 0.01 1 3 20 0.6876416 0.01 1 3 50 0.8243466 0.01 1 3 100 0.8243466 0.01 1 3 500 0.8213125 0.01 1 3 1000 0.8178246 0.01 1 5 10 0.6876416 0.01 1 5 20 0.6876416 ... [ reached getOption(\"max.print\") -- omitted 50 rows ]Accuracy was used to select the optimal model using the largest value.The final values used for the model were n.trees = 20, interaction.depth = 1, shrinkage = 0.1 and n.minobsinnode = 3.plot(model_gbm) 对于在扩展.grid()中列出的所有参数组合，将使用交叉验证创建和测试一个模型。使用最好的交叉验证性能的参数集将被用来创建最终的模型。 6.2. 使用tuneLength与为每个调优参数指定精确值相反，我们可以简单地要求它通过tuneLength为每个调优参数使用任意数量的可能值。让我们尝试一个使用tuneLength=10的例子。1234567891011#using tune lengthmodel_gbm&lt;-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneLength=10)print(model_gbm)...Tuning parameter 'shrinkage' was held constant at a value of 0.1Tuning parameter 'n.minobsinnode' was held constant at a value of 10Accuracy was used to select the optimal model using the largest value.The final values used for the model were n.trees = 50, interaction.depth = 1, shrinkage = 0.1 and n.minobsinnode = 10.plot(model_gbm) 这里，保持shrinkage和n.minobsinnode参数不变。n.trees和interaction.depth改变10个值，并使用最好的组合来训练最终模型。 7. 变量重要性Caret还通过使用varImp()对任何模型进行变量的评估。让我们来看看我们创建的四个模型的变量的重要性。12345#Checking variable importance for GBM#Variable ImportancevarImp(object=model_gbm)#Plotting Varianle importance for GBMplot(varImp(object=model_gbm),main=\"GBM - Variable Importance\") 1234#Checking variable importance for RFvarImp(object=model_rf)#Plotting Varianle importance for Random Forestplot(varImp(object=model_rf),main=\"RF - Variable Importance\") 1234#Checking variable importance for NNETvarImp(object=model_nnet)#Plotting Variable importance for Neural Networkplot(varImp(object=model_nnet),main=\"NNET - Variable Importance\") 1234#Checking variable importance for GLMvarImp(object=model_glm)#Plotting Variable importance for GLMplot(varImp(object=model_glm),main=\"GLM - Variable Importance\") 很明显，不同模型的变量重要性是不同的。从不同的模型中得到的变量重要性的两个主要用途是: 对大多数模型来说重要的预测因子代表着真正重要的预测因子。 组合预测中，我们应该使用那些具有显著不同重要性的模型进行预测，因为他们的预测也会有所不同。但必须确保所有这些都是足够准确的。8. 结果预测为了预测测试集的相关变量，Caret提供了predict.train()函数。您需要指定模型名称、测试数据。对于分类问题，Caret还提供了另一个名为type的特性，可以设置为prob或raw。对于类型raw，预测只是测试数据的结果类，而对于类型prob，它将给出在不同类型的结果变量中出现的每个观察结果的概率。 让我们来看看我们的GBM模型的预测:1234567891011121314151617181920212223#Predictionspredictions&lt;-predict.train(object=model_gbm,testSet[,predictors],type=\"raw\")confusionMatrix(predictions,testSet[,outcomeName])Confusion Matrix and Statistics ReferencePrediction 0 1 0 14 2 1 34 103 Accuracy : 0.7647 95% CI : (0.6894, 0.8294) No Information Rate : 0.6863 P-Value [Acc &gt; NIR] : 0.02055 Kappa : 0.3328 Mcnemar's Test P-Value : 2.383e-07 Sensitivity : 0.2917 Specificity : 0.9810 Pos Pred Value : 0.8750 Neg Pred Value : 0.7518 Prevalence : 0.3137 Detection Rate : 0.0915 Detection Prevalence : 0.1046 Balanced Accuracy : 0.6363 'Positive' Class : 0 RF的预测：1234567891011121314151617181920212223#Predictionspredictions&lt;-predict.train(object=model_rf,testSet[,predictors],type=\"raw\")confusionMatrix(predictions,testSet[,outcomeName])Confusion Matrix and Statistics ReferencePrediction 0 1 0 17 6 1 31 99 Accuracy : 0.7582 95% CI : (0.6824, 0.8237) No Information Rate : 0.6863 P-Value [Acc &gt; NIR] : 0.0315 Kappa : 0.3459 Mcnemar's Test P-Value : 7.961e-05 Sensitivity : 0.3542 Specificity : 0.9429 Pos Pred Value : 0.7391 Neg Pred Value : 0.7615 Prevalence : 0.3137 Detection Rate : 0.1111 Detection Prevalence : 0.1503 Balanced Accuracy : 0.6485 'Positive' Class : 0 NNET的预测：1234567891011121314151617181920212223#Predictionspredictions&lt;-predict.train(object=model_nnet,testSet[,predictors],type=\"raw\")confusionMatrix(predictions,testSet[,outcomeName])Confusion Matrix and Statistics ReferencePrediction 0 1 0 14 2 1 34 103 Accuracy : 0.7647 95% CI : (0.6894, 0.8294) No Information Rate : 0.6863 P-Value [Acc &gt; NIR] : 0.02055 Kappa : 0.3328 Mcnemar's Test P-Value : 2.383e-07 Sensitivity : 0.2917 Specificity : 0.9810 Pos Pred Value : 0.8750 Neg Pred Value : 0.7518 Prevalence : 0.3137 Detection Rate : 0.0915 Detection Prevalence : 0.1046 Balanced Accuracy : 0.6363 'Positive' Class : 0 GLM的预测：1234567891011121314151617181920212223#Predictionspredictions&lt;-predict.train(object=model_glm,testSet[,predictors],type=\"raw\")confusionMatrix(predictions,testSet[,outcomeName])Confusion Matrix and Statistics ReferencePrediction 0 1 0 14 2 1 34 103 Accuracy : 0.7647 95% CI : (0.6894, 0.8294) No Information Rate : 0.6863 P-Value [Acc &gt; NIR] : 0.02055 Kappa : 0.3328 Mcnemar's Test P-Value : 2.383e-07 Sensitivity : 0.2917 Specificity : 0.9810 Pos Pred Value : 0.8750 Neg Pred Value : 0.7518 Prevalence : 0.3137 Detection Rate : 0.0915 Detection Prevalence : 0.1046 Balanced Accuracy : 0.6363 'Positive' Class : 0","tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://weibo.com/youthcolor/tags/机器学习/"},{"name":"Caret预测","slug":"Caret预测","permalink":"http://weibo.com/youthcolor/tags/Caret预测/"},{"name":"数据预处理","slug":"数据预处理","permalink":"http://weibo.com/youthcolor/tags/数据预处理/"},{"name":"数据分割","slug":"数据分割","permalink":"http://weibo.com/youthcolor/tags/数据分割/"},{"name":"特征选择","slug":"特征选择","permalink":"http://weibo.com/youthcolor/tags/特征选择/"},{"name":"参数调优","slug":"参数调优","permalink":"http://weibo.com/youthcolor/tags/参数调优/"},{"name":"变量重要性评估","slug":"变量重要性评估","permalink":"http://weibo.com/youthcolor/tags/变量重要性评估/"}]},{"title":"购物篮关联规则分析","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/购物篮关联规则分析/","text":"我们生活在一个快速变化的数字世界。在当今时代，亚马逊，淘宝，京东等电商都在购物推荐算法方面做了不同的工作。 这给零售商带来了一个有趣的机遇和挑战。如果你能告诉顾客他们想要买什么，它不仅能提高你的销售量，还能提高顾客体验，最终提高产品价值。 另一方面，如果您无法预测下一次客户将要购买什么，客户可能不会返回您的商店。 在这篇文章中，我们将学习关联规则算法。 数据准备12345678910111213141516171819202122232425262728293031323334353637# install.packages(&quot;pacman&quot;)pacman::p_load(PACKAGE_NAME)pacman::p_load(arules, arulesViz)library(arules)library(arulesViz)data(&quot;Groceries&quot;)# transactions类型str(Groceries)Formal class &apos;transactions&apos; [package &quot;arules&quot;] with 3 slots ..@ data :Formal class &apos;ngCMatrix&apos; [package &quot;Matrix&quot;] with 5 slots .. .. ..@ i : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ... .. .. ..@ p : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ... .. .. ..@ Dim : int [1:2] 169 9835 .. .. ..@ Dimnames:List of 2 .. .. .. ..$ : NULL .. .. .. ..$ : NULL .. .. ..@ factors : list() ..@ itemInfo :&apos;data.frame&apos;: 169 obs. of 3 variables: .. ..$ labels: chr [1:169] &quot;frankfurter&quot; &quot;sausage&quot; &quot;liver loaf&quot; &quot;ham&quot; ... .. ..$ level2: Factor w/ 55 levels &quot;baby food&quot;,&quot;bags&quot;,..: 44 44 44 44 44 44 44 42 42 41 ... .. ..$ level1: Factor w/ 10 levels &quot;canned food&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... ..@ itemsetInfo:&apos;data.frame&apos;: 0 obs. of 0 variables# 查看数据head(Groceries@itemInfo, n=12) labels level2 level11 frankfurter sausage meat and sausage2 sausage sausage meat and sausage3 liver loaf sausage meat and sausage4 ham sausage meat and sausage5 meat sausage meat and sausage6 finished products sausage meat and sausage7 organic sausage sausage meat and sausage8 chicken poultry meat and sausage9 turkey poultry meat and sausage10 pork pork meat and sausage11 beef beef meat and sausage12 hamburger meat beef meat and sausage 自定义数据transactions类型转换123456789101112131415161718192021222324252627282930313233343536data &lt;- list( c(\"a\",\"b\",\"c\"), c(\"a\",\"b\"), c(\"a\",\"b\",\"d\"), c(\"b\",\"e\"), c(\"b\",\"c\",\"e\"), c(\"a\",\"d\",\"e\"), c(\"a\",\"c\"), c(\"a\",\"b\",\"d\"), c(\"c\",\"e\"), c(\"a\",\"b\",\"d\",\"e\"), c(\"a\",'b','e','c'))data &lt;- as(data, \"transactions\")inspect(data) items [1] &#123;a,b,c&#125; [2] &#123;a,b&#125; [3] &#123;a,b,d&#125; [4] &#123;b,e&#125; [5] &#123;b,c,e&#125; [6] &#123;a,d,e&#125; [7] &#123;a,c&#125; [8] &#123;a,b,d&#125; [9] &#123;c,e&#125; [10] &#123;a,b,d,e&#125;[11] &#123;a,b,c,e&#125;# Convert transactions to transaction ID liststl &lt;- as(data, \"tidLists\")inspect(tl) items transationIDs 1 a &#123;1,2,3,6,7,8,10,11&#125;2 b &#123;1,2,3,4,5,8,10,11&#125;3 c &#123;1,5,7,9,11&#125; 4 d &#123;3,6,8,10&#125; 5 e &#123;4,5,6,9,10,11&#125; summary函数查看购买频次最高的商品1234567891011121314151617181920212223242526summary(Groceries)transactions as itemMatrix in sparse format with 9835 rows (elements/itemsets/transactions) and 169 columns (items) and a density of 0.02609146 most frequent items: whole milk other vegetables rolls/buns soda yogurt 2513 1903 1809 1715 1372 (Other) 34055 element (itemset/transaction) length distribution:sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 46 29 18 19 20 21 22 23 24 26 27 28 29 32 14 14 9 11 4 6 1 1 1 1 3 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information - examples: labels level2 level11 frankfurter sausage meat and sausage2 sausage sausage meat and sausage3 liver loaf sausage meat and sausage 实现关联规则算法123456789101112131415161718192021222324rules &lt;- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.80))inspect(rules[1:10]) lhs rhs support [1] &#123;liquor,red/blush wine&#125; =&gt; &#123;bottled beer&#125; 0.001931876[2] &#123;curd,cereals&#125; =&gt; &#123;whole milk&#125; 0.001016777[3] &#123;yogurt,cereals&#125; =&gt; &#123;whole milk&#125; 0.001728521[4] &#123;butter,jam&#125; =&gt; &#123;whole milk&#125; 0.001016777[5] &#123;soups,bottled beer&#125; =&gt; &#123;whole milk&#125; 0.001118454[6] &#123;napkins,house keeping products&#125; =&gt; &#123;whole milk&#125; 0.001321810[7] &#123;whipped/sour cream,house keeping products&#125; =&gt; &#123;whole milk&#125; 0.001220132[8] &#123;pastry,sweet spreads&#125; =&gt; &#123;whole milk&#125; 0.001016777[9] &#123;turkey,curd&#125; =&gt; &#123;other vegetables&#125; 0.001220132[10] &#123;rice,sugar&#125; =&gt; &#123;whole milk&#125; 0.001220132 confidence lift [1] 0.9047619 11.235269[2] 0.9090909 3.557863[3] 0.8095238 3.168192[4] 0.8333333 3.261374[5] 0.9166667 3.587512[6] 0.8125000 3.179840[7] 0.9230769 3.612599[8] 0.9090909 3.557863[9] 0.8000000 4.134524[10] 1.0000000 3.913649 频率直方图1234library(RColorBrewer)arules::itemFrequencyPlot(Groceries,topN=20,col=brewer.pal(8,'Pastel2'), main='Relative Item Frequency Plot', type=\"relative\",ylab=\"Item Frequency (Relative)\") 支持度和提升度的图形表示1plot(rules[1:20], method = \"graph\", control = list(type = \"items\")) 平行坐标系1plot(rules[1:20], method = \"paracoord\", control = list(reorder = TRUE)) 矩阵表示1plot(rules[1:20], method = \"matrix\", control = list(reorder = TRUE)) 互动式散点图1arulesViz::plotly_arules(rules)","tags":[{"name":"购物篮","slug":"购物篮","permalink":"http://weibo.com/youthcolor/tags/购物篮/"},{"name":"关联规则","slug":"关联规则","permalink":"http://weibo.com/youthcolor/tags/关联规则/"},{"name":"推荐算法","slug":"推荐算法","permalink":"http://weibo.com/youthcolor/tags/推荐算法/"}]},{"title":"译：机器学习算法要点（附Python和R代码）","date":"2018-08-22T03:33:23.214Z","path":"2018/08/22/译：机器学习算法要点（附Python和R代码）/","text":"最近一直忙于琐事，无力抽身学习，现抽出两天点滴时间，粗略翻译了一篇机器学习算法要点（附Python和R代码），以便需要之时查阅。此篇文章并未涉及深度学习、迁移学习、对抗学习等内容，待后续补充。 “谷歌的自动驾驶汽车和机器人受到了很多媒体的关注，但该公司真正的未来在于机器学习，这是一种让计算机变得更智能、更个性化的技术。” ——埃里克.施密特(谷歌董事会主席)我们可能生活在人类历史上最具决定性的时期。计算从大型主机到pc再到云计算的时代。但它的定义并不是发生了什么，而是未来几年将会发生什么。 对于像我这样的人来说，这段时期令人兴奋的是工具和技术的民主化，这是随着计算机技术的发展而来的。今天，作为一名数据科学家，我可以用复杂的算法构建数据处理机器，每小时几美元。但是，做到这些并不容易！如果我没有没日没夜的付出。 谁将从这份指南获益匪浅创建这个指南的目的是为了简化世界各地有抱负的数据科学家和机器学习爱好者的学习路径。通过本指南，我将使您能够处理机器学习问题并从经验中获益。我对各种机器学习算法以及R和Python代码进行了高水平的理解，并运行它们。这些应该足够让你的手忙碌了。 我故意跳过了这些技术背后的统计知识，因为您在开始时不需要了解它们。所以，如果你在寻找对这些算法的统计理解，你应该去别处看看。但是，如果你想要装备自己来开始建造机器学习项目，你完全可以信任它。 总的来说，有三种类型的机器学习算法 监督学习 它是如何工作的:这个算法由一个变量/因变量(或依赖变量)组成，这个变量是由一个给定的预测集(独立变量)来预测的。使用这组变量，我们生成一个将输入映射到所需输出的函数。培训过程将继续，直到模型达到训练数据所需的精确程度。监督学习的例子:线性回归、决策树、随机森林、最近邻（KNN)、逻辑回归等。 无监督学习 它是如何工作的:在这个算法中，我们没有任何目标或结果变量来预测/估计。它适用于不同组的聚类，广泛用于细分不同群体，以进行特定的干预。无监督学习的例子:关联算法，k-均值。 强化学习 它是如何工作的:使用这个算法，机器被训练来做出具体的决定。它的工作原理是这样的:机器被暴露在一个环境中，在这个环境中，它不断地通过试错来训练自己。这台机器从过去的经验中学习，并试图获取最好的知识以做出准确的决策。强化学习的例子:马尔科夫决策过程。 一般机器学习算法列表 线性回归 逻辑回归 决策树 支持向量机 朴素贝叶斯 最近邻（KNN） K-均值 随机森林 降维算法 梯度下降算法 GBM XGBoost LightGBM CatBoost 1.线性回归它用于估计基于连续变量(s)的实际值(房子的成本，电话的数量，总销售额等)。在这里，我们通过拟合最佳直线，建立了独立变量和依赖变量之间的关系。这条最佳拟合线被称为回归线，由线性方程Y=aX+b表示。 理解线性回归最好的方法就是重温童年的经历。比如说，你让一个五年级的孩子通过体重递增方式排列他的同学，而不问他们的具体重量！你认为孩子会做什么?他/她可能会(在视觉上分析)身高和身材，并利用这些可见的参数来排列他们。这是现实生活中的线性回归！这个孩子实际上已经知道了身高与体重之间的关联，这看起来就像上面的等式。 在这个方程中: Y -因变量 a -斜率 X -独立变量 b -截距 这些系数a和b是基于最小化数据点和回归线之间距离的平方差的。 请看下面的例子。这里我们已经确定了线性方程y=0.2811x+13.9的最佳拟合线。利用这个方程，我们可以计算出一个人的身高。 线性回归主要有两种类型:一元线性回归和多元线性回归。一元线性回归具有一个独立变量的特征。并且，多元线性回归(如名字所示)具有多个(2个以上)独立变量的特征。在寻找最佳拟合线时，你可以选择一个多项式或曲线回归。这些被称为多项式或曲线线性回归。 Python Code 123456789101112131415161718#Import Library#Import other necessary libraries like pandas, numpy...from sklearn import linear_model#Load Train and Test datasets#Identify feature and response variable(s) and values must be numeric and numpy arraysx_train=input_variables_values_training_datasetsy_train=target_variables_values_training_datasetsx_test=input_variables_values_test_datasets# Create linear regression objectlinear = linear_model.LinearRegression()# Train the model using the training sets and check scorelinear.fit(x_train, y_train)linear.score(x_train, y_train)#Equation coefficient and Interceptprint('Coefficient: \\n', linear.coef_)print('Intercept: \\n', linear.intercept_)#Predict Outputpredicted= linear.predict(x_test) R Code 1234567891011#Load Train and Test datasets#Identify feature and response variable(s) and values must be numeric and numpy arraysx_train &lt;- input_variables_values_training_datasetsy_train &lt;- target_variables_values_training_datasetsx_test &lt;- input_variables_values_test_datasetsx &lt;- cbind(x_train,y_train)# Train the model using the training sets and check scorelinear &lt;- lm(y_train ~ ., data = x)summary(linear)#Predict Outputpredicted= predict(linear,x_test) 2.逻辑回归别被它的名字搞糊涂了！它是一种分类而不是回归算法。它用于估计离散值(二进制值，如0/1、yes/no、true/false)，基于给定的独立变量(s)。简单地说，它通过将数据匹配到logit函数来预测事件发生的可能性。因此，它也被称为logit回归。因为它预测了概率，它的输出值在0到1之间(如预期的那样)。 让我们再通过一个简单的例子来理解这个问题。 假设你的朋友给了你出了一道题。只有两种结果——要么你解决了，要么你没有。现在想象一下，你被给予了各种各样的智力测验/小测验，试着去理解你擅长的科目。这个研究的结果是这样的-如果你有一个基于十年级的三角测量法，你有70%的可能会解决这个问题。另一方面，如果这是第五次历史问题，得到答案的概率只有30%。这就是逻辑回归所提供的。 在数学计算中，结果的对数概率被建模为预测变量的线性组合。 123odds= p/ (1-p) = probability of event occurrence / probability of not event occurrenceln(odds) = ln(p/(1-p))logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk 上面，p是我们感兴趣的特征的概率。它选择参数来最大化观察样本值的可能性，而不是最小化平方误差的总和(就像普通的回归一样)。 现在，你可能会问，为什么要写一个日志呢?为了简单起见，我们假设这是复制一个阶跃函数的最好的数学方法之一。我可以详细介绍，但这将超出本文的目的。 Python Code 12345678910111213#Import Libraryfrom sklearn.linear_model import LogisticRegression#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create logistic regression objectmodel = LogisticRegression()# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Equation coefficient and Interceptprint('Coefficient: \\n', model.coef_)print('Intercept: \\n', model.intercept_)#Predict Outputpredicted= model.predict(x_test) R Code 123456x &lt;- cbind(x_train,y_train)# Train the model using the training sets and check scorelogistic &lt;- glm(y_train ~ ., data = x,family='binomial')summary(logistic)#Predict Outputpredicted= predict(logistic,x_test) 另外，如果想提高改进模型，有许多不同的方法可以尝试: 包括相互作用项 删减特征 正则化技术 使用非线性模型3.决策树这是我最喜欢的算法之一，我经常使用它。它是一种被监督的学习算法，主要用于分类问题。令人惊讶的是，它适用于分类和连续相关变量。使用这个算法，我们可以把种群分成两个或更多的同类集合。它基于最重要的属性/独立变量，以使其尽可能地成为不同的组。要了解更多细节，您可以阅读有关决策树的建模实现过程。 在上面的图片中，你可以看到，根据不同的属性，人们被划分为四个不同的组，以确定“他们是否会出去玩”。为了将人口分成不同的不同的群体，它使用了各种各样的技术，比如基尼系数、信息增益、卡方、熵。 要理解决策树的工作原理，最好的方法是玩Jezzball——这是微软的一款经典游戏(下图)。基本上，你有一间有移动墙的房间，你需要创建墙壁，这样没有球的最大区域就会被清理。 所以，每次你用一面墙把房间分成两部分，你就会在同一个房间里创造两种不同的人群。决策树以非常相似的方式工作，将一个种群划分为不同的组。 Python Code 123456789101112#Import Library#Import other necessary libraries like pandas, numpy...from sklearn import tree#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create tree object model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini # model = tree.DecisionTreeRegressor() for regression# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Predict Outputpredicted= model.predict(x_test) R Code 1234567library(rpart)x &lt;- cbind(x_train,y_train)# grow tree fit &lt;- rpart(y_train ~ ., data = x,method=\"class\")summary(fit)#Predict Output predicted= predict(fit,x_test) 4. SVM (支持向量机)这是一种分类方法。在这个算法中，我们将每个数据项绘制成n维空间中的一个点(其中n是您拥有的特性的数量)，每个特性的值都是一个特定坐标的值。 例如，如果我们只有两个特征，比如身高和头发的长度，我们首先会在二维空间中绘制这两个变量，其中每个点都有两个坐标(这些坐标被称为支持向量)。 现在，我们将找到一些在两种不同类别的数据之间分割的线。这条线是这样的，这两个簇两个最近点之间是最远的。 在上面的示例中，将数据划分为两个不同的分组的那条线是黑线，因为这两个最接近的点离黑线最远。这条线是我们的分类器。然后，根据测试数据位置在这条线的哪一边，对新数据进行分类。 如果把这个算法看作是在n维空间中玩JezzBall。游戏的微调是: 你可以在任何角度画线/平面(而不是像传统游戏那样水平或垂直)。 游戏的目的是在不同的房间中分离不同颜色的球。 球是固定不动的。 Python Code12345678910#Import Libraryfrom sklearn import svm#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create SVM classification object model = svm.svc() # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Predict Outputpredicted= model.predict(x_test) R Code1234567library(e1071)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;-svm(y_train ~ ., data = x)summary(fit)#Predict Output predicted= predict(fit,x_test) 5. 朴素贝叶斯这是一种基于贝叶斯定理的分类技术，前提基于预测者之间的独立性假设。简单地讲，朴素贝叶斯的分类器假设一个类的特定特性与其他任何特性的存在无关。例如，如果水果是红色的，圆形的，直径大约3英寸，就可以认为它是苹果。即使这些特征相互依赖，或者其他特征的存在，一个朴素贝叶斯分类器也会考虑所有这些特性各自独立地为这个水果是苹果的概率可能性做出的贡献。 简单的贝叶斯模型很容易构建，对于非常大的数据集尤其有用。除了简洁性，朴素贝叶斯也被认为比高度复杂的分类方法表现出色。 贝叶斯定理提供了一种由P(c)、P(x)和P(x|c)中计算出概率P(c|x)的方法。请看下面的等式: 这里， P(c|x)是分类(目标)的后验概率(属性)。 P(c)是分类的先验概率。 P(x|c)是预测给定分类的概率可能性。 P(x)是预测的先验概率。 例子:让我们用一个例子来理解它。下面我有一组天气预报和相应的目标变量“Play”。现在，我们需要根据天气情况对玩家是否会出去玩进行分类。让我们按照下面的步骤来执行它。 步骤1:将数据集转换为频率表 第2步:通过寻找可能性的概率来创建可能性表，例如:阴天概率=0.29，而玩的概率是0.64。 第三步:用简单的贝叶斯方程来计算每个类的后验概率。具有最高后验概率的类是预测的结果。 问题:如果天气晴朗，球员会出去玩，这句话是正确的吗? 我们可以用上面讨论的方法来解它，所以 P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny) 这里P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64 现在，P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60，这有更高的概率。 Naive Bayes使用了一种类似的方法来预测不同类型的不同属性的可能性。该算法主要用于文本分类和有多个类的问题。 Python Code12345678#Import Libraryfrom sklearn.naive_bayes import GaussianNB#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicted= model.predict(x_test) R Code1234567library(e1071)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;-naiveBayes(y_train ~ ., data = x)summary(fit)#Predict Output predicted= predict(fit,x_test) 6. KNN (K-最近邻)它可以用于分类和回归问题。然而，它更广泛地应用于工业中的分类问题。K最近邻是一个简单的算法，它可以存储所有可用的情况，并通过它的K个邻居的多数投票来对新情况进行分类。被分配到一个类的实例在它的K最近的邻居中也是最常见的，它是由一个距离函数测量的。 这些距离函数可以是欧氏、曼哈顿、闵可夫斯基和海明距离。第3个函数用于连续函数，第4个(海明)用于分类变量。如果K=1,那么，这个实例被简单地分配给它最近的邻居。很多时候，在使用KNN建模时，对于K的选择往往是一个挑战。 KNN很容易被映射到我们的真实生活中。如果你想了解一个你没有任何信息的人，你可能会想了解他的亲密朋友和他的圈子，并由此获得他/她的信息！ 在选择KNN之前要考虑的事情: KNN在计算上代价是昂贵的 变量应该被规范化否则更高范围的变量会导致偏差 在使用KNN之前进行预处理十分重要，比如离群值，消除噪音 Python Code123456789#Import Libraryfrom sklearn.neighbors import KNeighborsClassifier#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create KNeighbors classifier object model KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicte R Code1234567library(knn)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;-knn(y_train ~ ., data = x,k=5)summary(fit)#Predict Output predicted= predict(fit,x_test) 7. K-均值它是一种不受监督的算法，解决了聚类的问题。它的过程遵循一种简单而容易的方法，通过一定数量的簇(假设k个簇)对给定的数据集进行分类。簇中的数据点是同类的，异构的，对等的。 还记得从墨水污渍中找出形状吗?k均值与其有点类似。你看一下这个形状，然后扩散开来，看看有多少不同的簇出现！ k-均值聚簇方法: k-均值为每个中心点挑选出K个点。 每个数据点形成一个具有最接近的中心点的簇，即k个簇。 根据现有的簇成员查找每个簇的中心点。这里有新的中心点。 当我们有新的中心点时，重复第2步和第3步。从新的中心找到每个数据点的最近距离，并与新的k个簇联系起来。重复这个过程直到收敛发生，即中心点不会改变。 如何确定K的值: 在k-均值中，我们有这样的簇，每个簇都有自己的中心点。中心点和一个簇中的数据点之间的平方和构成了该集群的平方值的总和。此外，当添加了所有簇的平方总和时，它就变成了簇解决方案的平方总和。 我们知道，随着集群的数量增加，这个值一直在减少但如果你画出结果，你会发现，距离的平方和k的值会急剧下降，之后会更慢。我们可以以此找到最优簇个数K。 Python Code123456789#Import Libraryfrom sklearn.cluster import KMeans#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset# Create KNeighbors classifier object model k_means = KMeans(n_clusters=3, random_state=0)# Train the model using the training sets and check scoremodel.fit(X)#Predict Outputpredicted= model.predict(x_test) R Code12library(cluster)fit &lt;- kmeans(X, 3) # 3 cluster solution 8. 随机森林随机森林是一组决策树的标术语。在随机森林中，我们收集了决策树(也就是所谓的“森林”)。为了对基于属性的新对象进行分类，每棵树都给出一个分类，我们说这个树给类进行“投票”。森林选择了拥有最多选票的分类。 每棵树都是种下并生长的: 如果训练集的实例数是N，则随机可替换抽取这N个实例的样本。这个样本将作为树生长的训练集。 如果有M个输入变量，那么在每个节点上都指定了一个m值的M变量，m变量是在M中中随机选择的，在这些m中最好的分割是去用来分割节点的。在森林生长过程中，m的值保持不变。 每棵树都在尽可能大的范围内生长。没有修剪。 Python Code123456789#Import Libraryfrom sklearn.ensemble import RandomForestClassifier#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create Random Forest objectmodel= RandomForestClassifier()# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicted= model.predict(x_test) R Code1234567library(randomForest)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;- randomForest(Species ~ ., x,ntree=500)summary(fit)#Predict Output predicted= predict(fit,x_test) 9. 降维算法在过去的4-5年中，在每个可能的阶段，数据收集都呈指数级增长。企业/政府机构/研究机构不仅提供了新的信息来源，而且还在详细地收集数据。 例如:电子商务公司正在获取更多关于客户的细节信息，比如他们的人口统计数据、网络爬行历史、他们喜欢或不喜欢的东西、购买历史记录、反馈信息等等，这些都比你最近的杂货店老板更能给他们个性化的关注。 作为一名数据科学家，我们提供的数据也包含许多特性，这对于构建良好的健壮模型来说是很好的，但也有一个挑战。你如何从1000或2000变量中识别出高度显著的变量(s)?在这种情况下，降维算法可以帮助我们，与决策树、随机森林、主成分分析、因子分析等其他算法相结合，以关联矩阵、缺失值比例等为基础进行识别。 Python Code12345678910#Import Libraryfrom sklearn import decomposition#Assumed you have training and test data set as train and test# Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)# For Factor analysis#fa= decomposition.FactorAnalysis()# Reduced the dimension of training dataset using PCAtrain_reduced = pca.fit_transform(train)#Reduced the dimension of test datasettest_reduced = pca.transform(test) R Code1234library(stats)pca &lt;- princomp(train, cor = TRUE)train_reduced &lt;- predict(pca,train)test_reduced &lt;- predict(pca,test) 10. 梯度下降算法 10.1. GBM GBM是一种增强算法，当我们以高预测能力处理大量数据时，它是一种增强算法。增强实际上是一套学习算法的集合，它结合了几个基本估计量的预测，以提高对单一估计量的鲁棒性。它将多个弱或平均的预测因子结合构建到一个强的预测器中。这些增强算法在诸如Kaggle、AV Hackathon、CrowdAnalytix等数据科学竞赛中都很有效。 Python Code123456789#Import Libraryfrom sklearn.ensemble import GradientBoostingClassifier#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create Gradient Boosting Classifier objectmodel= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicted= model.predict(x_test) R Code123456library(caret)x &lt;- cbind(x_train,y_train)# Fitting modelfitControl &lt;- trainControl( method = \"repeatedcv\", number = 4, repeats = 4)fit &lt;- train(y ~ ., data = x, method = \"gbm\", trControl = fitControl,verbose = FALSE)predicted= predict(fit,x_test,type= \"prob\")[,2] 10.2. XGBoost 另一种经典的梯度下降算法，它被认为是在某些Kaggle比赛中获胜与失败之间的决定性选择。 XGBoost具有非常高的预测能力，这使它在预测模型中成为最佳的选择，因为它既具有线性模型又有树学习算法，使得算法比现有的梯度下降技术快了10倍。 它可以支持的功能，包括回归、分类和排名。 XGBoost最有趣的一点是它也被称为规则化的增强技术。这有助于减少过度拟合的建模，并对诸如Scala、Java、R、Python、Julia和C++等一系列语言提供了大量支持。 它也支持分布式大规模训练，包括GCE、AWS、Azure和Yarn 集群。XGBoost也可以与Spark、Flink和其他云数据系统集成在一起，在每次迭代的过程中都建立了交叉验证。 Python Code123456789101112131415from xgboost import XGBClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoreX = dataset[:,0:10]Y = dataset[:,10:]seed = 1X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)model = XGBClassifier()model.fit(X_train, y_train)#Make predictions for test datay_pred = model.predict(X_test) R Code123456789101112131415require(caret)x &lt;- cbind(x_train,y_train)# Fitting modelTrainControl &lt;- trainControl( method = \"repeatedcv\", number = 10, repeats = 4)model&lt;- train(y ~ ., data = x, method = \"xgbLinear\", trControl = TrainControl,verbose = FALSE)OR model&lt;- train(y ~ ., data = x, method = \"xgbTree\", trControl = TrainControl,verbose = FALSE)predicted &lt;- predict(model, x_test) 10.3. LightGBM LightGBM是一个使用基于树的学习算法的梯度增强框架。它的设计是分布式的和高效的，有以下优点: 更快的训练速度和更高的效率 降低内存使用 更好的精度 并行和GPU学习支持 能够处理大规模数据 该框架是一种基于决策树算法的快速和高性能梯度增强，用于排序、分类和许多其他机器学习任务。它是在微软的分布式机器学习工具包项目下开发的。 由于LightGBM是基于决策树算法的，它将树叶和最适合的树分割开，而其他的提升算法将树的深度和水平都分割开，而不是叶节点。因此，当LightGBM在相同的叶子生长时，叶节点的智慧算法能够比水平的算法减少更多的损失，从而产生更好的精度，而这在现有的增强算法中几乎是不可能实现的。 而且，它的速度惊人的快，因此用了“光”这个词。 Python Code1234567891011121314151617data = np.random.rand(500, 10) # 500 entities, each contains 10 featureslabel = np.random.randint(2, size=500) # binary targettrain_data = lgb.Dataset(data, label=label)test_data = train_data.create_valid('test.svm')param = &#123;'num_leaves':31, 'num_trees':100, 'objective':'binary'&#125;param['metric'] = 'auc'num_round = 10bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])bst.save_model('model.txt')# 7 entities, each contains 10 featuresdata = np.random.rand(7, 10)ypred = bst.predict(data) R Code1234567891011121314151617181920212223242526library(RLightGBM)data(example.binary)#Parametersnum_iterations &lt;- 100config &lt;- list(objective = \"binary\", metric=\"binary_logloss,auc\", learning_rate = 0.1, num_leaves = 63, tree_learner = \"serial\", feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)#Create data handle and boosterhandle.data &lt;- lgbm.data.create(x)lgbm.data.setField(handle.data, \"label\", y)handle.booster &lt;- lgbm.booster.create(handle.data, lapply(config, as.character))#Train for num_iterations iterations and eval every 5 stepslgbm.booster.train(handle.booster, num_iterations, 5)#Predictpred &lt;- lgbm.booster.predict(handle.booster, x.test)#Test accuracysum(y.test == (y.pred &gt; 0.5)) / length(y.test)#Save model (can be loaded again via lgbm.booster.load(filename))lgbm.booster.save(handle.booster, filename = \"/tmp/model.txt\") 如果您熟悉R中的Caret包，下面是实现LightGBM的另一种方式。1234567891011121314151617require(caret)require(RLightGBM)data(iris)model &lt;-caretModel.LGBM()fit &lt;- train(Species ~ ., data = iris, method=model, verbosity = 0)print(fit)y.pred &lt;- predict(fit, iris[,1:4])library(Matrix)model.sparse &lt;- caretModel.LGBM.sparse()#Generate a sparse matrixmat &lt;- Matrix(as.matrix(iris[,1:4]), sparse = T)fit &lt;- train(data.frame(idx = 1:nrow(iris)), iris$Species, method = model.sparse, matrix = mat, verbosity = 0)print(fit) 10.4. Catboost CatBoost是最近开源的一款来自Yandex的机器学习算法。它可以很容易地与谷歌的TensorFlow和苹果的Core ML等深度学习框架集成。 关于CatBoost的最好的地方是它不需要像其他ML模型那样进行大量的数据进行训练，并且可以处理各种数据格式，也不会影响它的鲁棒性。 在执行算法之前，请确保您已对缺失的数据进行了处理。 Catboost可以自动处理分类变量，而不显示类型转换错误，这有助于您集中精力优化您的模型，而不是处理一些琐碎的错误。 Python Code1234567891011121314151617181920212223242526272829303132import pandas as pdimport numpy as npfrom catboost import CatBoostRegressor#Read training and testing filestrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")#Imputing missing values for both train and testtrain.fillna(-999, inplace=True)test.fillna(-999,inplace=True)#Creating a training set for modeling and validation set to check model performanceX = train.drop(['Item_Outlet_Sales'], axis=1)y = train.Item_Outlet_Salesfrom sklearn.model_selection import train_test_splitX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)categorical_features_indices = np.where(X.dtypes != np.float)[0]#importing library and building modelfrom catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)submission = pd.DataFrame()submission['Item_Identifier'] = test['Item_Identifier']submission['Outlet_Identifier'] = test['Outlet_Identifier']submission['Item_Outlet_Sales'] = model.predict(test) R Code123456789101112131415161718192021222324252627set.seed(1)require(titanic)require(caret)require(catboost)tt &lt;- titanic::titanic_train[complete.cases(titanic::titanic_train),]data &lt;- as.data.frame(as.matrix(tt), stringsAsFactors = TRUE)drop_columns = c(\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\")x &lt;- data[,!(names(data) %in% drop_columns)]y &lt;- data[,c(\"Survived\")]fit_control &lt;- trainControl(method = \"cv\", number = 4,classProbs = TRUE)grid &lt;- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_leaf_reg = 1e-3, rsm = 0.95, border_count = 64)report &lt;- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = TRUE, preProc = NULL,tuneGrid = grid, trControl = fit_control)print(report)importance &lt;- varImp(report, scale = FALSE)print(importance) 写在最后现在，我敢肯定，你会有一种大体如何使用机器学习算法的想法。我写这篇文章并提供R和Python代码的唯一目的就是让你马上开始。如果你想要精通机器学习，那就马上开始吧。解决问题，不断加强对过程的物理理解，应用这些代码，并发现乐趣！ 译自：https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/","tags":[{"name":"线性回归","slug":"线性回归","permalink":"http://weibo.com/youthcolor/tags/线性回归/"},{"name":"机器学习","slug":"机器学习","permalink":"http://weibo.com/youthcolor/tags/机器学习/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://weibo.com/youthcolor/tags/逻辑回归/"},{"name":"决策树","slug":"决策树","permalink":"http://weibo.com/youthcolor/tags/决策树/"},{"name":"支持向量机","slug":"支持向量机","permalink":"http://weibo.com/youthcolor/tags/支持向量机/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"http://weibo.com/youthcolor/tags/朴素贝叶斯/"},{"name":"最近邻（KNN）","slug":"最近邻（KNN）","permalink":"http://weibo.com/youthcolor/tags/最近邻（KNN）/"},{"name":"K-均值","slug":"K-均值","permalink":"http://weibo.com/youthcolor/tags/K-均值/"},{"name":"随机森林","slug":"随机森林","permalink":"http://weibo.com/youthcolor/tags/随机森林/"},{"name":"降维算法","slug":"降维算法","permalink":"http://weibo.com/youthcolor/tags/降维算法/"},{"name":"梯度下降算法","slug":"梯度下降算法","permalink":"http://weibo.com/youthcolor/tags/梯度下降算法/"},{"name":"GBM","slug":"GBM","permalink":"http://weibo.com/youthcolor/tags/GBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"http://weibo.com/youthcolor/tags/XGBoost/"},{"name":"LightGBM","slug":"LightGBM","permalink":"http://weibo.com/youthcolor/tags/LightGBM/"},{"name":"CatBoost","slug":"CatBoost","permalink":"http://weibo.com/youthcolor/tags/CatBoost/"}]}]