[{"title":"侦测欺诈交易","date":"2017-08-22T16:59:25.616Z","path":"2017/08/22/侦测欺诈交易/","text":"本案例来自葡萄牙作者Luis Torgo所著的《数据挖掘与Ｒ语言》一书第４章：侦测欺诈交易。 本章介绍了一类数据挖掘问题：离群值排序。特别是采用了一个从不同的方面处理这个任务的数据集。即应用了有监督的、无监督的和半监督的方法。对于应用有限的资源来找到一个现象的异常观测值这一常见问题，本章的应用可以看做是它的一个实例。多个实际问题可以映射到这个通用框架中，例如信用卡、电信和税收的欺诈侦测等。在证券领域，也有这种欺诈侦测一般概念的多个应用。 1234567891011121314151617181920212223242526272829303132333435363738394041# 侦测欺骗交易setwd(\"~/Downloads\")###################################################### Loading the data into R###################################################load('sales.Rdata')library(DMwR)data(sales)head(sales)###################################################### Exploring the data set###################################################summary(sales)nlevels(sales$ID)nlevels(sales$Prod)length(which(is.na(sales$Quant) &amp; is.na(sales$Val)))sum(is.na(sales$Quant) &amp; is.na(sales$Val))table(sales$Insp)/nrow(sales)*100totS &lt;- table(sales$ID)totP &lt;- table(sales$Prod)barplot(totS,main='Transactions per salespeople',names.arg='',xlab='Salespeople', ylab='Amount') 12barplot(totP,main='Transactions per product',names.arg='',xlab='Products', ylab='Amount') 12345678910111213sales$Uprice &lt;- sales$Val/sales$Quantsummary(sales$Uprice)attach(sales)upp &lt;- aggregate(Uprice,list(Prod),median,na.rm=T)topP &lt;- sapply(c(T,F),function(o) upp[order(upp[,2],decreasing=o)[1:5],1])colnames(topP) &lt;- c('Expensive','Cheap')topPtops &lt;- sales[Prod %in% topP[1,],c('Prod','Uprice')]tops$Prod &lt;- factor(tops$Prod)boxplot(Uprice ~ Prod,data=tops,ylab='Uprice',log=\"y\") 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122vs &lt;- aggregate(Val,list(ID),sum,na.rm=T)scoresSs &lt;- sapply(c(T,F),function(o) vs[order(vs$x,decreasing=o)[1:5],1])colnames(scoresSs) &lt;- c('Most','Least')scoresSssum(vs[order(vs$x,decreasing=T)[1:100],2])/sum(Val,na.rm=T)*100sum(vs[order(vs$x,decreasing=F)[1:2000],2])/sum(Val,na.rm=T)*100qs &lt;- aggregate(Quant,list(Prod),sum,na.rm=T)scoresPs &lt;- sapply(c(T,F),function(o) qs[order(qs$x,decreasing=o)[1:5],1])colnames(scoresPs) &lt;- c('Most','Least')scoresPssum(as.double(qs[order(qs$x,decreasing=T)[1:100],2]))/ sum(as.double(Quant),na.rm=T)*100sum(as.double(qs[order(qs$x,decreasing=F)[1:4000],2]))/ sum(as.double(Quant),na.rm=T)*100out &lt;- tapply(Uprice,list(Prod=Prod), function(x) length(boxplot.stats(x)$out))out[order(out,decreasing=T)[1:10]]sum(out)sum(out)/nrow(sales)*100###################################################### Data problems###################################################totS &lt;- table(ID)totP &lt;- table(Prod)nas &lt;- sales[which(is.na(Quant) &amp; is.na(Val)),c('ID','Prod')]propS &lt;- 100*table(nas$ID)/totSpropS[order(propS,decreasing=T)[1:10]]propP &lt;- 100*table(nas$Prod)/totPpropP[order(propP,decreasing=T)[1:10]]detach(sales)sales &lt;- sales[-which(is.na(sales$Quant) &amp; is.na(sales$Val)),]nnasQp &lt;- tapply(sales$Quant,list(sales$Prod), function(x) sum(is.na(x)))propNAsQp &lt;- nnasQp/table(sales$Prod)propNAsQp[order(propNAsQp,decreasing=T)[1:10]]sales &lt;- sales[!sales$Prod %in% c('p2442','p2443'),]nlevels(sales$Prod)sales$Prod &lt;- factor(sales$Prod)nlevels(sales$Prod)nnasQs &lt;- tapply(sales$Quant,list(sales$ID),function(x) sum(is.na(x)))propNAsQs &lt;- nnasQs/table(sales$ID)propNAsQs[order(propNAsQs,decreasing=T)[1:10]]nnasVp &lt;- tapply(sales$Val,list(sales$Prod), function(x) sum(is.na(x)))propNAsVp &lt;- nnasVp/table(sales$Prod)propNAsVp[order(propNAsVp,decreasing=T)[1:10]]nnasVs &lt;- tapply(sales$Val,list(sales$ID),function(x) sum(is.na(x)))propNAsVs &lt;- nnasVs/table(sales$ID)propNAsVs[order(propNAsVs,decreasing=T)[1:10]]tPrice &lt;- tapply(sales[sales$Insp != 'fraud','Uprice'], list(sales[sales$Insp != 'fraud','Prod']), median,na.rm=T)noQuant &lt;- which(is.na(sales$Quant))sales[noQuant,'Quant'] &lt;- ceiling(sales[noQuant,'Val'] / tPrice[sales[noQuant,'Prod']])noVal &lt;- which(is.na(sales$Val))sales[noVal,'Val'] &lt;- sales[noVal,'Quant'] * tPrice[sales[noVal,'Prod']]sales$Uprice &lt;- sales$Val/sales$Quantsave(sales,file='salesClean.Rdata')attach(sales)notF &lt;- which(Insp != 'fraud')ms &lt;- tapply(Uprice[notF],list(Prod=Prod[notF]),function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2])&#125;)ms &lt;- matrix(unlist(ms), length(ms),2, byrow=T,dimnames=list(names(ms),c('median','iqr')))head(ms)par(mfrow=c(1,2))plot(ms[,1],ms[,2],xlab='Median',ylab='IQR',main='')plot(ms[,1],ms[,2],xlab='Median',ylab='IQR',main='',col='grey',log=\"xy\")smalls &lt;- which(table(Prod) &lt; 20)points(log(ms[smalls,1]),log(ms[smalls,2]),pch='+') 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152dms &lt;- scale(ms)smalls &lt;- which(table(Prod) &lt; 20)prods &lt;- tapply(sales$Uprice,sales$Prod,list)similar &lt;- matrix(NA,length(smalls),7,dimnames=list(names(smalls), c('Simil','ks.stat','ks.p','medP','iqrP','medS','iqrS')))for(i in seq(along=smalls)) &#123; d &lt;- scale(dms,dms[smalls[i],],FALSE) d &lt;- sqrt(drop(d^2 %*% rep(1,ncol(d)))) stat &lt;- ks.test(prods[[smalls[i]]],prods[[order(d)[2]]]) similar[i,] &lt;- c(order(d)[2],stat$statistic,stat$p.value,ms[smalls[i],], ms[order(d)[2],])&#125;head(similar)levels(Prod)[similar[1,1]]nrow(similar[similar[,'ks.p'] &gt;= 0.9,])sum(similar[,'ks.p'] &gt;= 0.9)save(similar,file='similarProducts.Rdata')###################################################### Evaluation criteria###################################################library(ROCR)data(ROCR.simple)pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )perf &lt;- performance(pred,'prec','rec')plot(perf)PRcurve &lt;- function(preds,trues,...) &#123; require(ROCR,quietly=T) pd &lt;- prediction(preds,trues) pf &lt;- performance(pd,'prec','rec') pf@y.values &lt;- lapply(pf@y.values,function(x) rev(cummax(rev(x)))) plot(pf,...)&#125;PRcurve(ROCR.simple$predictions, ROCR.simple$labels ) 123456789101112131415pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )perf &lt;- performance(pred,'lift','rpp')plot(perf,main='Lift Chart')CRchart &lt;- function(preds,trues,...) &#123; require(ROCR,quietly=T) pd &lt;- prediction(preds,trues) pf &lt;- performance(pd,'rec','rpp') plot(pf,...)&#125; CRchart(ROCR.simple$predictions, ROCR.simple$labels, main='Cumulative Recall Chart') 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104avgNDTP &lt;- function(toInsp,train,stats) &#123; if (missing(train) &amp;&amp; missing(stats)) stop('Provide either the training data or the product stats') if (missing(stats)) &#123; notF &lt;- which(train$Insp != 'fraud') stats &lt;- tapply(train$Uprice[notF], list(Prod=train$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;) stats &lt;- matrix(unlist(stats), length(stats),2,byrow=T, dimnames=list(names(stats),c('median','iqr'))) stats[which(stats[,'iqr']==0),'iqr'] &lt;- stats[which(stats[,'iqr']==0),'median'] &#125; mdtp &lt;- mean(abs(toInsp$Uprice-stats[toInsp$Prod,'median']) / stats[toInsp$Prod,'iqr']) return(mdtp)&#125;###################################################### Experimental Methodology###################################################evalOutlierRanking &lt;- function(testSet,rankOrder,Threshold,statsProds) &#123; ordTS &lt;- testSet[rankOrder,] N &lt;- nrow(testSet) nF &lt;- if (Threshold &lt; 1) as.integer(Threshold*N) else Threshold cm &lt;- table(c(rep('fraud',nF),rep('ok',N-nF)),ordTS$Insp) prec &lt;- cm['fraud','fraud']/sum(cm['fraud',]) rec &lt;- cm['fraud','fraud']/sum(cm[,'fraud']) AVGndtp &lt;- avgNDTP(ordTS[nF,],stats=statsProds) return(c(Precision=prec,Recall=rec,avgNDTP=AVGndtp))&#125;###################################################### The modified box plot rule###################################################BPrule &lt;- function(train,test) &#123; notF &lt;- which(train$Insp != 'fraud') ms &lt;- tapply(train$Uprice[notF],list(Prod=train$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;) ms &lt;- matrix(unlist(ms),length(ms),2,byrow=T, dimnames=list(names(ms),c('median','iqr'))) ms[which(ms[,'iqr']==0),'iqr'] &lt;- ms[which(ms[,'iqr']==0),'median'] ORscore &lt;- abs(test$Uprice-ms[test$Prod,'median']) / ms[test$Prod,'iqr'] return(list(rankOrder=order(ORscore,decreasing=T), rankScore=ORscore))&#125;notF &lt;- which(sales$Insp != 'fraud')globalStats &lt;- tapply(sales$Uprice[notF], list(Prod=sales$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;)globalStats &lt;- matrix(unlist(globalStats), length(globalStats),2,byrow=T, dimnames=list(names(globalStats),c('median','iqr')))globalStats[which(globalStats[,'iqr']==0),'iqr'] &lt;- globalStats[which(globalStats[,'iqr']==0),'median']ho.BPrule &lt;- function(form, train, test, ...) &#123; res &lt;- BPrule(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;bp.res &lt;- holdOut(learner('ho.BPrule', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(bp.res)par(mfrow=c(1,2))info &lt;- attr(bp.res,'itsInfo')PTs.bp &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',avg='vertical')CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',avg='vertical') 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657###################################################### Local outlier factors (LOF)###################################################ho.LOF &lt;- function(form, train, test, k, ...) &#123; ntr &lt;- nrow(train) all &lt;- rbind(train,test) N &lt;- nrow(all) ups &lt;- split(all$Uprice,all$Prod) r &lt;- list(length=ups) for(u in seq(along=ups)) r[[u]] &lt;- if (NROW(ups[[u]]) &gt; 3) lofactor(ups[[u]],min(k,NROW(ups[[u]]) %/% 2)) else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) else NULL all$lof &lt;- vector(length=N) split(all$lof,all$Prod) &lt;- r all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] &lt;- SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))]) structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'], decreasing=T),...), itInfo=list(preds=all[(ntr+1):N,'lof'], trues=ifelse(test$Insp=='fraud',1,0)) )&#125;lof.res &lt;- holdOut(learner('ho.LOF', pars=list(k=7,Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(lof.res)par(mfrow=c(1,2))info &lt;- attr(lof.res,'itsInfo')PTs.lof &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')legend('topright',c('BPrule','LOF'),lty=c(1,2))CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')legend('bottomright',c('BPrule','LOF'),lty=c(1,2)) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566###################################################### Clustering-based outlier rankings (OR_h)###################################################ho.ORh &lt;- function(form, train, test, ...) &#123; require(dprep,quietly=T) ntr &lt;- nrow(train) all &lt;- rbind(train,test) N &lt;- nrow(all) ups &lt;- split(all$Uprice,all$Prod) r &lt;- list(length=ups) for(u in seq(along=ups)) r[[u]] &lt;- if (NROW(ups[[u]]) &gt; 3) outliers.ranking(ups[[u]])$prob.outliers else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) else NULL all$lof &lt;- vector(length=N) split(all$lof,all$Prod) &lt;- r all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] &lt;- SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))]) structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'], decreasing=T),...), itInfo=list(preds=all[(ntr+1):N,'lof'], trues=ifelse(test$Insp=='fraud',1,0)) )&#125;orh.res &lt;- holdOut(learner('ho.ORh', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(orh.res)par(mfrow=c(1,2))info &lt;- attr(orh.res,'itsInfo')PTs.orh &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('BPrule','LOF','ORh'), lty=c(1,2,1),col=c('black','black','grey'))CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical')legend('bottomright',c('BPrule','LOF','ORh'), lty=c(1,2,1),col=c('black','black','grey')) 12345678910111213###################################################### The class imbalance problem###################################################data(iris)data &lt;- iris[,c(1,2,5)]data$Species &lt;- factor(ifelse(data$Species == 'setosa','rare','common'))newData &lt;- SMOTE(Species ~ .,data,perc.over=600)table(newData$Species)par(mfrow=c(1,2))plot(data[,1],data[,2],pch=19+as.integer(data[,3]),main='Original Data')plot(newData[,1],newData[,2],pch=19+as.integer(newData[,3]),main=\"SMOTE'd Data\") 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859###################################################### Naive Bayes###################################################nb &lt;- function(train,test) &#123; require(e1071,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) model &lt;- naiveBayes(Insp ~ .,data) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nb &lt;- function(form, train, test, ...) &#123; res &lt;- nb(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nb.res &lt;- holdOut(learner('ho.nb', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nb.res)par(mfrow=c(1,2))info &lt;- attr(nb.res,'itsInfo')PTs.nb &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('NaiveBayes','ORh'), lty=1,col=c('black','grey'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('bottomright',c('NaiveBayes','ORh'), lty=1,col=c('black','grey')) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263nb.s &lt;- function(train,test) &#123; require(e1071,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) newData &lt;- SMOTE(Insp ~ .,data,perc.over=700) model &lt;- naiveBayes(Insp ~ .,newData) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nbs &lt;- function(form, train, test, ...) &#123; res &lt;- nb.s(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nbs.res &lt;- holdOut(learner('ho.nbs', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nbs.res)par(mfrow=c(1,2))info &lt;- attr(nbs.res,'itsInfo')PTs.nbs &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.nbs[,,1],PTs.nbs[,,2], add=T,lty=2, avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('NaiveBayes','smoteNaiveBayes','ORh'), lty=c(1,2,1),col=c('black','black','grey'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.nbs[,,1],PTs.nbs[,,2], add=T,lty=2, avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('bottomright',c('NaiveBayes','smoteNaiveBayes','ORh'), lty=c(1,2,1),col=c('black','black','grey')) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182###################################################### AdaBoost###################################################library(RWeka)WOW(AdaBoostM1)data(iris)idx &lt;- sample(150,100)model &lt;- AdaBoostM1(Species ~ .,iris[idx,], control=Weka_control(I=100))preds &lt;- predict(model,iris[-idx,])head(preds)table(preds,iris[-idx,'Species'])prob.preds &lt;- predict(model,iris[-idx,],type='probability')head(prob.preds)ab &lt;- function(train,test) &#123; require(RWeka,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) model &lt;- AdaBoostM1(Insp ~ .,data, control=Weka_control(I=100)) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='probability') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.ab &lt;- function(form, train, test, ...) &#123; res &lt;- ab(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;ab.res &lt;- holdOut(learner('ho.ab', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(ab.res)par(mfrow=c(1,2))info &lt;- attr(ab.res,'itsInfo')PTs.ab &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.ab[,,1],PTs.ab[,,2], add=T,lty=2, avg='vertical') legend('topright',c('NaiveBayes','ORh','AdaBoostM1'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.ab[,,1],PTs.ab[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('NaiveBayes','ORh','AdaBoostM1'), lty=c(1,1,2),col=c('black','grey','black')) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293###################################################### Semi-supervised approaches###################################################set.seed(1234) # Just to ensrure you get the same results as in the booklibrary(DMwR)library(e1071)data(iris)idx &lt;- sample(150,100)tr &lt;- iris[idx,]ts &lt;- iris[-idx,]nb &lt;- naiveBayes(Species ~ .,tr)table(predict(nb,ts),ts$Species)trST &lt;- trnas &lt;- sample(100,90)trST[nas,'Species'] &lt;- NAfunc &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='raw') data.frame(cl=colnames(p)[apply(p,1,which.max)],p=apply(p,1,max))&#125;nbSTbase &lt;- naiveBayes(Species ~ .,trST[-nas,])table(predict(nbSTbase,ts),ts$Species)nbST &lt;- SelfTrain(Species ~ .,trST,learner('naiveBayes',list()),'func')table(predict(nbST,ts),ts$Species)pred.nb &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='raw') data.frame(cl=colnames(p)[apply(p,1,which.max)], p=apply(p,1,max) )&#125;nb.st &lt;- function(train,test) &#123; require(e1071,quietly=T) train &lt;- train[,c('ID','Prod','Uprice','Insp')] train[which(train$Insp == 'unkn'),'Insp'] &lt;- NA train$Insp &lt;- factor(train$Insp,levels=c('ok','fraud')) model &lt;- SelfTrain(Insp ~ .,train, learner('naiveBayes',list()),'pred.nb') preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nb.st &lt;- function(form, train, test, ...) &#123; res &lt;- nb.st(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nb.st.res &lt;- holdOut(learner('ho.nb.st', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nb.st.res)par(mfrow=c(1,2))info &lt;- attr(nb.st.res,'itsInfo')PTs.nb.st &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.nb.st[,,1],PTs.nb.st[,,2], add=T,lty=2, avg='vertical') legend('topright',c('NaiveBayes','ORh','NaiveBayes-ST'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.nb.st[,,1],PTs.nb.st[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('NaiveBayes','ORh','NaiveBayes-ST'), lty=c(1,1,2),col=c('black','grey','black')) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768pred.ada &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='probability') data.frame(cl=colnames(p)[apply(p,1,which.max)], p=apply(p,1,max) )&#125;ab.st &lt;- function(train,test) &#123; require(RWeka,quietly=T) train &lt;- train[,c('ID','Prod','Uprice','Insp')] train[which(train$Insp == 'unkn'),'Insp'] &lt;- NA train$Insp &lt;- factor(train$Insp,levels=c('ok','fraud')) model &lt;- SelfTrain(Insp ~ .,train, learner('AdaBoostM1', list(control=Weka_control(I=100))), 'pred.ada') preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='probability') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.ab.st &lt;- function(form, train, test, ...) &#123; res &lt;- ab.st(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;ab.st.res &lt;- holdOut(learner('ho.ab.st', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(ab.st.res)par(mfrow=c(1,2))info &lt;- attr(ab.st.res,'itsInfo')PTs.ab.st &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.ab[,,1],PTs.ab[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.ab.st[,,1],PTs.ab.st[,,2], add=T,lty=2, avg='vertical') legend('topright',c('AdaBoostM1','ORh','AdaBoostM1-ST'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.ab[,,1],PTs.ab[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.ab.st[,,1],PTs.ab.st[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('AdaBoostM1','ORh','AdaBoostM1-ST'), lty=c(1,1,2),col=c('black','grey','black'))","tags":[{"name":"聚类","slug":"聚类","permalink":"http://weibo.com/youthcolor/tags/聚类/"},{"name":"欺诈侦测","slug":"欺诈侦测","permalink":"http://weibo.com/youthcolor/tags/欺诈侦测/"},{"name":"半监督分类","slug":"半监督分类","permalink":"http://weibo.com/youthcolor/tags/半监督分类/"},{"name":"类失衡","slug":"类失衡","permalink":"http://weibo.com/youthcolor/tags/类失衡/"},{"name":"贝叶斯分类器","slug":"贝叶斯分类器","permalink":"http://weibo.com/youthcolor/tags/贝叶斯分类器/"},{"name":"AdaBoost分类器","slug":"AdaBoost分类器","permalink":"http://weibo.com/youthcolor/tags/AdaBoost分类器/"},{"name":"决策精确度","slug":"决策精确度","permalink":"http://weibo.com/youthcolor/tags/决策精确度/"},{"name":"回溯精确度","slug":"回溯精确度","permalink":"http://weibo.com/youthcolor/tags/回溯精确度/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://weibo.com/youthcolor/tags/交叉验证/"},{"name":"离群值侦测排序","slug":"离群值侦测排序","permalink":"http://weibo.com/youthcolor/tags/离群值侦测排序/"}]},{"title":"购物篮关联规则分析","date":"2017-08-18T11:43:34.334Z","path":"2017/08/18/购物篮关联规则分析/","text":"我们生活在一个快速变化的数字世界。在当今时代，亚马逊，淘宝，京东等电商都在购物推荐算法方面做了不同的工作。 这给零售商带来了一个有趣的机遇和挑战。如果你能告诉顾客他们想要买什么，它不仅能提高你的销售量，还能提高顾客体验，最终提高产品价值。 另一方面，如果您无法预测下一次客户将要购买什么，客户可能不会返回您的商店。 在这篇文章中，我们将学习关联规则算法。 数据准备12345678910111213141516171819202122232425262728293031323334353637# install.packages(&quot;pacman&quot;)pacman::p_load(PACKAGE_NAME)pacman::p_load(arules, arulesViz)library(arules)library(arulesViz)data(&quot;Groceries&quot;)# transactions类型str(Groceries)Formal class &apos;transactions&apos; [package &quot;arules&quot;] with 3 slots ..@ data :Formal class &apos;ngCMatrix&apos; [package &quot;Matrix&quot;] with 5 slots .. .. ..@ i : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ... .. .. ..@ p : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ... .. .. ..@ Dim : int [1:2] 169 9835 .. .. ..@ Dimnames:List of 2 .. .. .. ..$ : NULL .. .. .. ..$ : NULL .. .. ..@ factors : list() ..@ itemInfo :&apos;data.frame&apos;: 169 obs. of 3 variables: .. ..$ labels: chr [1:169] &quot;frankfurter&quot; &quot;sausage&quot; &quot;liver loaf&quot; &quot;ham&quot; ... .. ..$ level2: Factor w/ 55 levels &quot;baby food&quot;,&quot;bags&quot;,..: 44 44 44 44 44 44 44 42 42 41 ... .. ..$ level1: Factor w/ 10 levels &quot;canned food&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... ..@ itemsetInfo:&apos;data.frame&apos;: 0 obs. of 0 variables# 查看数据head(Groceries@itemInfo, n=12) labels level2 level11 frankfurter sausage meat and sausage2 sausage sausage meat and sausage3 liver loaf sausage meat and sausage4 ham sausage meat and sausage5 meat sausage meat and sausage6 finished products sausage meat and sausage7 organic sausage sausage meat and sausage8 chicken poultry meat and sausage9 turkey poultry meat and sausage10 pork pork meat and sausage11 beef beef meat and sausage12 hamburger meat beef meat and sausage 自定义数据transactions类型转换123456789101112131415161718192021222324252627282930313233343536data &lt;- list( c(\"a\",\"b\",\"c\"), c(\"a\",\"b\"), c(\"a\",\"b\",\"d\"), c(\"b\",\"e\"), c(\"b\",\"c\",\"e\"), c(\"a\",\"d\",\"e\"), c(\"a\",\"c\"), c(\"a\",\"b\",\"d\"), c(\"c\",\"e\"), c(\"a\",\"b\",\"d\",\"e\"), c(\"a\",'b','e','c'))data &lt;- as(data, \"transactions\")inspect(data) items [1] &#123;a,b,c&#125; [2] &#123;a,b&#125; [3] &#123;a,b,d&#125; [4] &#123;b,e&#125; [5] &#123;b,c,e&#125; [6] &#123;a,d,e&#125; [7] &#123;a,c&#125; [8] &#123;a,b,d&#125; [9] &#123;c,e&#125; [10] &#123;a,b,d,e&#125;[11] &#123;a,b,c,e&#125;# Convert transactions to transaction ID liststl &lt;- as(data, \"tidLists\")inspect(tl) items transationIDs 1 a &#123;1,2,3,6,7,8,10,11&#125;2 b &#123;1,2,3,4,5,8,10,11&#125;3 c &#123;1,5,7,9,11&#125; 4 d &#123;3,6,8,10&#125; 5 e &#123;4,5,6,9,10,11&#125; summary函数查看购买频次最高的商品1234567891011121314151617181920212223242526summary(Groceries)transactions as itemMatrix in sparse format with 9835 rows (elements/itemsets/transactions) and 169 columns (items) and a density of 0.02609146 most frequent items: whole milk other vegetables rolls/buns soda yogurt 2513 1903 1809 1715 1372 (Other) 34055 element (itemset/transaction) length distribution:sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 46 29 18 19 20 21 22 23 24 26 27 28 29 32 14 14 9 11 4 6 1 1 1 1 3 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information - examples: labels level2 level11 frankfurter sausage meat and sausage2 sausage sausage meat and sausage3 liver loaf sausage meat and sausage 实现关联规则算法123456789101112131415161718192021222324rules &lt;- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.80))inspect(rules[1:10]) lhs rhs support [1] &#123;liquor,red/blush wine&#125; =&gt; &#123;bottled beer&#125; 0.001931876[2] &#123;curd,cereals&#125; =&gt; &#123;whole milk&#125; 0.001016777[3] &#123;yogurt,cereals&#125; =&gt; &#123;whole milk&#125; 0.001728521[4] &#123;butter,jam&#125; =&gt; &#123;whole milk&#125; 0.001016777[5] &#123;soups,bottled beer&#125; =&gt; &#123;whole milk&#125; 0.001118454[6] &#123;napkins,house keeping products&#125; =&gt; &#123;whole milk&#125; 0.001321810[7] &#123;whipped/sour cream,house keeping products&#125; =&gt; &#123;whole milk&#125; 0.001220132[8] &#123;pastry,sweet spreads&#125; =&gt; &#123;whole milk&#125; 0.001016777[9] &#123;turkey,curd&#125; =&gt; &#123;other vegetables&#125; 0.001220132[10] &#123;rice,sugar&#125; =&gt; &#123;whole milk&#125; 0.001220132 confidence lift [1] 0.9047619 11.235269[2] 0.9090909 3.557863[3] 0.8095238 3.168192[4] 0.8333333 3.261374[5] 0.9166667 3.587512[6] 0.8125000 3.179840[7] 0.9230769 3.612599[8] 0.9090909 3.557863[9] 0.8000000 4.134524[10] 1.0000000 3.913649 频率直方图1234library(RColorBrewer)arules::itemFrequencyPlot(Groceries,topN=20,col=brewer.pal(8,'Pastel2'), main='Relative Item Frequency Plot', type=\"relative\",ylab=\"Item Frequency (Relative)\") 支持度和提升度的图形表示1plot(rules[1:20], method = \"graph\", control = list(type = \"items\")) 平行坐标系1plot(rules[1:20], method = \"paracoord\", control = list(reorder = TRUE)) 矩阵表示1plot(rules[1:20], method = \"matrix\", control = list(reorder = TRUE)) 互动式散点图1arulesViz::plotly_arules(rules)","tags":[{"name":"购物篮","slug":"购物篮","permalink":"http://weibo.com/youthcolor/tags/购物篮/"},{"name":"关联规则","slug":"关联规则","permalink":"http://weibo.com/youthcolor/tags/关联规则/"},{"name":"推荐算法","slug":"推荐算法","permalink":"http://weibo.com/youthcolor/tags/推荐算法/"}]},{"title":"R中如何对量化策略进行回测","date":"2017-07-31T16:10:24.151Z","path":"2017/07/31/R中如何对量化策略进行回测/","text":"R中提供了极为便利的回测方法，如PerformanceAnalytics包，下面是简短实例。以A股福田汽车600166.SS为例。 第1步: 获取股票数据12setSymbolLookup(MYSTOCK=list(name='600166.SS',src='yahoo',from='2007-01-01'))getSymbols('MYSTOCK') 第2步: 建立指标12MYSTOCK &lt;- na.omit(MYSTOCK)dvi &lt;- DVI(Cl(MYSTOCK)) 第3步: 构建交易规则1sig &lt;- Lag(ifelse(dvi$dvi &lt; 0.5, 1, -1)) 第4步: 交易规则/资金曲线1234ret &lt;- ROC(Cl(MYSTOCK))*(sig-0.00025)ret &lt;- ret['2013-09-01/2014-06-01']eq &lt;- exp(cumsum(ret))plot(eq) 第5步: 评估量化策略的表现123456789101112131415161718192021222324table.Drawdowns(ret, top=10) From Trough To Depth Length To Trough Recovery1 2014-01-29 2014-02-18 2014-03-31 -0.1574 39 10 292 2013-09-02 2014-01-13 2014-01-28 -0.1246 99 88 113 2014-04-23 2014-05-09 2014-05-19 -0.0484 17 11 64 2014-04-01 2014-04-03 2014-04-08 -0.0206 5 3 25 2014-04-09 2014-04-14 2014-04-15 -0.0163 5 4 16 2014-05-22 2014-05-22 2014-05-26 -0.0061 3 1 27 2014-05-27 2014-05-27 2014-05-28 -0.0060 2 1 18 2014-05-20 2014-05-20 2014-05-21 -0.0041 2 1 1table.DownsideRisk(ret) 600166.SS.CloseSemi Deviation 0.0147Gain Deviation 0.0119Loss Deviation 0.0153Downside Deviation (MAR=210%) 0.0188Downside Deviation (Rf=0%) 0.0143Downside Deviation (0%) 0.0143Maximum Drawdown 0.1574Historical VaR (95%) -0.0268Historical ES (95%) -0.0466Modified VaR (95%) -0.0334Modified ES (95%) -0.0617charts.PerformanceSummary(ret)","tags":[{"name":"量化金融","slug":"量化金融","permalink":"http://weibo.com/youthcolor/tags/量化金融/"},{"name":"回测","slug":"回测","permalink":"http://weibo.com/youthcolor/tags/回测/"}]},{"title":"R中的大数据分析","date":"2017-07-11T10:53:54.652Z","path":"2017/07/11/R中的大数据分析/","text":"尽管R语言是一门十分强大且健壮的统计型语言，但它最大的短板是对数据大小的限制，因为R需要将数据一次性先加载到内存。R只支持大约4G大小内存的数据量，一旦达到RAM的阀值，操作将无法继续。 但是，R中有一些包很好地支持了大数据分析，比如，bigmemory包被广泛用于大数据的统计与计算，还有biganalytics,bigtabulate,bigalgebra等包解决了大数据的管理与统计分析的问题。还一个与bigmemory相关联的ff包，它支持用户处理大型向量和矩阵，并能同时操作多个大数据文件，ff包的有一大好处就是它可以像操作原生的R向量一样进行操作，尽管数据不存储在内存中而是常驻在磁盘中。下面是几个简单的事例。 聚类载入大矩阵1234567891011install.packages(\"bigmemory\")install.packages(\"biganalytics\")library(bigmemory)library(biganalytics)x &lt;- read.big.matrix(\"FlightTicketData.csv\", type = 'integer', header = TRUE, backingfile=\"data.bin\", descriptorfile=\"data.desc\")head(x)class(x)xm &lt;- as.matrix(x)nrow(xm)[1] 3156925 聚类分析123456789res_bigkmeans &lt;- lapply(1:10, function(i)&#123; bigkmeans(x, centers = i, iter.max = 50, nstart = 1)&#125;)class(res_bigkmeans)lapply(res_bigkmeans, function(x) x$withinss)var &lt;- sapply(res_bigkmeans, function(x) sum(x$withinss))varplot(1:10, var, type = 'b', xlab = \"Number of clusters\", ylab = \"Percentage of variance explained\") 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950res_big &lt;- bigkmeans(x, centers = 3, iter.max = 50, nstart = 1)res_bigK-means clustering with 3 clusters of sizes 1120691, 919959, 1116275Cluster means: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8][1,] 2.757645 11040.08 1104010 30910.66 0.6813850 0.03740460 1.989817 2211.801[2,] 2.663235 12850.78 1285081 32097.61 0.6323662 0.03459393 2.084982 2305.836[3,] 2.744241 14513.19 1451322 32768.11 0.6545699 0.02660276 1.974971 2390.292 [,9][1,] 1.949151[2,] 1.929160[3,] 1.930394Clustering vector: [1] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 [40] 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 [79] 3 1 1 1 2 3 3 1 1 1 2 2 2 2 2 2 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [118] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [157] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [196] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 3 3 3 1 1 1 1 1 2 2 3 3 1 2 2 1 1 1 1 [235] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 [274] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 1 1 1 1 1 1 1 2 2 [313] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [352] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 [391] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 [430] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 [469] 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 2 3 3 3 3 3 3 3 3 1 1 2 2 1 1 2 2 [508] 2 2 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 [547] 3 3 3 3 3 3 2 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 [586] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [625] 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 [664] 3 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 3 3 3 3 3 3 2 3 1 2 2 2 1 2 1 1 1 1 1 1 1 [703] 1 1 1 1 1 1 1 1 1 1 3 3 1 2 1 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [742] 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 3 3 1 1 2 2 2 3 1 1 1 1 1 1 1 1 3 3 1 2 2 3 1 1 [781] 2 3 3 3 1 3 3 3 3 3 3 3 3 1 3 1 1 1 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [820] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [859] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [898] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [937] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [976] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 [ reached getOption(\"max.print\") -- omitted 3155925 entries ]Within cluster sum of squares by cluster:[1] 2.183142e+15 2.010160e+15 2.466224e+15Available components:[1] \"cluster\" \"centers\" \"withinss\" \"size\" 线性回归分析载入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253install.packages(\"ff\")install.packages(\"biglm\")library(ff)library(biglm)download.file(\"http://www.irs.gov/file_source/pub/irs-soi/12zpallagi.csv\", \"soi.csv\")x &lt;- read.csv.ffdf(file = \"soi.csv\", header = TRUE)class(x)[1] \"ffdf\"head(x)ffdf (all open) dim=c(73732,6), dimorder=c(1,2) row.names=NULLffdf virtual mapping PhysicalName VirtualVmode PhysicalVmode AsIs VirtualIsMatrixSTATEFIPS STATEFIPS integer integer FALSE FALSESTATE STATE integer integer FALSE FALSEzipcode zipcode integer integer FALSE FALSEAGI_STUB AGI_STUB integer integer FALSE FALSEN1 N1 double double FALSE FALSEMARS1 MARS1 double double FALSE FALSE PhysicalIsMatrix PhysicalElementNo PhysicalFirstCol PhysicalLastColSTATEFIPS FALSE 1 1 1STATE FALSE 2 1 1zipcode FALSE 3 1 1AGI_STUB FALSE 4 1 1N1 FALSE 5 1 1MARS1 FALSE 6 1 1 PhysicalIsOpenSTATEFIPS TRUESTATE TRUEzipcode TRUEAGI_STUB TRUEN1 TRUEMARS1 TRUEffdf data STATEFIPS STATE zipcode AGI_STUB N1 MARS11 1 AL 0 1 889920 4908502 1 AL 0 2 491150 1943703 1 AL 0 3 254280 681604 1 AL 0 4 160160 230205 1 AL 0 5 183320 158806 1 AL 0 6 44840 34207 1 AL 35004 1 1600 9908 1 AL 35004 2 1310 570: : : : : : :73725 27 MN 55412 1 4820 289073726 27 MN 55412 2 3020 171073727 27 MN 55412 3 1280 63073728 27 MN 55412 4 590 20073729 27 MN 55412 5 440 9073730 27 MN 55412 6 40 073731 27 MN 55413 1 2630 208073732 27 MN 5 NA NA NA 应用线性回归模型123456789101112131415require(biglm)mymodel &lt;- biglm(A02300 ~ A00200+AGI_STUB+NUMDEP+MARS2, data = x)summary(mymodel)Large data regression model: biglm(A02300 ~ A00200 + AGI_STUB + NUMDEP + MARS2, data = x)Sample size = 73731 Coef (95% CI) SE p(Intercept) -60.0777 -161.8333 41.6779 50.8778 0.2377A00200 -0.0014 -0.0015 -0.0014 0.0000 0.0000AGI_STUB -2.8467 -28.9734 23.2800 13.0634 0.8275NUMDEP 1.0457 1.0421 1.0493 0.0018 0.0000MARS2 -0.3549 -0.3679 -0.3419 0.0065 0.0000summary(mymodel)$rsq[1] 0.9424789 可覆盖解释了94.24789%的线性回归模型看起来相当不错！","tags":[{"name":"大数据","slug":"大数据","permalink":"http://weibo.com/youthcolor/tags/大数据/"},{"name":"聚类","slug":"聚类","permalink":"http://weibo.com/youthcolor/tags/聚类/"},{"name":"线性回归","slug":"线性回归","permalink":"http://weibo.com/youthcolor/tags/线性回归/"}]},{"title":"R机器学习之股票预测初探","date":"2017-07-11T10:53:54.644Z","path":"2017/07/11/R机器学习之股票预测初探/","text":"如今机器学习在工业界和学术界都愈发火热，智能化的崭新时代正在向我们走来。在金融领域，是否也能应用机器学习的方法做股票涨跌的预测呢？下面就以R语言的相关机器学习包为例，简单做股票预测方面的初探。 数据准备昨天，我已经从同花顺中导出了“分众传媒”（股票代码：002027）的历史交易数据，该数据集包含了“时间,开盘,最高,最低,收盘,涨幅,振幅,总手,金额,换手%,成交次数”等维度的数据。 12345678910111213141516171819202122# 载入包library(quantmod);library(TTR);library(randomForest); library(caret);library(corrplot);library(pROC);library(FSelector)XYZQ &lt;- read.csv(\"002027.csv\", header = T, stringsAsFactors = F)names(XYZQ) &lt;- c(\"Index\", \"Open\", \"High\", \"Low\", \"Close\", \"zhangfu\", \"zhenfu\", \"Volume\", \"jine\", \"huanshou\", \"chengjiaocishu\")XYZQ &lt;- XYZQ[XYZQ$Volume != 0, ]tail(XYZQ) Index Open High Low Close zhangfu zhenfu Volume2979 2017-06-15,四 102.94 104.48 101.04 101.53 -1.77% 3.33% 34,873,4182980 2017-06-16,五 101.53 103.15 100.27 102.73 1.18% 2.83% 31,332,3362981 2017-06-19,一 92.55 97.11 92.55 92.76 -9.71% 4.44% 162,266,2502982 2017-06-20,二 93.11 95.57 93.11 94.23 1.58% 2.65% 44,149,7182983 2017-06-21,三 94.30 96.41 91.71 94.58 0.37% 4.99% 41,641,7572984 2017-06-22,四 94.09 97.95 93.88 96.48 2.01% 4.30% 23,065,242 jine huanshou chengjiaocishu2979 503,424,470 0.861 230562980 448,854,630 0.774 211372981 2,141,561,400 4.010 581422982 586,119,870 1.090 269752983 550,581,210 1.030 249892984 313,326,010 0.570 13919 数据预处理下面需要对导出的数据进行处理，使他符合自己期望的格式要求。1234567891011121314151617for(i in 1:nrow(XYZQ))&#123; # XYZQ[i,\"week\"] &lt;- strsplit(as.character(XYZQ[i,\"Index\"]), split = \",\")[[1]][2] # 提取日期 XYZQ[i,1] &lt;- strsplit(as.character(XYZQ[i,1]), split = \",\")[[1]][1] # 去除%号 XYZQ[i,\"zhangfu\"] &lt;- strsplit(as.character(XYZQ[i,\"zhangfu\"]), split = \"%\")[[1]][1] XYZQ[i,\"zhenfu\"] &lt;- strsplit(as.character(XYZQ[i,\"zhenfu\"]), split = \"%\")[[1]][1]&#125;# 类型转换XYZQ[,1] &lt;- as.Date(XYZQ[,1])# XYZQ$week &lt;- as.factor(XYZQ$week)XYZQ$zhangfu &lt;- as.numeric(XYZQ$zhangfu)XYZQ$zhenfu &lt;- as.numeric(XYZQ$zhenfu)XYZQ$Volume &lt;- as.numeric(gsub(\",\",\"\", XYZQ$Volume))XYZQ$jine &lt;- as.numeric(gsub(\",\",\"\", XYZQ$jine))# 时间序列xts类型转换XYZQ &lt;- xts(XYZQ[,-1], order.by = XYZQ[, 1]) 特征提取在R中提供了许多金融指标的TTR包，使用它可以很方便的提取出各种金融指标。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151# 换手率huanshou &lt;- XYZQ$huanshouhuanshou &lt;- as.data.frame(huanshou)$huanshouhuanshou &lt;- c(NA, huanshou)# 振幅zhenfu &lt;- XYZQ$zhenfuzhenfu &lt;- as.data.frame(zhenfu)$zhenfuzhenfu &lt;- c(NA, zhenfu)# 第二日涨跌信号Close &lt;- ifelse(XYZQ$zhangfu &gt; 0, 1, 0)Close &lt;- as.data.frame(Close)$zhangfuClose &lt;- c(Close,NA)# TTR指标myATR &lt;- ATR(HLC(XYZQ))[,'atr'] ; myATR &lt;- c(NA,myATR) ;mySMI &lt;- SMI(HLC(XYZQ))[,'SMI'] ; mySMI &lt;- c(NA,mySMI) ;myADX &lt;- ADX(HLC(XYZQ))[,'ADX'] ; myADX &lt;- c(NA,myADX) ;myAroon &lt;- aroon(XYZQ[,c('High','Low')])$oscillator ; myAroon &lt;- c(NA,myAroon) ;myBB &lt;- BBands(HLC(XYZQ))[,'pctB'] ; myBB &lt;- c(NA,myBB) ;myChaikinVol &lt;- Delt(chaikinVolatility(XYZQ[,c(\"High\",\"Low\")]))[,1] ; myChaikinVol &lt;- c(NA,myChaikinVol) ;myCLV &lt;- EMA(CLV(HLC(XYZQ)))[,1] ; myCLV &lt;- c(NA,myCLV) ;myEMV &lt;- EMV(XYZQ[,c('High','Low')],XYZQ[,'Volume'])[,2] ; myEMV &lt;- c(NA,myEMV) ;myMACD &lt;- MACD(Cl(XYZQ))[,2] ; myMACD &lt;- c(NA,myMACD) ;myMFI &lt;- MFI(XYZQ[,c(\"High\",\"Low\",\"Close\")], XYZQ[,\"Volume\"]) ; myMFI &lt;- c(NA,myMFI) ;mySAR &lt;- SAR(XYZQ[,c('High','Close')]) [,1] ; mySAR &lt;- c(NA,mySAR) ;myVolat &lt;- volatility(OHLC(XYZQ),calc=\"garman\")[,1] ; myVolat &lt;- c(NA,myVolat) ;myCMO &lt;- CMO(Cl(XYZQ)) ; myCMO &lt;- c(NA,myCMO) ;myEMA &lt;- EMA(Delt(Cl(XYZQ))) ; myEMA &lt;- c(NA,myEMA) ;forceindex &lt;- (XYZQ$Close - XYZQ$Open) * XYZQ$Volume ; forceindex &lt;- c(NA,forceindex) ;WillR5 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 5) ; WillR5 &lt;- c(NA,WillR5) ;WillR10 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 10) ; WillR10 &lt;- c(NA,WillR10) ;WillR15 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 15) ; WillR15 &lt;- c(NA,WillR15) ;WillR30 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 30) ; WillR30 &lt;- c(NA,WillR30) ;WillR45 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 45) ; WillR45 &lt;- c(NA,WillR45) ;WillR60 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 60) ; WillR60 &lt;- c(NA,WillR60) ;RSI5 &lt;- RSI(XYZQ$Close, n = 5,maType=\"WMA\") ;RSI5 &lt;- c(NA,RSI5) ;RSI10 &lt;- RSI(XYZQ$Close, n = 10,maType=\"WMA\") ;RSI10 &lt;- c(NA,RSI10) ;RSI15 &lt;- RSI(XYZQ$Close, n = 15,maType=\"WMA\") ;RSI15 &lt;- c(NA,RSI15) ;RSI30 &lt;- RSI(XYZQ$Close, n = 30,maType=\"WMA\") ;RSI30 &lt;- c(NA,RSI30) ;RSI45 &lt;- RSI(XYZQ$Close, n = 45,maType=\"WMA\") ;RSI45 &lt;- c(NA,RSI45) ;RSI60 &lt;- RSI(XYZQ$Close, n = 60,maType=\"WMA\") ;RSI60 &lt;- c(NA,RSI60) ;ROC5 &lt;- ROC(XYZQ$Close, n = 5,type =\"discrete\")*100 ; ROC5 &lt;- c(NA,ROC5) ;ROC10 &lt;- ROC(XYZQ$Close, n = 10,type =\"discrete\")*100 ; ROC10 &lt;- c(NA,ROC10) ;ROC15 &lt;- ROC(XYZQ$Close, n = 15,type =\"discrete\")*100 ; ROC15 &lt;- c(NA,ROC15) ;ROC30 &lt;- ROC(XYZQ$Close, n = 30,type =\"discrete\")*100 ; ROC30 &lt;- c(NA,ROC30) ;ROC45 &lt;- ROC(XYZQ$Close, n = 45,type =\"discrete\")*100 ; ROC45 &lt;- c(NA,ROC45) ;ROC60 &lt;- ROC(XYZQ$Close, n = 60,type =\"discrete\")*100 ; ROC60 &lt;- c(NA,ROC60) ;MOM5 &lt;- momentum(XYZQ$Close, n = 5, na.pad = TRUE) ; MOM5 &lt;- c(NA,MOM5) ;MOM10 &lt;- momentum(XYZQ$Close, n = 10, na.pad = TRUE) ; MOM10 &lt;- c(NA,MOM10) ;MOM15 &lt;- momentum(XYZQ$Close, n = 15, na.pad = TRUE) ; MOM15 &lt;- c(NA,MOM15) ;MOM30 &lt;- momentum(XYZQ$Close, n = 30, na.pad = TRUE) ; MOM30 &lt;- c(NA,MOM30) ;MOM45 &lt;- momentum(XYZQ$Close, n = 45, na.pad = TRUE) ; MOM45 &lt;- c(NA,MOM45) ;MOM60 &lt;- momentum(XYZQ$Close, n = 60, na.pad = TRUE) ; MOM60 &lt;- c(NA,MOM60) ;ATR5 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 5, maType=\"WMA\")[,1] ; ATR5 &lt;- c(NA,ATR5) ;ATR10 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 10, maType=\"WMA\")[,1]; ATR10 &lt;- c(NA,ATR10) ;ATR15 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 15, maType=\"WMA\")[,1]; ATR15 &lt;- c(NA,ATR15) ;ATR30 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 30, maType=\"WMA\")[,1] ; ATR30 &lt;- c(NA,ATR30) ;ATR45 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 45, maType=\"WMA\")[,1]; ATR45 &lt;- c(NA,ATR45) ;ATR60 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 60, maType=\"WMA\")[,1]; ATR60 &lt;- c(NA,ATR60) ;Mean5 &lt;- runMean(Cl(XYZQ), n = 5); Mean5 &lt;- c(NA,Mean5) ;Mean10 &lt;- runMean(Cl(XYZQ), n = 10); Mean10 &lt;- c(NA,Mean10) ;Mean15 &lt;- runMean(Cl(XYZQ), n = 15); Mean15 &lt;- c(NA,Mean15) ;Mean30 &lt;- runMean(Cl(XYZQ), n = 30); Mean30 &lt;- c(NA,Mean30) ;Mean45 &lt;- runMean(Cl(XYZQ), n = 45); Mean45 &lt;- c(NA,Mean45) ;Mean60 &lt;- runMean(Cl(XYZQ), n = 60); Mean60 &lt;- c(NA,Mean60) ;SD5 &lt;- runSD(Cl(XYZQ), n = 5); SD5 &lt;- c(NA,SD5) ;SD10 &lt;- runSD(Cl(XYZQ), n = 10); SD10 &lt;- c(NA,SD10) ;SD15 &lt;- runSD(Cl(XYZQ), n = 15); SD15 &lt;- c(NA,SD15) ;SD30 &lt;- runSD(Cl(XYZQ), n = 30); SD30 &lt;- c(NA,SD30) ;SD45 &lt;- runSD(Cl(XYZQ), n = 45); SD45 &lt;- c(NA,SD45) ;SD60 &lt;- runSD(Cl(XYZQ), n = 60); SD60 &lt;- c(NA,SD60) ;# 创建特征数据集mydataset &lt;- data.frame(Close,myATR,mySMI,myADX,myAroon,myBB,myChaikinVol, myCLV,myEMV,myMACD,myMFI,mySAR,myVolat,myCMO,myEMA, forceindex,WillR5,WillR10,WillR15,WillR30,WillR45,WillR60, RSI5,RSI10,RSI15,RSI30,RSI45,RSI60, ROC5,ROC10,ROC15,ROC30,ROC45,ROC60, MOM5,MOM10,MOM15,MOM30,MOM45,MOM60, ATR5,ATR10,ATR15,ATR30,ATR45,ATR60, Mean5,Mean10,Mean15,Mean30,Mean45,Mean60, SD5,SD10,SD15,SD30,SD45,SD60,zhenfu,huanshou)tail(mydataset)Close myATR mySMI myADX myAroon myBB myChaikinVol myCLV2980 1 3.541269 59.81687 28.90271 55 0.7459216 -1.1581303 -0.139450172981 0 3.494036 58.12533 28.46763 55 0.7118186 0.3881411 0.014692292982 1 3.971604 49.35320 27.41221 55 0.3746141 -3.0420371 -0.153050812983 1 3.888633 39.81658 26.43217 55 0.3783665 0.2083101 -0.141483552984 1 3.946588 31.02765 25.88484 55 0.3630058 -0.8824859 -0.075527162985 NA 3.955403 24.34497 24.82386 55 0.4283426 -6.2060752 -0.01131472 myEMV myMACD myMFI mySAR myVolat myCMO myEMA2980 6.188251e-04 3.383335 82.26928 98.32502 0.3562983 45.267176 0.00084080882981 4.658566e-04 3.476127 77.92001 99.45121 0.3493821 46.711260 0.00283687382982 2.671371e-04 3.360701 65.19533 107.71000 0.3824663 9.544950 -0.01532447062983 -3.034233e-05 3.136216 63.25320 107.71000 0.3857953 7.727144 -0.00965686722984 -4.895251e-04 2.853403 55.06603 107.11200 0.4081373 -4.286628 -0.00722574302985 -2.754052e-04 2.573196 52.62331 106.53792 0.3798513 -1.747815 -0.0022594600 forceindex WillR5 WillR10 WillR15 WillR30 WillR45 WillR602980 -49171519 0.9265367 0.4537445 0.2915094 0.2915094 0.2086428 0.20524742981 37598803 0.6693548 0.4055375 0.2580311 0.2349057 0.1681296 0.16539362982 34075913 0.9854772 0.9861478 0.7746114 0.7051887 0.5047265 0.49651282983 49447684 0.8772827 0.8891821 0.8067026 0.6358491 0.4550979 0.44769182984 11659692 0.7752545 0.8206250 0.7857570 0.6193396 0.4432816 0.43606782985 55125928 0.5830420 0.7018750 0.7018750 0.5297170 0.4060014 0.3755853 RSI5 RSI10 RSI15 RSI30 RSI45 RSI60 ROC5 ROC102980 21.975737 51.34959 62.96328 63.85533 63.19654 62.64153 -1.168111 5.8486242981 31.805274 54.59128 64.75479 65.85176 64.42324 63.65843 -3.367510 5.8635612982 8.578284 23.67412 36.13496 46.71875 50.26581 52.31439 -11.866983 -3.7859142983 19.109948 27.06896 38.44325 48.65479 51.67119 53.47462 -11.246115 -2.8957132984 24.012449 25.28028 37.25706 48.82718 51.85549 53.64726 -8.494582 -7.4287952985 45.294925 33.95349 41.28556 51.58082 53.73532 55.13925 -4.973899 -6.083909 ROC15 ROC30 ROC45 ROC60 MOM5 MOM10 MOM15 MOM30 MOM45 MOM602980 13.5809375 9.125107 19.78528 22.63558 -1.20 5.61 12.14 8.49 16.77 18.742981 14.5645143 12.890110 22.32674 23.14793 -3.58 5.69 13.06 11.73 18.75 19.312982 2.8837622 4.837251 11.01005 14.06788 -12.49 -3.65 2.60 4.28 9.20 11.442983 5.4970891 6.752011 12.67488 18.96225 -11.94 -2.81 4.91 5.96 10.60 15.022984 3.3661202 4.014077 13.28303 19.19345 -8.78 -7.59 3.08 3.65 11.09 15.232985 0.5838198 4.404285 18.94957 23.66060 -5.05 -6.25 0.56 4.07 15.37 18.46 ATR5 ATR10 ATR15 ATR30 ATR45 ATR60 Mean5 Mean10 Mean15 Mean30 Mean452980 3.44 3.44 3.44 3.44 3.44 3.44 104.524 101.801 98.30533 94.60333 92.102442981 2.88 2.88 2.88 2.88 2.88 2.88 103.808 102.370 99.17600 94.99433 92.519112982 10.18 10.18 10.18 10.18 10.18 10.18 101.310 102.005 99.34933 95.13700 92.723562983 2.81 2.81 2.81 2.81 2.81 2.81 98.922 101.724 99.67667 95.33567 92.959112984 4.70 4.70 4.70 4.70 4.70 4.70 97.166 100.965 99.88200 95.45733 93.205562985 4.07 4.07 4.07 4.07 4.07 4.07 96.156 100.340 99.91933 95.59300 93.54711 Mean60 SD5 SD10 SD15 SD30 SD45 SD60 zhenfu huanshou2980 89.86567 2.046040 3.789094 6.124109 5.798568 6.304757 6.834129 3.33 0.8612981 90.18750 1.884840 3.402179 5.724033 5.940947 6.374945 6.978591 2.83 0.7742982 90.37817 5.074185 4.212042 5.465018 5.829461 6.226899 6.887901 4.44 4.0102983 90.62850 5.024527 4.650943 4.943558 5.687187 6.073691 6.746620 2.65 1.0902984 90.88233 4.602253 5.161378 4.633946 5.628425 5.903346 6.600000 4.99 1.0302985 91.19000 3.906678 5.300442 4.601892 5.601426 5.625728 6.418002 4.30 0.570print(dim(mydataset))[1] 2985 60# data.test.tmp &lt;- mydataset[nrow(mydataset),]mydataset.tmp &lt;- na.omit(mydataset[1:(nrow(mydataset)-1),])print(dim(mydataset.tmp))[1] 2923 60y = mydataset.tmp$Closecbind(freq=table(y), percentage=prop.table(table(y))*100) freq percentage0 1384 47.348611 1539 52.65139# correlations = cor(mydataset.tmp[,c(2:58)])# print(head(correlations))# corrplot(correlations, method=\"circle\")set.seed(5)weights &lt;- random.forest.importance(Close~., mydataset.tmp, importance.type = 1)# 选取前十个权重较大的特征subset = cutoff.k(weights, 10)print(subset)[1] \"ROC45\" \"huanshou\" \"WillR60\" \"Mean60\" \"SD15\" \"myCLV\" \"zhenfu\" [8] \"MOM60\" \"SD60\" \"myATR\" 构建训练集、测试集用选取出来的十个权重较大的特征构建训练集测试集123456789101112131415dataset_rf = data.frame(Close,mydataset[,subset[1]],mydataset[,subset[2]], mydataset[,subset[3]],mydataset[,subset[4]], mydataset[,subset[5]],mydataset[,subset[6]], mydataset[,subset[7]],mydataset[,subset[8]], mydataset[,subset[9]],mydataset[,subset[10]])# write.csv(na.omit(dataset_rf),\"XYZQ.csv\", quote = F, row.names = F)# 测试集data.test &lt;- dataset_rf[nrow(dataset_rf),]# 训练集dataset_rf &lt;- na.omit(dataset_rf[1:(nrow(dataset_rf)-1),])dataset_rf$Close &lt;- factor(dataset_rf$Close)# 十次交叉验证trainControl = trainControl(method=\"cv\", number=10)metric = \"Accuracy\" 机器学习模型分别以nnet，naive bayes，rpart，pcaNNet，svmRadial，gbm，xgbTree七个模型进行训练测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# nnetfit.nnet = train(Close~., data=dataset_rf, method=\"nnet\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# naive bayesfit.nb = train(Close~., data=dataset_rf, method=\"nb\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# rpartfit.rpart = train(Close~., data=dataset_rf, method=\"rpart\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# pcaNNetfit.pcaNNet = train(Close~., data=dataset_rf, method=\"pcaNNet\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# svmRadialfit.svmRadial = train(Close~., data=dataset_rf, method=\"svmRadial\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# gbmfit.gbm = train(Close~., data=dataset_rf, method=\"gbm\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# xgbTreefit.xgbTree = train(Close~., data=dataset_rf, method=\"xgbTree\", metric=metric,preProc=c(\"range\"),trControl=trainControl)## Evaluating the algorithms using the \"Accuracy\" metricresults = resamples(list(nnet=fit.nnet,nb=fit.nb,rpart=fit.rpart, pcaNNet=fit.pcaNNet, svmRadial=fit.svmRadial, gbm=fit.gbm, xgbTree=fit.xgbTree))summary(results)Call:summary.resamples(object = results)Models: nnet, nb, rpart, pcaNNet, svmRadial, gbm, xgbTree Number of resamples: 10 Accuracy Min. 1st Qu. Median Mean 3rd Qu. Max. NA'snnet 0.5119454 0.5278005 0.5350531 0.5374879 0.5448303 0.5807560 0nb 0.4914676 0.5179795 0.5435738 0.5371219 0.5593500 0.5616438 0rpart 0.5034247 0.5239726 0.5299161 0.5289004 0.5376712 0.5426621 0pcaNNet 0.5034247 0.5226314 0.5273973 0.5299406 0.5439741 0.5582192 0svmRadial 0.5034247 0.5106918 0.5188356 0.5264996 0.5445205 0.5631399 0gbm 0.5017065 0.5393836 0.5462329 0.5473935 0.5571672 0.5890411 0xgbTree 0.5119454 0.5397722 0.5487166 0.5545761 0.5758279 0.5972696 0Kappa Min. 1st Qu. Median Mean 3rd Qu. Max. NA'snnet -0.018275937 0.019774954 0.03377084 0.03687602 0.05092740 0.12767212 0nb -0.054848141 -0.001926439 0.05667858 0.04044339 0.08712856 0.09197804 0rpart -0.019553073 0.018301156 0.03064388 0.02607515 0.03487674 0.06389776 0pcaNNet -0.037948617 0.001225074 0.02214420 0.02116072 0.04925753 0.07875171 0svmRadial -0.029068637 -0.010983769 0.00421168 0.01941413 0.05314826 0.09151688 0gbm -0.015573809 0.060255860 0.07418475 0.07746468 0.09846185 0.16428163 0xgbTree 0.004939796 0.062262588 0.07858693 0.09198844 0.13641100 0.17330592 0 dotplot(results) 预测涨跌首先根据每个模型的预测效果的平均数，计算出每个模型的权重，再根据这个权重计算出最终的涨跌概率。有点类似于7个常委进行民主投票进行最终的决策。12345678910111213141516171819202122232425262728293031323334353637383940pres.pcaNNet &lt;- predict(fit.pcaNNet, data.test)pres.nnet &lt;- predict(fit.nnet, data.test)pres.xgbTree &lt;- predict(fit.xgbTree, data.test)pres.svmRadial &lt;- predict(fit.svmRadial, data.test)pres.gbm &lt;- predict(fit.gbm, data.test)pres.nb &lt;- predict(fit.nb, data.test)pres.rpart &lt;- predict(fit.rpart, data.test)pres.pcaNNet &lt;- as.numeric(as.character(pres.pcaNNet))pres.nnet &lt;- as.numeric(as.character(pres.nnet))pres.xgbTree &lt;- as.numeric(as.character(pres.xgbTree))pres.svmRadial &lt;- as.numeric(as.character(pres.svmRadial))pres.gbm &lt;- as.numeric(as.character(pres.gbm))pres.nb &lt;- as.numeric(as.character(pres.nb))pres.rpart &lt;- as.numeric(as.character(pres.rpart))pres &lt;- data.frame(pres.pcaNNet,pres.nnet,pres.xgbTree,pres.svmRadial,pres.gbm,pres.nb,pres.rpart) prespres.pcaNNet pres.nnet pres.xgbTree pres.svmRadial pres.gbm pres.nb pres.rpart1 1 1 0 1 0 0 1# names(getModelInfo())results.acc &lt;- as.data.frame(summary(results)[3]$statistics$Accuracy)mean.sum &lt;- sum(results.acc$Mean)mean.pcaNNet &lt;- results.acc[\"pcaNNet\",\"Mean\"]mean.nnet &lt;- results.acc[\"nnet\",\"Mean\"]mean.xgbTree &lt;- results.acc[\"xgbTree\",\"Mean\"]mean.svmRadial &lt;- results.acc[\"svmRadial\",\"Mean\"]mean.gbm &lt;- results.acc[\"gbm\",\"Mean\"]mean.nb &lt;- results.acc[\"nb\",\"Mean\"]mean.rpart &lt;- results.acc[\"rpart\",\"Mean\"]# 加入权重计算涨跌概率q.pcaNNet &lt;- (mean.pcaNNet / mean.sum) * nrow(results.acc)q.nnet &lt;- (mean.nnet / mean.sum) * nrow(results.acc)q.xgbTree &lt;- (mean.xgbTree / mean.sum) * nrow(results.acc)q.svmRadial &lt;- (mean.svmRadial / mean.sum) * nrow(results.acc)q.gbm &lt;- (mean.gbm / mean.sum) * nrow(results.acc)q.nb &lt;- (mean.nb / mean.sum) * nrow(results.acc)q.rpart &lt;- (mean.rpart / mean.sum) * nrow(results.acc)# 最终的预测结果p.result &lt;- mean(c(pres.pcaNNet*q.pcaNNet,pres.nnet*q.nnet,pres.xgbTree*q.xgbTree,pres.svmRadial*q.svmRadial,pres.gbm*q.gbm,pres.nb*q.nb,pres.rpart*q.rpart))p.result[1] 0.5642939 最终计算出的今日涨跌概率为0.56，超出了0.5，那么可以认为，今天的分众传媒的预测结果为涨。实际上，分众传媒今天上涨了0.45%，这是巧合吗？有待进一步的探索。如设置止损点、挖掘更有效的特征以及配合蒙特卡洛方法。","tags":[{"name":"量化金融","slug":"量化金融","permalink":"http://weibo.com/youthcolor/tags/量化金融/"},{"name":"股票预测","slug":"股票预测","permalink":"http://weibo.com/youthcolor/tags/股票预测/"},{"name":"机器学习","slug":"机器学习","permalink":"http://weibo.com/youthcolor/tags/机器学习/"},{"name":"特征选取","slug":"特征选取","permalink":"http://weibo.com/youthcolor/tags/特征选取/"}]},{"title":"Hello World","date":"2017-07-11T09:35:53.024Z","path":"2017/07/11/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]