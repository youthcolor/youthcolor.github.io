[{"title":"译：机器学习算法要点（附Python和R代码）","date":"2017-11-10T13:38:04.328Z","path":"2017/11/10/译：机器学习算法要点（附Python和R代码）/","text":"“谷歌的自动驾驶汽车和机器人受到了很多媒体的关注，但该公司真正的未来在于机器学习，这是一种让计算机变得更智能、更个性化的技术。” ——埃里克.施密特(谷歌董事会主席) 我们可能生活在人类历史上最具决定性的时期。计算从大型主机到pc再到云计算的时代。但它的定义并不是发生了什么，而是未来几年将会发生什么。 对于像我这样的人来说，这段时期令人兴奋的是工具和技术的民主化，这是随着计算机技术的发展而来的。今天，作为一名数据科学家，我可以用复杂的算法构建数据处理机器，每小时几美元。但是，做到这些并不容易！如果我没有没日没夜的付出。 谁将从这份指南获益匪浅创建这个指南的目的是为了简化世界各地有抱负的数据科学家和机器学习爱好者的学习路径。通过本指南，我将使您能够处理机器学习问题并从经验中获益。我对各种机器学习算法以及R和Python代码进行了高水平的理解，并运行它们。这些应该足够让你的手忙碌了。 我故意跳过了这些技术背后的统计知识，因为您在开始时不需要了解它们。所以，如果你在寻找对这些算法的统计理解，你应该去别处看看。但是，如果你想要装备自己来开始建造机器学习项目，你完全可以信任它。 总的来说，有三种类型的机器学习算法 监督学习 它是如何工作的:这个算法由一个变量/因变量(或依赖变量)组成，这个变量是由一个给定的预测集(独立变量)来预测的。使用这组变量，我们生成一个将输入映射到所需输出的函数。培训过程将继续，直到模型达到训练数据所需的精确程度。监督学习的例子:线性回归、决策树、随机森林、最近邻（KNN)、逻辑回归等。 无监督学习 它是如何工作的:在这个算法中，我们没有任何目标或结果变量来预测/估计。它适用于不同组的聚类，广泛用于细分不同群体，以进行特定的干预。无监督学习的例子:关联算法，k-均值。 强化学习 它是如何工作的:使用这个算法，机器被训练来做出具体的决定。它的工作原理是这样的:机器被暴露在一个环境中，在这个环境中，它不断地通过试错来训练自己。这台机器从过去的经验中学习，并试图获取最好的知识以做出准确的决策。强化学习的例子:马尔科夫决策过程。 一般机器学习算法列表 线性回归 逻辑回归 决策树 支持向量机 朴素贝叶斯 最近邻（KNN） K-均值 随机森林 降维算法 梯度下降算法 GBM XGBoost LightGBM CatBoost 1.线性回归它用于估计基于连续变量(s)的实际值(房子的成本，电话的数量，总销售额等)。在这里，我们通过拟合最佳直线，建立了独立变量和依赖变量之间的关系。这条最佳拟合线被称为回归线，由线性方程Y=aX+b表示。 理解线性回归最好的方法就是重温童年的经历。比如说，你让一个五年级的孩子通过体重递增方式排列他的同学，而不问他们的具体重量！你认为孩子会做什么?他/她可能会(在视觉上分析)身高和身材，并利用这些可见的参数来排列他们。这是现实生活中的线性回归！这个孩子实际上已经知道了身高与体重之间的关联，这看起来就像上面的等式。 在这个方程中: Y -因变量 a -斜率 X -独立变量 b -截距 这些系数a和b是基于最小化数据点和回归线之间距离的平方差的。 请看下面的例子。这里我们已经确定了线性方程y=0.2811x+13.9的最佳拟合线。利用这个方程，我们可以计算出一个人的身高。 线性回归主要有两种类型:一元线性回归和多元线性回归。一元线性回归具有一个独立变量的特征。并且，多元线性回归(如名字所示)具有多个(2个以上)独立变量的特征。在寻找最佳拟合线时，你可以选择一个多项式或曲线回归。这些被称为多项式或曲线线性回归。 Python Code 123456789101112131415161718#Import Library#Import other necessary libraries like pandas, numpy...from sklearn import linear_model#Load Train and Test datasets#Identify feature and response variable(s) and values must be numeric and numpy arraysx_train=input_variables_values_training_datasetsy_train=target_variables_values_training_datasetsx_test=input_variables_values_test_datasets# Create linear regression objectlinear = linear_model.LinearRegression()# Train the model using the training sets and check scorelinear.fit(x_train, y_train)linear.score(x_train, y_train)#Equation coefficient and Interceptprint('Coefficient: \\n', linear.coef_)print('Intercept: \\n', linear.intercept_)#Predict Outputpredicted= linear.predict(x_test) R Code 1234567891011#Load Train and Test datasets#Identify feature and response variable(s) and values must be numeric and numpy arraysx_train &lt;- input_variables_values_training_datasetsy_train &lt;- target_variables_values_training_datasetsx_test &lt;- input_variables_values_test_datasetsx &lt;- cbind(x_train,y_train)# Train the model using the training sets and check scorelinear &lt;- lm(y_train ~ ., data = x)summary(linear)#Predict Outputpredicted= predict(linear,x_test) 2.逻辑回归别被它的名字搞糊涂了！它是一种分类而不是回归算法。它用于估计离散值(二进制值，如0/1、yes/no、true/false)，基于给定的独立变量(s)。简单地说，它通过将数据匹配到logit函数来预测事件发生的可能性。因此，它也被称为logit回归。因为它预测了概率，它的输出值在0到1之间(如预期的那样)。 让我们再通过一个简单的例子来理解这个问题。 假设你的朋友给了你出了一道题。只有两种结果——要么你解决了，要么你没有。现在想象一下，你被给予了各种各样的智力测验/小测验，试着去理解你擅长的科目。这个研究的结果是这样的-如果你有一个基于十年级的三角测量法，你有70%的可能会解决这个问题。另一方面，如果这是第五次历史问题，得到答案的概率只有30%。这就是逻辑回归所提供的。 在数学计算中，结果的对数概率被建模为预测变量的线性组合。 123odds= p/ (1-p) = probability of event occurrence / probability of not event occurrenceln(odds) = ln(p/(1-p))logit(p) = ln(p/(1-p)) = b0+b1X1+b2X2+b3X3....+bkXk 上面，p是我们感兴趣的特征的概率。它选择参数来最大化观察样本值的可能性，而不是最小化平方误差的总和(就像普通的回归一样)。 现在，你可能会问，为什么要写一个日志呢?为了简单起见，我们假设这是复制一个阶跃函数的最好的数学方法之一。我可以详细介绍，但这将超出本文的目的。 Python Code 12345678910111213#Import Libraryfrom sklearn.linear_model import LogisticRegression#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create logistic regression objectmodel = LogisticRegression()# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Equation coefficient and Interceptprint('Coefficient: \\n', model.coef_)print('Intercept: \\n', model.intercept_)#Predict Outputpredicted= model.predict(x_test) R Code 123456x &lt;- cbind(x_train,y_train)# Train the model using the training sets and check scorelogistic &lt;- glm(y_train ~ ., data = x,family='binomial')summary(logistic)#Predict Outputpredicted= predict(logistic,x_test) 另外，如果想提高改进模型，有许多不同的方法可以尝试: 包括相互作用项 删减特征 正则化技术 使用非线性模型3.决策树这是我最喜欢的算法之一，我经常使用它。它是一种被监督的学习算法，主要用于分类问题。令人惊讶的是，它适用于分类和连续相关变量。使用这个算法，我们可以把种群分成两个或更多的同类集合。它基于最重要的属性/独立变量，以使其尽可能地成为不同的组。要了解更多细节，您可以阅读有关决策树的建模实现过程。 在上面的图片中，你可以看到，根据不同的属性，人们被划分为四个不同的组，以确定“他们是否会出去玩”。为了将人口分成不同的不同的群体，它使用了各种各样的技术，比如基尼系数、信息增益、卡方、熵。 要理解决策树的工作原理，最好的方法是玩Jezzball——这是微软的一款经典游戏(下图)。基本上，你有一间有移动墙的房间，你需要创建墙壁，这样没有球的最大区域就会被清理。 所以，每次你用一面墙把房间分成两部分，你就会在同一个房间里创造两种不同的人群。决策树以非常相似的方式工作，将一个种群划分为不同的组。 Python Code 123456789101112#Import Library#Import other necessary libraries like pandas, numpy...from sklearn import tree#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create tree object model = tree.DecisionTreeClassifier(criterion='gini') # for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini # model = tree.DecisionTreeRegressor() for regression# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Predict Outputpredicted= model.predict(x_test) R Code 1234567library(rpart)x &lt;- cbind(x_train,y_train)# grow tree fit &lt;- rpart(y_train ~ ., data = x,method=\"class\")summary(fit)#Predict Output predicted= predict(fit,x_test) 4. SVM (支持向量机)这是一种分类方法。在这个算法中，我们将每个数据项绘制成n维空间中的一个点(其中n是您拥有的特性的数量)，每个特性的值都是一个特定坐标的值。 例如，如果我们只有两个特征，比如身高和头发的长度，我们首先会在二维空间中绘制这两个变量，其中每个点都有两个坐标(这些坐标被称为支持向量)。 现在，我们将找到一些在两种不同类别的数据之间分割的线。这条线是这样的，这两个簇两个最近点之间是最远的。 在上面的示例中，将数据划分为两个不同的分组的那条线是黑线，因为这两个最接近的点离黑线最远。这条线是我们的分类器。然后，根据测试数据位置在这条线的哪一边，对新数据进行分类。 如果把这个算法看作是在n维空间中玩JezzBall。游戏的微调是: 你可以在任何角度画线/平面(而不是像传统游戏那样水平或垂直)。 游戏的目的是在不同的房间中分离不同颜色的球。 球是固定不动的。 Python Code12345678910#Import Libraryfrom sklearn import svm#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create SVM classification object model = svm.svc() # there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.# Train the model using the training sets and check scoremodel.fit(X, y)model.score(X, y)#Predict Outputpredicted= model.predict(x_test) R Code1234567library(e1071)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;-svm(y_train ~ ., data = x)summary(fit)#Predict Output predicted= predict(fit,x_test) 5. 朴素贝叶斯这是一种基于贝叶斯定理的分类技术，前提基于预测者之间的独立性假设。简单地讲，朴素贝叶斯的分类器假设一个类的特定特性与其他任何特性的存在无关。例如，如果水果是红色的，圆形的，直径大约3英寸，就可以认为它是苹果。即使这些特征相互依赖，或者其他特征的存在，一个朴素贝叶斯分类器也会考虑所有这些特性各自独立地为这个水果是苹果的概率可能性做出的贡献。 简单的贝叶斯模型很容易构建，对于非常大的数据集尤其有用。除了简洁性，朴素贝叶斯也被认为比高度复杂的分类方法表现出色。 贝叶斯定理提供了一种由P(c)、P(x)和P(x|c)中计算出概率P(c|x)的方法。请看下面的等式: 这里， P(c|x)是分类(目标)的后验概率(属性)。 P(c)是分类的先验概率。 P(x|c)是预测给定分类的概率可能性。 P(x)是预测的先验概率。 例子:让我们用一个例子来理解它。下面我有一组天气预报和相应的目标变量“Play”。现在，我们需要根据天气情况对玩家是否会出去玩进行分类。让我们按照下面的步骤来执行它。 步骤1:将数据集转换为频率表 第2步:通过寻找可能性的概率来创建可能性表，例如:阴天概率=0.29，而玩的概率是0.64。 第三步:用简单的贝叶斯方程来计算每个类的后验概率。具有最高后验概率的类是预测的结果。 问题:如果天气晴朗，球员会出去玩，这句话是正确的吗? 我们可以用上面讨论的方法来解它，所以 P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny) 这里P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64 现在，P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60，这有更高的概率。 Naive Bayes使用了一种类似的方法来预测不同类型的不同属性的可能性。该算法主要用于文本分类和有多个类的问题。 Python Code12345678#Import Libraryfrom sklearn.naive_bayes import GaussianNB#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicted= model.predict(x_test) R Code1234567library(e1071)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;-naiveBayes(y_train ~ ., data = x)summary(fit)#Predict Output predicted= predict(fit,x_test) 6. KNN (K-最近邻)它可以用于分类和回归问题。然而，它更广泛地应用于工业中的分类问题。K最近邻是一个简单的算法，它可以存储所有可用的情况，并通过它的K个邻居的多数投票来对新情况进行分类。被分配到一个类的实例在它的K最近的邻居中也是最常见的，它是由一个距离函数测量的。 这些距离函数可以是欧氏、曼哈顿、闵可夫斯基和海明距离。第3个函数用于连续函数，第4个(海明)用于分类变量。如果K=1,那么，这个实例被简单地分配给它最近的邻居。很多时候，在使用KNN建模时，对于K的选择往往是一个挑战。 KNN很容易被映射到我们的真实生活中。如果你想了解一个你没有任何信息的人，你可能会想了解他的亲密朋友和他的圈子，并由此获得他/她的信息！ 在选择KNN之前要考虑的事情: KNN在计算上代价是昂贵的 变量应该被规范化否则更高范围的变量会导致偏差 在使用KNN之前进行预处理十分重要，比如离群值，消除噪音 Python Code123456789#Import Libraryfrom sklearn.neighbors import KNeighborsClassifier#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create KNeighbors classifier object model KNeighborsClassifier(n_neighbors=6) # default value for n_neighbors is 5# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicte R Code1234567library(knn)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;-knn(y_train ~ ., data = x,k=5)summary(fit)#Predict Output predicted= predict(fit,x_test) 7. K-均值它是一种不受监督的算法，解决了聚类的问题。它的过程遵循一种简单而容易的方法，通过一定数量的簇(假设k个簇)对给定的数据集进行分类。簇中的数据点是同类的，异构的，对等的。 还记得从墨水污渍中找出形状吗?k均值与其有点类似。你看一下这个形状，然后扩散开来，看看有多少不同的簇出现！ k-均值聚簇方法: k-均值为每个中心点挑选出K个点。 每个数据点形成一个具有最接近的中心点的簇，即k个簇。 根据现有的簇成员查找每个簇的中心点。这里有新的中心点。 当我们有新的中心点时，重复第2步和第3步。从新的中心找到每个数据点的最近距离，并与新的k个簇联系起来。重复这个过程直到收敛发生，即中心点不会改变。 如何确定K的值: 在k-均值中，我们有这样的簇，每个簇都有自己的中心点。中心点和一个簇中的数据点之间的平方和构成了该集群的平方值的总和。此外，当添加了所有簇的平方总和时，它就变成了簇解决方案的平方总和。 我们知道，随着集群的数量增加，这个值一直在减少但如果你画出结果，你会发现，距离的平方和k的值会急剧下降，之后会更慢。我们可以以此找到最优簇个数K。 Python Code123456789#Import Libraryfrom sklearn.cluster import KMeans#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset# Create KNeighbors classifier object model k_means = KMeans(n_clusters=3, random_state=0)# Train the model using the training sets and check scoremodel.fit(X)#Predict Outputpredicted= model.predict(x_test) R Code12library(cluster)fit &lt;- kmeans(X, 3) # 3 cluster solution 8. 随机森林随机森林是一组决策树的标术语。在随机森林中，我们收集了决策树(也就是所谓的“森林”)。为了对基于属性的新对象进行分类，每棵树都给出一个分类，我们说这个树给类进行“投票”。森林选择了拥有最多选票的分类。 每棵树都是种下并生长的: 如果训练集的实例数是N，则随机可替换抽取这N个实例的样本。这个样本将作为树生长的训练集。 如果有M个输入变量，那么在每个节点上都指定了一个m值的M变量，m变量是在M中中随机选择的，在这些m中最好的分割是去用来分割节点的。在森林生长过程中，m的值保持不变。 每棵树都在尽可能大的范围内生长。没有修剪。 Python Code123456789#Import Libraryfrom sklearn.ensemble import RandomForestClassifier#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create Random Forest objectmodel= RandomForestClassifier()# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicted= model.predict(x_test) R Code1234567library(randomForest)x &lt;- cbind(x_train,y_train)# Fitting modelfit &lt;- randomForest(Species ~ ., x,ntree=500)summary(fit)#Predict Output predicted= predict(fit,x_test) 9. 降维算法在过去的4-5年中，在每个可能的阶段，数据收集都呈指数级增长。企业/政府机构/研究机构不仅提供了新的信息来源，而且还在详细地收集数据。 例如:电子商务公司正在获取更多关于客户的细节信息，比如他们的人口统计数据、网络爬行历史、他们喜欢或不喜欢的东西、购买历史记录、反馈信息等等，这些都比你最近的杂货店老板更能给他们个性化的关注。 作为一名数据科学家，我们提供的数据也包含许多特性，这对于构建良好的健壮模型来说是很好的，但也有一个挑战。你如何从1000或2000变量中识别出高度显著的变量(s)?在这种情况下，降维算法可以帮助我们，与决策树、随机森林、主成分分析、因子分析等其他算法相结合，以关联矩阵、缺失值比例等为基础进行识别。 Python Code12345678910#Import Libraryfrom sklearn import decomposition#Assumed you have training and test data set as train and test# Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)# For Factor analysis#fa= decomposition.FactorAnalysis()# Reduced the dimension of training dataset using PCAtrain_reduced = pca.fit_transform(train)#Reduced the dimension of test datasettest_reduced = pca.transform(test) R Code1234library(stats)pca &lt;- princomp(train, cor = TRUE)train_reduced &lt;- predict(pca,train)test_reduced &lt;- predict(pca,test) 10. 梯度下降算法 10.1. GBM GBM是一种增强算法，当我们以高预测能力处理大量数据时，它是一种增强算法。增强实际上是一套学习算法的集合，它结合了几个基本估计量的预测，以提高对单一估计量的鲁棒性。它将多个弱或平均的预测因子结合构建到一个强的预测器中。这些增强算法在诸如Kaggle、AV Hackathon、CrowdAnalytix等数据科学竞赛中都很有效。 Python Code123456789#Import Libraryfrom sklearn.ensemble import GradientBoostingClassifier#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset# Create Gradient Boosting Classifier objectmodel= GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)# Train the model using the training sets and check scoremodel.fit(X, y)#Predict Outputpredicted= model.predict(x_test) R Code123456library(caret)x &lt;- cbind(x_train,y_train)# Fitting modelfitControl &lt;- trainControl( method = \"repeatedcv\", number = 4, repeats = 4)fit &lt;- train(y ~ ., data = x, method = \"gbm\", trControl = fitControl,verbose = FALSE)predicted= predict(fit,x_test,type= \"prob\")[,2] 10.2. XGBoost 另一种经典的梯度下降算法，它被认为是在某些Kaggle比赛中获胜与失败之间的决定性选择。 XGBoost具有非常高的预测能力，这使它在预测模型中成为最佳的选择，因为它既具有线性模型又有树学习算法，使得算法比现有的梯度下降技术快了10倍。 它可以支持的功能，包括回归、分类和排名。 XGBoost最有趣的一点是它也被称为规则化的增强技术。这有助于减少过度拟合的建模，并对诸如Scala、Java、R、Python、Julia和C++等一系列语言提供了大量支持。 它也支持分布式大规模训练，包括GCE、AWS、Azure和Yarn 集群。XGBoost也可以与Spark、Flink和其他云数据系统集成在一起，在每次迭代的过程中都建立了交叉验证。 Python Code123456789101112131415from xgboost import XGBClassifierfrom sklearn.model_selection import train_test_splitfrom sklearn.metrics import accuracy_scoreX = dataset[:,0:10]Y = dataset[:,10:]seed = 1X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)model = XGBClassifier()model.fit(X_train, y_train)#Make predictions for test datay_pred = model.predict(X_test) R Code123456789101112131415require(caret)x &lt;- cbind(x_train,y_train)# Fitting modelTrainControl &lt;- trainControl( method = \"repeatedcv\", number = 10, repeats = 4)model&lt;- train(y ~ ., data = x, method = \"xgbLinear\", trControl = TrainControl,verbose = FALSE)OR model&lt;- train(y ~ ., data = x, method = \"xgbTree\", trControl = TrainControl,verbose = FALSE)predicted &lt;- predict(model, x_test) 10.3. LightGBM LightGBM是一个使用基于树的学习算法的梯度增强框架。它的设计是分布式的和高效的，有以下优点: 更快的训练速度和更高的效率 降低内存使用 更好的精度 并行和GPU学习支持 能够处理大规模数据 该框架是一种基于决策树算法的快速和高性能梯度增强，用于排序、分类和许多其他机器学习任务。它是在微软的分布式机器学习工具包项目下开发的。 由于LightGBM是基于决策树算法的，它将树叶和最适合的树分割开，而其他的提升算法将树的深度和水平都分割开，而不是叶节点。因此，当LightGBM在相同的叶子生长时，叶节点的智慧算法能够比水平的算法减少更多的损失，从而产生更好的精度，而这在现有的增强算法中几乎是不可能实现的。 而且，它的速度惊人的快，因此用了“光”这个词。 Python Code1234567891011121314151617data = np.random.rand(500, 10) # 500 entities, each contains 10 featureslabel = np.random.randint(2, size=500) # binary targettrain_data = lgb.Dataset(data, label=label)test_data = train_data.create_valid('test.svm')param = &#123;'num_leaves':31, 'num_trees':100, 'objective':'binary'&#125;param['metric'] = 'auc'num_round = 10bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])bst.save_model('model.txt')# 7 entities, each contains 10 featuresdata = np.random.rand(7, 10)ypred = bst.predict(data) R Code1234567891011121314151617181920212223242526library(RLightGBM)data(example.binary)#Parametersnum_iterations &lt;- 100config &lt;- list(objective = \"binary\", metric=\"binary_logloss,auc\", learning_rate = 0.1, num_leaves = 63, tree_learner = \"serial\", feature_fraction = 0.8, bagging_freq = 5, bagging_fraction = 0.8, min_data_in_leaf = 50, min_sum_hessian_in_leaf = 5.0)#Create data handle and boosterhandle.data &lt;- lgbm.data.create(x)lgbm.data.setField(handle.data, \"label\", y)handle.booster &lt;- lgbm.booster.create(handle.data, lapply(config, as.character))#Train for num_iterations iterations and eval every 5 stepslgbm.booster.train(handle.booster, num_iterations, 5)#Predictpred &lt;- lgbm.booster.predict(handle.booster, x.test)#Test accuracysum(y.test == (y.pred &gt; 0.5)) / length(y.test)#Save model (can be loaded again via lgbm.booster.load(filename))lgbm.booster.save(handle.booster, filename = \"/tmp/model.txt\") 如果您熟悉R中的Caret包，下面是实现LightGBM的另一种方式。1234567891011121314151617require(caret)require(RLightGBM)data(iris)model &lt;-caretModel.LGBM()fit &lt;- train(Species ~ ., data = iris, method=model, verbosity = 0)print(fit)y.pred &lt;- predict(fit, iris[,1:4])library(Matrix)model.sparse &lt;- caretModel.LGBM.sparse()#Generate a sparse matrixmat &lt;- Matrix(as.matrix(iris[,1:4]), sparse = T)fit &lt;- train(data.frame(idx = 1:nrow(iris)), iris$Species, method = model.sparse, matrix = mat, verbosity = 0)print(fit) 10.4. Catboost CatBoost是最近开源的一款来自Yandex的机器学习算法。它可以很容易地与谷歌的TensorFlow和苹果的Core ML等深度学习框架集成。 关于CatBoost的最好的地方是它不需要像其他ML模型那样进行大量的数据进行训练，并且可以处理各种数据格式，也不会影响它的鲁棒性。 在执行算法之前，请确保您已对缺失的数据进行了处理。 Catboost可以自动处理分类变量，而不显示类型转换错误，这有助于您集中精力优化您的模型，而不是处理一些琐碎的错误。 Python Code1234567891011121314151617181920212223242526272829303132import pandas as pdimport numpy as npfrom catboost import CatBoostRegressor#Read training and testing filestrain = pd.read_csv(\"train.csv\")test = pd.read_csv(\"test.csv\")#Imputing missing values for both train and testtrain.fillna(-999, inplace=True)test.fillna(-999,inplace=True)#Creating a training set for modeling and validation set to check model performanceX = train.drop(['Item_Outlet_Sales'], axis=1)y = train.Item_Outlet_Salesfrom sklearn.model_selection import train_test_splitX_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=0.7, random_state=1234)categorical_features_indices = np.where(X.dtypes != np.float)[0]#importing library and building modelfrom catboost import CatBoostRegressormodel=CatBoostRegressor(iterations=50, depth=3, learning_rate=0.1, loss_function='RMSE')model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=True)submission = pd.DataFrame()submission['Item_Identifier'] = test['Item_Identifier']submission['Outlet_Identifier'] = test['Outlet_Identifier']submission['Item_Outlet_Sales'] = model.predict(test) R Code123456789101112131415161718192021222324252627set.seed(1)require(titanic)require(caret)require(catboost)tt &lt;- titanic::titanic_train[complete.cases(titanic::titanic_train),]data &lt;- as.data.frame(as.matrix(tt), stringsAsFactors = TRUE)drop_columns = c(\"PassengerId\", \"Survived\", \"Name\", \"Ticket\", \"Cabin\")x &lt;- data[,!(names(data) %in% drop_columns)]y &lt;- data[,c(\"Survived\")]fit_control &lt;- trainControl(method = \"cv\", number = 4,classProbs = TRUE)grid &lt;- expand.grid(depth = c(4, 6, 8),learning_rate = 0.1,iterations = 100, l2_leaf_reg = 1e-3, rsm = 0.95, border_count = 64)report &lt;- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = TRUE, preProc = NULL,tuneGrid = grid, trControl = fit_control)print(report)importance &lt;- varImp(report, scale = FALSE)print(importance) 写在最后现在，我敢肯定，你会有一种大体如何使用机器学习算法的想法。我写这篇文章并提供R和Python代码的唯一目的就是让你马上开始。如果你想要精通机器学习，那就马上开始吧。解决问题，不断加强对过程的物理理解，应用这些代码，并发现乐趣！ 译自：https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/","tags":[{"name":"线性回归","slug":"线性回归","permalink":"http://weibo.com/youthcolor/tags/线性回归/"},{"name":"机器学习","slug":"机器学习","permalink":"http://weibo.com/youthcolor/tags/机器学习/"},{"name":"逻辑回归","slug":"逻辑回归","permalink":"http://weibo.com/youthcolor/tags/逻辑回归/"},{"name":"决策树","slug":"决策树","permalink":"http://weibo.com/youthcolor/tags/决策树/"},{"name":"支持向量机","slug":"支持向量机","permalink":"http://weibo.com/youthcolor/tags/支持向量机/"},{"name":"朴素贝叶斯","slug":"朴素贝叶斯","permalink":"http://weibo.com/youthcolor/tags/朴素贝叶斯/"},{"name":"最近邻（KNN）","slug":"最近邻（KNN）","permalink":"http://weibo.com/youthcolor/tags/最近邻（KNN）/"},{"name":"K-均值","slug":"K-均值","permalink":"http://weibo.com/youthcolor/tags/K-均值/"},{"name":"随机森林","slug":"随机森林","permalink":"http://weibo.com/youthcolor/tags/随机森林/"},{"name":"降维算法","slug":"降维算法","permalink":"http://weibo.com/youthcolor/tags/降维算法/"},{"name":"梯度下降算法","slug":"梯度下降算法","permalink":"http://weibo.com/youthcolor/tags/梯度下降算法/"},{"name":"GBM","slug":"GBM","permalink":"http://weibo.com/youthcolor/tags/GBM/"},{"name":"XGBoost","slug":"XGBoost","permalink":"http://weibo.com/youthcolor/tags/XGBoost/"},{"name":"LightGBM","slug":"LightGBM","permalink":"http://weibo.com/youthcolor/tags/LightGBM/"},{"name":"CatBoost","slug":"CatBoost","permalink":"http://weibo.com/youthcolor/tags/CatBoost/"}]},{"title":"侦测欺诈交易","date":"2017-08-22T17:16:44.000Z","path":"2017/08/23/侦测欺诈交易/","text":"本案例来自葡萄牙作者Luis Torgo所著的《数据挖掘与Ｒ语言》一书第４章：侦测欺诈交易。 本章介绍了一类数据挖掘问题：离群值排序。特别是采用了一个从不同的方面处理这个任务的数据集。即应用了有监督的、无监督的和半监督的方法。对于应用有限的资源来找到一个现象的异常观测值这一常见问题，本章的应用可以看做是它的一个实例。多个实际问题可以映射到这个通用框架中，例如信用卡、电信和税收的欺诈侦测等。在证券领域，也有这种欺诈侦测一般概念的多个应用。 加载数据123456789# 侦测欺骗交易setwd(\"~/Downloads\")###################################################### Loading the data into R###################################################load('sales.Rdata')library(DMwR)data(sales)head(sales) 探索数据集12345678910111213###################################################### Exploring the dataset###################################################summary(sales)nlevels(sales$ID)nlevels(sales$Prod)length(which(is.na(sales$Quant) &amp; is.na(sales$Val)))sum(is.na(sales$Quant) &amp; is.na(sales$Val))table(sales$Insp)/nrow(sales)*100totS &lt;- table(sales$ID)totP &lt;- table(sales$Prod)barplot(totS,main='Transactions per salespeople',names.arg='',xlab='Salespeople', ylab='Amount') 12barplot(totP,main='Transactions per product',names.arg='',xlab='Products', ylab='Amount') 1234567891011sales$Uprice &lt;- sales$Val/sales$Quantsummary(sales$Uprice)attach(sales)upp &lt;- aggregate(Uprice,list(Prod),median,na.rm=T)topP &lt;- sapply(c(T,F),function(o) upp[order(upp[,2],decreasing=o)[1:5],1])colnames(topP) &lt;- c('Expensive','Cheap')topPtops &lt;- sales[Prod %in% topP[1,],c('Prod','Uprice')]tops$Prod &lt;- factor(tops$Prod)boxplot(Uprice ~ Prod,data=tops,ylab='Uprice',log=\"y\") 123456789101112131415161718192021vs &lt;- aggregate(Val,list(ID),sum,na.rm=T)scoresSs &lt;- sapply(c(T,F),function(o) vs[order(vs$x,decreasing=o)[1:5],1])colnames(scoresSs) &lt;- c('Most','Least')scoresSssum(vs[order(vs$x,decreasing=T)[1:100],2])/sum(Val,na.rm=T)*100sum(vs[order(vs$x,decreasing=F)[1:2000],2])/sum(Val,na.rm=T)*100qs &lt;- aggregate(Quant,list(Prod),sum,na.rm=T)scoresPs &lt;- sapply(c(T,F),function(o) qs[order(qs$x,decreasing=o)[1:5],1])colnames(scoresPs) &lt;- c('Most','Least')scoresPssum(as.double(qs[order(qs$x,decreasing=T)[1:100],2]))/ sum(as.double(Quant),na.rm=T)*100sum(as.double(qs[order(qs$x,decreasing=F)[1:4000],2]))/ sum(as.double(Quant),na.rm=T)*100out &lt;- tapply(Uprice,list(Prod=Prod), function(x) length(boxplot.stats(x)$out))out[order(out,decreasing=T)[1:10]]sum(out)sum(out)/nrow(sales)*100 数据问题1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556###################################################### Data problems###################################################totS &lt;- table(ID)totP &lt;- table(Prod)nas &lt;- sales[which(is.na(Quant) &amp; is.na(Val)),c('ID','Prod')]propS &lt;- 100*table(nas$ID)/totSpropS[order(propS,decreasing=T)[1:10]]propP &lt;- 100*table(nas$Prod)/totPpropP[order(propP,decreasing=T)[1:10]]detach(sales)sales &lt;- sales[-which(is.na(sales$Quant) &amp; is.na(sales$Val)),]nnasQp &lt;- tapply(sales$Quant,list(sales$Prod), function(x) sum(is.na(x)))propNAsQp &lt;- nnasQp/table(sales$Prod)propNAsQp[order(propNAsQp,decreasing=T)[1:10]]sales &lt;- sales[!sales$Prod %in% c('p2442','p2443'),]nlevels(sales$Prod)sales$Prod &lt;- factor(sales$Prod)nlevels(sales$Prod)nnasQs &lt;- tapply(sales$Quant,list(sales$ID),function(x) sum(is.na(x)))propNAsQs &lt;- nnasQs/table(sales$ID)propNAsQs[order(propNAsQs,decreasing=T)[1:10]]nnasVp &lt;- tapply(sales$Val,list(sales$Prod), function(x) sum(is.na(x)))propNAsVp &lt;- nnasVp/table(sales$Prod)propNAsVp[order(propNAsVp,decreasing=T)[1:10]]nnasVs &lt;- tapply(sales$Val,list(sales$ID),function(x) sum(is.na(x)))propNAsVs &lt;- nnasVs/table(sales$ID)propNAsVs[order(propNAsVs,decreasing=T)[1:10]]tPrice &lt;- tapply(sales[sales$Insp != 'fraud','Uprice'], list(sales[sales$Insp != 'fraud','Prod']), median,na.rm=T)noQuant &lt;- which(is.na(sales$Quant))sales[noQuant,'Quant'] &lt;- ceiling(sales[noQuant,'Val'] / tPrice[sales[noQuant,'Prod']])noVal &lt;- which(is.na(sales$Val))sales[noVal,'Val'] &lt;- sales[noVal,'Quant'] * tPrice[sales[noVal,'Prod']]sales$Uprice &lt;- sales$Val/sales$Quantsave(sales,file='salesClean.Rdata')attach(sales)notF &lt;- which(Insp != 'fraud')ms &lt;- tapply(Uprice[notF],list(Prod=Prod[notF]),function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2])&#125;)ms &lt;- matrix(unlist(ms), length(ms),2, byrow=T,dimnames=list(names(ms),c('median','iqr')))head(ms)par(mfrow=c(1,2))plot(ms[,1],ms[,2],xlab='Median',ylab='IQR',main='')plot(ms[,1],ms[,2],xlab='Median',ylab='IQR',main='',col='grey',log=\"xy\")smalls &lt;- which(table(Prod) &lt; 20)points(log(ms[smalls,1]),log(ms[smalls,2]),pch='+') 123456789101112131415161718dms &lt;- scale(ms)smalls &lt;- which(table(Prod) &lt; 20)prods &lt;- tapply(sales$Uprice,sales$Prod,list)similar &lt;- matrix(NA,length(smalls),7,dimnames=list(names(smalls), c('Simil','ks.stat','ks.p','medP','iqrP','medS','iqrS')))for(i in seq(along=smalls)) &#123; d &lt;- scale(dms,dms[smalls[i],],FALSE) d &lt;- sqrt(drop(d^2 %*% rep(1,ncol(d)))) stat &lt;- ks.test(prods[[smalls[i]]],prods[[order(d)[2]]]) similar[i,] &lt;- c(order(d)[2],stat$statistic,stat$p.value,ms[smalls[i],], ms[order(d)[2],])&#125;head(similar)levels(Prod)[similar[1,1]]nrow(similar[similar[,'ks.p'] &gt;= 0.9,])sum(similar[,'ks.p'] &gt;= 0.9)save(similar,file='similarProducts.Rdata') 测试标准12345678910111213141516###################################################### Evaluation criteria###################################################library(ROCR)data(ROCR.simple)pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )perf &lt;- performance(pred,'prec','rec')plot(perf)PRcurve &lt;- function(preds,trues,...) &#123; require(ROCR,quietly=T) pd &lt;- prediction(preds,trues) pf &lt;- performance(pd,'prec','rec') pf@y.values &lt;- lapply(pf@y.values,function(x) rev(cummax(rev(x)))) plot(pf,...)&#125;PRcurve(ROCR.simple$predictions, ROCR.simple$labels ) 1234567891011pred &lt;- prediction( ROCR.simple$predictions, ROCR.simple$labels )perf &lt;- performance(pred,'lift','rpp')plot(perf,main='Lift Chart')CRchart &lt;- function(preds,trues,...) &#123; require(ROCR,quietly=T) pd &lt;- prediction(preds,trues) pf &lt;- performance(pd,'rec','rpp') plot(pf,...)&#125; CRchart(ROCR.simple$predictions, ROCR.simple$labels, main='Cumulative Recall Chart') 12345678910111213141516171819202122avgNDTP &lt;- function(toInsp,train,stats) &#123; if (missing(train) &amp;&amp; missing(stats)) stop('Provide either the training data or the product stats') if (missing(stats)) &#123; notF &lt;- which(train$Insp != 'fraud') stats &lt;- tapply(train$Uprice[notF], list(Prod=train$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;) stats &lt;- matrix(unlist(stats), length(stats),2,byrow=T, dimnames=list(names(stats),c('median','iqr'))) stats[which(stats[,'iqr']==0),'iqr'] &lt;- stats[which(stats[,'iqr']==0),'median'] &#125; mdtp &lt;- mean(abs(toInsp$Uprice-stats[toInsp$Prod,'median']) / stats[toInsp$Prod,'iqr']) return(mdtp)&#125; 实验方法12345678910111213###################################################### Experimental Methodology###################################################evalOutlierRanking &lt;- function(testSet,rankOrder,Threshold,statsProds) &#123; ordTS &lt;- testSet[rankOrder,] N &lt;- nrow(testSet) nF &lt;- if (Threshold &lt; 1) as.integer(Threshold*N) else Threshold cm &lt;- table(c(rep('fraud',nF),rep('ok',N-nF)),ordTS$Insp) prec &lt;- cm['fraud','fraud']/sum(cm['fraud',]) rec &lt;- cm['fraud','fraud']/sum(cm[,'fraud']) AVGndtp &lt;- avgNDTP(ordTS[nF,],stats=statsProds) return(c(Precision=prec,Recall=rec,avgNDTP=AVGndtp))&#125; 修改的箱图规则12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455###################################################### The modified box plot rule###################################################BPrule &lt;- function(train,test) &#123; notF &lt;- which(train$Insp != 'fraud') ms &lt;- tapply(train$Uprice[notF],list(Prod=train$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;) ms &lt;- matrix(unlist(ms),length(ms),2,byrow=T, dimnames=list(names(ms),c('median','iqr'))) ms[which(ms[,'iqr']==0),'iqr'] &lt;- ms[which(ms[,'iqr']==0),'median'] ORscore &lt;- abs(test$Uprice-ms[test$Prod,'median']) / ms[test$Prod,'iqr'] return(list(rankOrder=order(ORscore,decreasing=T), rankScore=ORscore))&#125;notF &lt;- which(sales$Insp != 'fraud')globalStats &lt;- tapply(sales$Uprice[notF], list(Prod=sales$Prod[notF]), function(x) &#123; bp &lt;- boxplot.stats(x)$stats c(median=bp[3],iqr=bp[4]-bp[2]) &#125;)globalStats &lt;- matrix(unlist(globalStats), length(globalStats),2,byrow=T, dimnames=list(names(globalStats),c('median','iqr')))globalStats[which(globalStats[,'iqr']==0),'iqr'] &lt;- globalStats[which(globalStats[,'iqr']==0),'median']ho.BPrule &lt;- function(form, train, test, ...) &#123; res &lt;- BPrule(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;bp.res &lt;- holdOut(learner('ho.BPrule', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(bp.res)par(mfrow=c(1,2))info &lt;- attr(bp.res,'itsInfo')PTs.bp &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',avg='vertical')CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',avg='vertical') 局部离群值因子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051###################################################### Local outlier factors (LOF)###################################################ho.LOF &lt;- function(form, train, test, k, ...) &#123; ntr &lt;- nrow(train) all &lt;- rbind(train,test) N &lt;- nrow(all) ups &lt;- split(all$Uprice,all$Prod) r &lt;- list(length=ups) for(u in seq(along=ups)) r[[u]] &lt;- if (NROW(ups[[u]]) &gt; 3) lofactor(ups[[u]],min(k,NROW(ups[[u]]) %/% 2)) else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) else NULL all$lof &lt;- vector(length=N) split(all$lof,all$Prod) &lt;- r all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] &lt;- SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))]) structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'], decreasing=T),...), itInfo=list(preds=all[(ntr+1):N,'lof'], trues=ifelse(test$Insp=='fraud',1,0)) )&#125;lof.res &lt;- holdOut(learner('ho.LOF', pars=list(k=7,Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(lof.res)par(mfrow=c(1,2))info &lt;- attr(lof.res,'itsInfo')PTs.lof &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')legend('topright',c('BPrule','LOF'),lty=c(1,2))CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')legend('bottomright',c('BPrule','LOF'),lty=c(1,2)) 基于聚类的离群值排名123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960###################################################### Clustering-based outlier rankings (OR_h)###################################################ho.ORh &lt;- function(form, train, test, ...) &#123; require(dprep,quietly=T) ntr &lt;- nrow(train) all &lt;- rbind(train,test) N &lt;- nrow(all) ups &lt;- split(all$Uprice,all$Prod) r &lt;- list(length=ups) for(u in seq(along=ups)) r[[u]] &lt;- if (NROW(ups[[u]]) &gt; 3) outliers.ranking(ups[[u]])$prob.outliers else if (NROW(ups[[u]])) rep(0,NROW(ups[[u]])) else NULL all$lof &lt;- vector(length=N) split(all$lof,all$Prod) &lt;- r all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))] &lt;- SoftMax(all$lof[which(!(is.infinite(all$lof) | is.nan(all$lof)))]) structure(evalOutlierRanking(test,order(all[(ntr+1):N,'lof'], decreasing=T),...), itInfo=list(preds=all[(ntr+1):N,'lof'], trues=ifelse(test$Insp=='fraud',1,0)) )&#125;orh.res &lt;- holdOut(learner('ho.ORh', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(orh.res)par(mfrow=c(1,2))info &lt;- attr(orh.res,'itsInfo')PTs.orh &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.bp[,,1],PTs.bp[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('BPrule','LOF','ORh'), lty=c(1,2,1),col=c('black','black','grey'))CRchart(PTs.bp[,,1],PTs.bp[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.lof[,,1],PTs.lof[,,2], add=T,lty=2, avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical')legend('bottomright',c('BPrule','LOF','ORh'), lty=c(1,2,1),col=c('black','black','grey')) 类失衡问题1234567891011###################################################### The class imbalance problem###################################################data(iris)data &lt;- iris[,c(1,2,5)]data$Species &lt;- factor(ifelse(data$Species == 'setosa','rare','common'))newData &lt;- SMOTE(Species ~ .,data,perc.over=600)table(newData$Species)par(mfrow=c(1,2))plot(data[,1],data[,2],pch=19+as.integer(data[,3]),main='Original Data')plot(newData[,1],newData[,2],pch=19+as.integer(newData[,3]),main=\"SMOTE'd Data\") 简单贝叶斯方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051###################################################### Naive Bayes###################################################nb &lt;- function(train,test) &#123; require(e1071,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) model &lt;- naiveBayes(Insp ~ .,data) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nb &lt;- function(form, train, test, ...) &#123; res &lt;- nb(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nb.res &lt;- holdOut(learner('ho.nb', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nb.res)par(mfrow=c(1,2))info &lt;- attr(nb.res,'itsInfo')PTs.nb &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('NaiveBayes','ORh'), lty=1,col=c('black','grey'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('bottomright',c('NaiveBayes','ORh'), lty=1,col=c('black','grey')) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455nb.s &lt;- function(train,test) &#123; require(e1071,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) newData &lt;- SMOTE(Insp ~ .,data,perc.over=700) model &lt;- naiveBayes(Insp ~ .,newData) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')],type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nbs &lt;- function(form, train, test, ...) &#123; res &lt;- nb.s(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nbs.res &lt;- holdOut(learner('ho.nbs', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nbs.res)par(mfrow=c(1,2))info &lt;- attr(nbs.res,'itsInfo')PTs.nbs &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.nbs[,,1],PTs.nbs[,,2], add=T,lty=2, avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('topright',c('NaiveBayes','smoteNaiveBayes','ORh'), lty=c(1,2,1),col=c('black','black','grey'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.nbs[,,1],PTs.nbs[,,2], add=T,lty=2, avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') legend('bottomright',c('NaiveBayes','smoteNaiveBayes','ORh'), lty=c(1,2,1),col=c('black','black','grey')) AdaBoost方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970###################################################### AdaBoost###################################################library(RWeka)WOW(AdaBoostM1)data(iris)idx &lt;- sample(150,100)model &lt;- AdaBoostM1(Species ~ .,iris[idx,], control=Weka_control(I=100))preds &lt;- predict(model,iris[-idx,])head(preds)table(preds,iris[-idx,'Species'])prob.preds &lt;- predict(model,iris[-idx,],type='probability')head(prob.preds)ab &lt;- function(train,test) &#123; require(RWeka,quietly=T) sup &lt;- which(train$Insp != 'unkn') data &lt;- train[sup,c('ID','Prod','Uprice','Insp')] data$Insp &lt;- factor(data$Insp,levels=c('ok','fraud')) model &lt;- AdaBoostM1(Insp ~ .,data, control=Weka_control(I=100)) preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='probability') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.ab &lt;- function(form, train, test, ...) &#123; res &lt;- ab(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;ab.res &lt;- holdOut(learner('ho.ab', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(ab.res)par(mfrow=c(1,2))info &lt;- attr(ab.res,'itsInfo')PTs.ab &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.ab[,,1],PTs.ab[,,2], add=T,lty=2, avg='vertical') legend('topright',c('NaiveBayes','ORh','AdaBoostM1'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.ab[,,1],PTs.ab[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('NaiveBayes','ORh','AdaBoostM1'), lty=c(1,1,2),col=c('black','grey','black')) 半监督方法12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485###################################################### Semi-supervised approaches###################################################set.seed(1234) # Just to ensrure you get the same results as in the booklibrary(DMwR)library(e1071)data(iris)idx &lt;- sample(150,100)tr &lt;- iris[idx,]ts &lt;- iris[-idx,]nb &lt;- naiveBayes(Species ~ .,tr)table(predict(nb,ts),ts$Species)trST &lt;- trnas &lt;- sample(100,90)trST[nas,'Species'] &lt;- NAfunc &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='raw') data.frame(cl=colnames(p)[apply(p,1,which.max)],p=apply(p,1,max))&#125;nbSTbase &lt;- naiveBayes(Species ~ .,trST[-nas,])table(predict(nbSTbase,ts),ts$Species)nbST &lt;- SelfTrain(Species ~ .,trST,learner('naiveBayes',list()),'func')table(predict(nbST,ts),ts$Species)pred.nb &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='raw') data.frame(cl=colnames(p)[apply(p,1,which.max)], p=apply(p,1,max) )&#125;nb.st &lt;- function(train,test) &#123; require(e1071,quietly=T) train &lt;- train[,c('ID','Prod','Uprice','Insp')] train[which(train$Insp == 'unkn'),'Insp'] &lt;- NA train$Insp &lt;- factor(train$Insp,levels=c('ok','fraud')) model &lt;- SelfTrain(Insp ~ .,train, learner('naiveBayes',list()),'pred.nb') preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='raw') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.nb.st &lt;- function(form, train, test, ...) &#123; res &lt;- nb.st(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;nb.st.res &lt;- holdOut(learner('ho.nb.st', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(nb.st.res)par(mfrow=c(1,2))info &lt;- attr(nb.st.res,'itsInfo')PTs.nb.st &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.nb[,,1],PTs.nb[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.nb.st[,,1],PTs.nb.st[,,2], add=T,lty=2, avg='vertical') legend('topright',c('NaiveBayes','ORh','NaiveBayes-ST'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.nb[,,1],PTs.nb[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.nb.st[,,1],PTs.nb.st[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('NaiveBayes','ORh','NaiveBayes-ST'), lty=c(1,1,2),col=c('black','grey','black')) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364pred.ada &lt;- function(m,d) &#123; p &lt;- predict(m,d,type='probability') data.frame(cl=colnames(p)[apply(p,1,which.max)], p=apply(p,1,max) )&#125;ab.st &lt;- function(train,test) &#123; require(RWeka,quietly=T) train &lt;- train[,c('ID','Prod','Uprice','Insp')] train[which(train$Insp == 'unkn'),'Insp'] &lt;- NA train$Insp &lt;- factor(train$Insp,levels=c('ok','fraud')) model &lt;- SelfTrain(Insp ~ .,train, learner('AdaBoostM1', list(control=Weka_control(I=100))), 'pred.ada') preds &lt;- predict(model,test[,c('ID','Prod','Uprice','Insp')], type='probability') return(list(rankOrder=order(preds[,'fraud'],decreasing=T), rankScore=preds[,'fraud']) )&#125;ho.ab.st &lt;- function(form, train, test, ...) &#123; res &lt;- ab.st(train,test) structure(evalOutlierRanking(test,res$rankOrder,...), itInfo=list(preds=res$rankScore, trues=ifelse(test$Insp=='fraud',1,0) ) )&#125;ab.st.res &lt;- holdOut(learner('ho.ab.st', pars=list(Threshold=0.1, statsProds=globalStats)), dataset(Insp ~ .,sales), hldSettings(3,0.3,1234,T), itsInfo=TRUE)summary(ab.st.res)par(mfrow=c(1,2))info &lt;- attr(ab.st.res,'itsInfo')PTs.ab.st &lt;- aperm(array(unlist(info),dim=c(length(info[[1]]),2,3)), c(1,3,2))PRcurve(PTs.ab[,,1],PTs.ab[,,2], main='PR curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')PRcurve(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') PRcurve(PTs.ab.st[,,1],PTs.ab.st[,,2], add=T,lty=2, avg='vertical') legend('topright',c('AdaBoostM1','ORh','AdaBoostM1-ST'), lty=c(1,1,2),col=c('black','grey','black'))CRchart(PTs.ab[,,1],PTs.ab[,,2], main='Cumulative Recall curve',lty=1,xlim=c(0,1),ylim=c(0,1), avg='vertical')CRchart(PTs.orh[,,1],PTs.orh[,,2], add=T,lty=1,col='grey', avg='vertical') CRchart(PTs.ab.st[,,1],PTs.ab.st[,,2], add=T,lty=2, avg='vertical') legend('bottomright',c('AdaBoostM1','ORh','AdaBoostM1-ST'), lty=c(1,1,2),col=c('black','grey','black'))","tags":[{"name":"聚类","slug":"聚类","permalink":"http://weibo.com/youthcolor/tags/聚类/"},{"name":"欺诈侦测","slug":"欺诈侦测","permalink":"http://weibo.com/youthcolor/tags/欺诈侦测/"},{"name":"半监督分类","slug":"半监督分类","permalink":"http://weibo.com/youthcolor/tags/半监督分类/"},{"name":"类失衡","slug":"类失衡","permalink":"http://weibo.com/youthcolor/tags/类失衡/"},{"name":"贝叶斯分类器","slug":"贝叶斯分类器","permalink":"http://weibo.com/youthcolor/tags/贝叶斯分类器/"},{"name":"AdaBoost分类器","slug":"AdaBoost分类器","permalink":"http://weibo.com/youthcolor/tags/AdaBoost分类器/"},{"name":"决策精确度","slug":"决策精确度","permalink":"http://weibo.com/youthcolor/tags/决策精确度/"},{"name":"回溯精确度","slug":"回溯精确度","permalink":"http://weibo.com/youthcolor/tags/回溯精确度/"},{"name":"交叉验证","slug":"交叉验证","permalink":"http://weibo.com/youthcolor/tags/交叉验证/"},{"name":"离群值侦测排序","slug":"离群值侦测排序","permalink":"http://weibo.com/youthcolor/tags/离群值侦测排序/"}]},{"title":"购物篮关联规则分析","date":"2017-08-18T11:43:34.000Z","path":"2017/08/18/购物篮关联规则分析/","text":"我们生活在一个快速变化的数字世界。在当今时代，亚马逊，淘宝，京东等电商都在购物推荐算法方面做了不同的工作。 这给零售商带来了一个有趣的机遇和挑战。如果你能告诉顾客他们想要买什么，它不仅能提高你的销售量，还能提高顾客体验，最终提高产品价值。 另一方面，如果您无法预测下一次客户将要购买什么，客户可能不会返回您的商店。 在这篇文章中，我们将学习关联规则算法。 数据准备12345678910111213141516171819202122232425262728293031323334353637# install.packages(&quot;pacman&quot;)pacman::p_load(PACKAGE_NAME)pacman::p_load(arules, arulesViz)library(arules)library(arulesViz)data(&quot;Groceries&quot;)# transactions类型str(Groceries)Formal class &apos;transactions&apos; [package &quot;arules&quot;] with 3 slots ..@ data :Formal class &apos;ngCMatrix&apos; [package &quot;Matrix&quot;] with 5 slots .. .. ..@ i : int [1:43367] 13 60 69 78 14 29 98 24 15 29 ... .. .. ..@ p : int [1:9836] 0 4 7 8 12 16 21 22 27 28 ... .. .. ..@ Dim : int [1:2] 169 9835 .. .. ..@ Dimnames:List of 2 .. .. .. ..$ : NULL .. .. .. ..$ : NULL .. .. ..@ factors : list() ..@ itemInfo :&apos;data.frame&apos;: 169 obs. of 3 variables: .. ..$ labels: chr [1:169] &quot;frankfurter&quot; &quot;sausage&quot; &quot;liver loaf&quot; &quot;ham&quot; ... .. ..$ level2: Factor w/ 55 levels &quot;baby food&quot;,&quot;bags&quot;,..: 44 44 44 44 44 44 44 42 42 41 ... .. ..$ level1: Factor w/ 10 levels &quot;canned food&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... ..@ itemsetInfo:&apos;data.frame&apos;: 0 obs. of 0 variables# 查看数据head(Groceries@itemInfo, n=12) labels level2 level11 frankfurter sausage meat and sausage2 sausage sausage meat and sausage3 liver loaf sausage meat and sausage4 ham sausage meat and sausage5 meat sausage meat and sausage6 finished products sausage meat and sausage7 organic sausage sausage meat and sausage8 chicken poultry meat and sausage9 turkey poultry meat and sausage10 pork pork meat and sausage11 beef beef meat and sausage12 hamburger meat beef meat and sausage 自定义数据transactions类型转换123456789101112131415161718192021222324252627282930313233343536data &lt;- list( c(\"a\",\"b\",\"c\"), c(\"a\",\"b\"), c(\"a\",\"b\",\"d\"), c(\"b\",\"e\"), c(\"b\",\"c\",\"e\"), c(\"a\",\"d\",\"e\"), c(\"a\",\"c\"), c(\"a\",\"b\",\"d\"), c(\"c\",\"e\"), c(\"a\",\"b\",\"d\",\"e\"), c(\"a\",'b','e','c'))data &lt;- as(data, \"transactions\")inspect(data) items [1] &#123;a,b,c&#125; [2] &#123;a,b&#125; [3] &#123;a,b,d&#125; [4] &#123;b,e&#125; [5] &#123;b,c,e&#125; [6] &#123;a,d,e&#125; [7] &#123;a,c&#125; [8] &#123;a,b,d&#125; [9] &#123;c,e&#125; [10] &#123;a,b,d,e&#125;[11] &#123;a,b,c,e&#125;# Convert transactions to transaction ID liststl &lt;- as(data, \"tidLists\")inspect(tl) items transationIDs 1 a &#123;1,2,3,6,7,8,10,11&#125;2 b &#123;1,2,3,4,5,8,10,11&#125;3 c &#123;1,5,7,9,11&#125; 4 d &#123;3,6,8,10&#125; 5 e &#123;4,5,6,9,10,11&#125; summary函数查看购买频次最高的商品1234567891011121314151617181920212223242526summary(Groceries)transactions as itemMatrix in sparse format with 9835 rows (elements/itemsets/transactions) and 169 columns (items) and a density of 0.02609146 most frequent items: whole milk other vegetables rolls/buns soda yogurt 2513 1903 1809 1715 1372 (Other) 34055 element (itemset/transaction) length distribution:sizes 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 2159 1643 1299 1005 855 645 545 438 350 246 182 117 78 77 55 46 29 18 19 20 21 22 23 24 26 27 28 29 32 14 14 9 11 4 6 1 1 1 1 3 1 Min. 1st Qu. Median Mean 3rd Qu. Max. 1.000 2.000 3.000 4.409 6.000 32.000 includes extended item information - examples: labels level2 level11 frankfurter sausage meat and sausage2 sausage sausage meat and sausage3 liver loaf sausage meat and sausage 实现关联规则算法123456789101112131415161718192021222324rules &lt;- apriori(Groceries, parameter = list(supp = 0.001, conf = 0.80))inspect(rules[1:10]) lhs rhs support [1] &#123;liquor,red/blush wine&#125; =&gt; &#123;bottled beer&#125; 0.001931876[2] &#123;curd,cereals&#125; =&gt; &#123;whole milk&#125; 0.001016777[3] &#123;yogurt,cereals&#125; =&gt; &#123;whole milk&#125; 0.001728521[4] &#123;butter,jam&#125; =&gt; &#123;whole milk&#125; 0.001016777[5] &#123;soups,bottled beer&#125; =&gt; &#123;whole milk&#125; 0.001118454[6] &#123;napkins,house keeping products&#125; =&gt; &#123;whole milk&#125; 0.001321810[7] &#123;whipped/sour cream,house keeping products&#125; =&gt; &#123;whole milk&#125; 0.001220132[8] &#123;pastry,sweet spreads&#125; =&gt; &#123;whole milk&#125; 0.001016777[9] &#123;turkey,curd&#125; =&gt; &#123;other vegetables&#125; 0.001220132[10] &#123;rice,sugar&#125; =&gt; &#123;whole milk&#125; 0.001220132 confidence lift [1] 0.9047619 11.235269[2] 0.9090909 3.557863[3] 0.8095238 3.168192[4] 0.8333333 3.261374[5] 0.9166667 3.587512[6] 0.8125000 3.179840[7] 0.9230769 3.612599[8] 0.9090909 3.557863[9] 0.8000000 4.134524[10] 1.0000000 3.913649 频率直方图1234library(RColorBrewer)arules::itemFrequencyPlot(Groceries,topN=20,col=brewer.pal(8,'Pastel2'), main='Relative Item Frequency Plot', type=\"relative\",ylab=\"Item Frequency (Relative)\") 支持度和提升度的图形表示1plot(rules[1:20], method = \"graph\", control = list(type = \"items\")) 平行坐标系1plot(rules[1:20], method = \"paracoord\", control = list(reorder = TRUE)) 矩阵表示1plot(rules[1:20], method = \"matrix\", control = list(reorder = TRUE)) 互动式散点图1arulesViz::plotly_arules(rules)","tags":[{"name":"购物篮","slug":"购物篮","permalink":"http://weibo.com/youthcolor/tags/购物篮/"},{"name":"关联规则","slug":"关联规则","permalink":"http://weibo.com/youthcolor/tags/关联规则/"},{"name":"推荐算法","slug":"推荐算法","permalink":"http://weibo.com/youthcolor/tags/推荐算法/"}]},{"title":"R中如何对量化策略进行回测","date":"2017-07-31T16:10:24.000Z","path":"2017/08/01/R中如何对量化策略进行回测/","text":"R中提供了极为便利的回测方法，如PerformanceAnalytics包，下面是简短实例。以A股福田汽车600166.SS为例。 第1步: 获取股票数据12setSymbolLookup(MYSTOCK=list(name='600166.SS',src='yahoo',from='2007-01-01'))getSymbols('MYSTOCK') 第2步: 建立指标12MYSTOCK &lt;- na.omit(MYSTOCK)dvi &lt;- DVI(Cl(MYSTOCK)) 第3步: 构建交易规则1sig &lt;- Lag(ifelse(dvi$dvi &lt; 0.5, 1, -1)) 第4步: 交易规则/资金曲线1234ret &lt;- ROC(Cl(MYSTOCK))*(sig-0.00025)ret &lt;- ret['2013-09-01/2014-06-01']eq &lt;- exp(cumsum(ret))plot(eq) 第5步: 评估量化策略的表现123456789101112131415161718192021222324table.Drawdowns(ret, top=10) From Trough To Depth Length To Trough Recovery1 2014-01-29 2014-02-18 2014-03-31 -0.1574 39 10 292 2013-09-02 2014-01-13 2014-01-28 -0.1246 99 88 113 2014-04-23 2014-05-09 2014-05-19 -0.0484 17 11 64 2014-04-01 2014-04-03 2014-04-08 -0.0206 5 3 25 2014-04-09 2014-04-14 2014-04-15 -0.0163 5 4 16 2014-05-22 2014-05-22 2014-05-26 -0.0061 3 1 27 2014-05-27 2014-05-27 2014-05-28 -0.0060 2 1 18 2014-05-20 2014-05-20 2014-05-21 -0.0041 2 1 1table.DownsideRisk(ret) 600166.SS.CloseSemi Deviation 0.0147Gain Deviation 0.0119Loss Deviation 0.0153Downside Deviation (MAR=210%) 0.0188Downside Deviation (Rf=0%) 0.0143Downside Deviation (0%) 0.0143Maximum Drawdown 0.1574Historical VaR (95%) -0.0268Historical ES (95%) -0.0466Modified VaR (95%) -0.0334Modified ES (95%) -0.0617charts.PerformanceSummary(ret)","tags":[{"name":"量化金融","slug":"量化金融","permalink":"http://weibo.com/youthcolor/tags/量化金融/"},{"name":"回测","slug":"回测","permalink":"http://weibo.com/youthcolor/tags/回测/"}]},{"title":"R中的大数据分析","date":"2017-07-11T10:53:54.000Z","path":"2017/07/11/R中的大数据分析/","text":"尽管R语言是一门十分强大且健壮的统计型语言，但它最大的短板是对数据大小的限制，因为R需要将数据一次性先加载到内存。R只支持大约4G大小内存的数据量，一旦达到RAM的阀值，操作将无法继续。 但是，R中有一些包很好地支持了大数据分析，比如，bigmemory包被广泛用于大数据的统计与计算，还有biganalytics,bigtabulate,bigalgebra等包解决了大数据的管理与统计分析的问题。还一个与bigmemory相关联的ff包，它支持用户处理大型向量和矩阵，并能同时操作多个大数据文件，ff包的有一大好处就是它可以像操作原生的R向量一样进行操作，尽管数据不存储在内存中而是常驻在磁盘中。下面是几个简单的事例。 聚类载入大矩阵1234567891011install.packages(\"bigmemory\")install.packages(\"biganalytics\")library(bigmemory)library(biganalytics)x &lt;- read.big.matrix(\"FlightTicketData.csv\", type = 'integer', header = TRUE, backingfile=\"data.bin\", descriptorfile=\"data.desc\")head(x)class(x)xm &lt;- as.matrix(x)nrow(xm)[1] 3156925 聚类分析123456789res_bigkmeans &lt;- lapply(1:10, function(i)&#123; bigkmeans(x, centers = i, iter.max = 50, nstart = 1)&#125;)class(res_bigkmeans)lapply(res_bigkmeans, function(x) x$withinss)var &lt;- sapply(res_bigkmeans, function(x) sum(x$withinss))varplot(1:10, var, type = 'b', xlab = \"Number of clusters\", ylab = \"Percentage of variance explained\") 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950res_big &lt;- bigkmeans(x, centers = 3, iter.max = 50, nstart = 1)res_bigK-means clustering with 3 clusters of sizes 1120691, 919959, 1116275Cluster means: [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8][1,] 2.757645 11040.08 1104010 30910.66 0.6813850 0.03740460 1.989817 2211.801[2,] 2.663235 12850.78 1285081 32097.61 0.6323662 0.03459393 2.084982 2305.836[3,] 2.744241 14513.19 1451322 32768.11 0.6545699 0.02660276 1.974971 2390.292 [,9][1,] 1.949151[2,] 1.929160[3,] 1.930394Clustering vector: [1] 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 [40] 1 1 1 1 1 1 1 2 2 2 2 2 2 3 3 3 3 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 [79] 3 1 1 1 2 3 3 1 1 1 2 2 2 2 2 2 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [118] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [157] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [196] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 2 2 3 3 3 1 1 1 1 1 2 2 3 3 1 2 2 1 1 1 1 [235] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 [274] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 2 2 2 2 2 3 1 1 1 1 1 1 1 2 2 [313] 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [352] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 [391] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 [430] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 2 [469] 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 1 2 3 3 3 3 3 3 3 3 1 1 2 2 1 1 2 2 [508] 2 2 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 [547] 3 3 3 3 3 3 2 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 [586] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [625] 1 1 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 [664] 3 1 1 1 1 1 1 1 1 1 1 1 2 2 2 3 3 3 3 3 3 3 3 3 2 3 1 2 2 2 1 2 1 1 1 1 1 1 1 [703] 1 1 1 1 1 1 1 1 1 1 3 3 1 2 1 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [742] 3 3 3 3 3 3 3 1 1 2 2 2 2 2 2 3 3 1 1 2 2 2 3 1 1 1 1 1 1 1 1 3 3 1 2 2 3 1 1 [781] 2 3 3 3 1 3 3 3 3 3 3 3 3 1 3 1 1 1 2 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [820] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 [859] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 [898] 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [937] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 [976] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 1 1 1 1 1 1 1 1 1 1 [ reached getOption(\"max.print\") -- omitted 3155925 entries ]Within cluster sum of squares by cluster:[1] 2.183142e+15 2.010160e+15 2.466224e+15Available components:[1] \"cluster\" \"centers\" \"withinss\" \"size\" 线性回归分析载入数据1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253install.packages(\"ff\")install.packages(\"biglm\")library(ff)library(biglm)download.file(\"http://www.irs.gov/file_source/pub/irs-soi/12zpallagi.csv\", \"soi.csv\")x &lt;- read.csv.ffdf(file = \"soi.csv\", header = TRUE)class(x)[1] \"ffdf\"head(x)ffdf (all open) dim=c(73732,6), dimorder=c(1,2) row.names=NULLffdf virtual mapping PhysicalName VirtualVmode PhysicalVmode AsIs VirtualIsMatrixSTATEFIPS STATEFIPS integer integer FALSE FALSESTATE STATE integer integer FALSE FALSEzipcode zipcode integer integer FALSE FALSEAGI_STUB AGI_STUB integer integer FALSE FALSEN1 N1 double double FALSE FALSEMARS1 MARS1 double double FALSE FALSE PhysicalIsMatrix PhysicalElementNo PhysicalFirstCol PhysicalLastColSTATEFIPS FALSE 1 1 1STATE FALSE 2 1 1zipcode FALSE 3 1 1AGI_STUB FALSE 4 1 1N1 FALSE 5 1 1MARS1 FALSE 6 1 1 PhysicalIsOpenSTATEFIPS TRUESTATE TRUEzipcode TRUEAGI_STUB TRUEN1 TRUEMARS1 TRUEffdf data STATEFIPS STATE zipcode AGI_STUB N1 MARS11 1 AL 0 1 889920 4908502 1 AL 0 2 491150 1943703 1 AL 0 3 254280 681604 1 AL 0 4 160160 230205 1 AL 0 5 183320 158806 1 AL 0 6 44840 34207 1 AL 35004 1 1600 9908 1 AL 35004 2 1310 570: : : : : : :73725 27 MN 55412 1 4820 289073726 27 MN 55412 2 3020 171073727 27 MN 55412 3 1280 63073728 27 MN 55412 4 590 20073729 27 MN 55412 5 440 9073730 27 MN 55412 6 40 073731 27 MN 55413 1 2630 208073732 27 MN 5 NA NA NA 应用线性回归模型123456789101112131415require(biglm)mymodel &lt;- biglm(A02300 ~ A00200+AGI_STUB+NUMDEP+MARS2, data = x)summary(mymodel)Large data regression model: biglm(A02300 ~ A00200 + AGI_STUB + NUMDEP + MARS2, data = x)Sample size = 73731 Coef (95% CI) SE p(Intercept) -60.0777 -161.8333 41.6779 50.8778 0.2377A00200 -0.0014 -0.0015 -0.0014 0.0000 0.0000AGI_STUB -2.8467 -28.9734 23.2800 13.0634 0.8275NUMDEP 1.0457 1.0421 1.0493 0.0018 0.0000MARS2 -0.3549 -0.3679 -0.3419 0.0065 0.0000summary(mymodel)$rsq[1] 0.9424789 可覆盖解释了94.24789%的线性回归模型看起来相当不错！","tags":[{"name":"大数据","slug":"大数据","permalink":"http://weibo.com/youthcolor/tags/大数据/"},{"name":"聚类","slug":"聚类","permalink":"http://weibo.com/youthcolor/tags/聚类/"},{"name":"线性回归","slug":"线性回归","permalink":"http://weibo.com/youthcolor/tags/线性回归/"}]},{"title":"R机器学习之股票预测初探","date":"2017-07-11T10:53:54.000Z","path":"2017/07/11/R机器学习之股票预测初探/","text":"如今机器学习在工业界和学术界都愈发火热，智能化的崭新时代正在向我们走来。在金融领域，是否也能应用机器学习的方法做股票涨跌的预测呢？下面就以R语言的相关机器学习包为例，简单做股票预测方面的初探。 数据准备昨天，我已经从同花顺中导出了“分众传媒”（股票代码：002027）的历史交易数据，该数据集包含了“时间,开盘,最高,最低,收盘,涨幅,振幅,总手,金额,换手%,成交次数”等维度的数据。 12345678910111213141516171819202122# 载入包library(quantmod);library(TTR);library(randomForest); library(caret);library(corrplot);library(pROC);library(FSelector)XYZQ &lt;- read.csv(\"002027.csv\", header = T, stringsAsFactors = F)names(XYZQ) &lt;- c(\"Index\", \"Open\", \"High\", \"Low\", \"Close\", \"zhangfu\", \"zhenfu\", \"Volume\", \"jine\", \"huanshou\", \"chengjiaocishu\")XYZQ &lt;- XYZQ[XYZQ$Volume != 0, ]tail(XYZQ) Index Open High Low Close zhangfu zhenfu Volume2979 2017-06-15,四 102.94 104.48 101.04 101.53 -1.77% 3.33% 34,873,4182980 2017-06-16,五 101.53 103.15 100.27 102.73 1.18% 2.83% 31,332,3362981 2017-06-19,一 92.55 97.11 92.55 92.76 -9.71% 4.44% 162,266,2502982 2017-06-20,二 93.11 95.57 93.11 94.23 1.58% 2.65% 44,149,7182983 2017-06-21,三 94.30 96.41 91.71 94.58 0.37% 4.99% 41,641,7572984 2017-06-22,四 94.09 97.95 93.88 96.48 2.01% 4.30% 23,065,242 jine huanshou chengjiaocishu2979 503,424,470 0.861 230562980 448,854,630 0.774 211372981 2,141,561,400 4.010 581422982 586,119,870 1.090 269752983 550,581,210 1.030 249892984 313,326,010 0.570 13919 数据预处理下面需要对导出的数据进行处理，使他符合自己期望的格式要求。1234567891011121314151617for(i in 1:nrow(XYZQ))&#123; # XYZQ[i,\"week\"] &lt;- strsplit(as.character(XYZQ[i,\"Index\"]), split = \",\")[[1]][2] # 提取日期 XYZQ[i,1] &lt;- strsplit(as.character(XYZQ[i,1]), split = \",\")[[1]][1] # 去除%号 XYZQ[i,\"zhangfu\"] &lt;- strsplit(as.character(XYZQ[i,\"zhangfu\"]), split = \"%\")[[1]][1] XYZQ[i,\"zhenfu\"] &lt;- strsplit(as.character(XYZQ[i,\"zhenfu\"]), split = \"%\")[[1]][1]&#125;# 类型转换XYZQ[,1] &lt;- as.Date(XYZQ[,1])# XYZQ$week &lt;- as.factor(XYZQ$week)XYZQ$zhangfu &lt;- as.numeric(XYZQ$zhangfu)XYZQ$zhenfu &lt;- as.numeric(XYZQ$zhenfu)XYZQ$Volume &lt;- as.numeric(gsub(\",\",\"\", XYZQ$Volume))XYZQ$jine &lt;- as.numeric(gsub(\",\",\"\", XYZQ$jine))# 时间序列xts类型转换XYZQ &lt;- xts(XYZQ[,-1], order.by = XYZQ[, 1]) 特征提取在R中提供了许多金融指标的TTR包，使用它可以很方便的提取出各种金融指标。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151# 换手率huanshou &lt;- XYZQ$huanshouhuanshou &lt;- as.data.frame(huanshou)$huanshouhuanshou &lt;- c(NA, huanshou)# 振幅zhenfu &lt;- XYZQ$zhenfuzhenfu &lt;- as.data.frame(zhenfu)$zhenfuzhenfu &lt;- c(NA, zhenfu)# 第二日涨跌信号Close &lt;- ifelse(XYZQ$zhangfu &gt; 0, 1, 0)Close &lt;- as.data.frame(Close)$zhangfuClose &lt;- c(Close,NA)# TTR指标myATR &lt;- ATR(HLC(XYZQ))[,'atr'] ; myATR &lt;- c(NA,myATR) ;mySMI &lt;- SMI(HLC(XYZQ))[,'SMI'] ; mySMI &lt;- c(NA,mySMI) ;myADX &lt;- ADX(HLC(XYZQ))[,'ADX'] ; myADX &lt;- c(NA,myADX) ;myAroon &lt;- aroon(XYZQ[,c('High','Low')])$oscillator ; myAroon &lt;- c(NA,myAroon) ;myBB &lt;- BBands(HLC(XYZQ))[,'pctB'] ; myBB &lt;- c(NA,myBB) ;myChaikinVol &lt;- Delt(chaikinVolatility(XYZQ[,c(\"High\",\"Low\")]))[,1] ; myChaikinVol &lt;- c(NA,myChaikinVol) ;myCLV &lt;- EMA(CLV(HLC(XYZQ)))[,1] ; myCLV &lt;- c(NA,myCLV) ;myEMV &lt;- EMV(XYZQ[,c('High','Low')],XYZQ[,'Volume'])[,2] ; myEMV &lt;- c(NA,myEMV) ;myMACD &lt;- MACD(Cl(XYZQ))[,2] ; myMACD &lt;- c(NA,myMACD) ;myMFI &lt;- MFI(XYZQ[,c(\"High\",\"Low\",\"Close\")], XYZQ[,\"Volume\"]) ; myMFI &lt;- c(NA,myMFI) ;mySAR &lt;- SAR(XYZQ[,c('High','Close')]) [,1] ; mySAR &lt;- c(NA,mySAR) ;myVolat &lt;- volatility(OHLC(XYZQ),calc=\"garman\")[,1] ; myVolat &lt;- c(NA,myVolat) ;myCMO &lt;- CMO(Cl(XYZQ)) ; myCMO &lt;- c(NA,myCMO) ;myEMA &lt;- EMA(Delt(Cl(XYZQ))) ; myEMA &lt;- c(NA,myEMA) ;forceindex &lt;- (XYZQ$Close - XYZQ$Open) * XYZQ$Volume ; forceindex &lt;- c(NA,forceindex) ;WillR5 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 5) ; WillR5 &lt;- c(NA,WillR5) ;WillR10 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 10) ; WillR10 &lt;- c(NA,WillR10) ;WillR15 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 15) ; WillR15 &lt;- c(NA,WillR15) ;WillR30 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 30) ; WillR30 &lt;- c(NA,WillR30) ;WillR45 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 45) ; WillR45 &lt;- c(NA,WillR45) ;WillR60 &lt;- WPR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 60) ; WillR60 &lt;- c(NA,WillR60) ;RSI5 &lt;- RSI(XYZQ$Close, n = 5,maType=\"WMA\") ;RSI5 &lt;- c(NA,RSI5) ;RSI10 &lt;- RSI(XYZQ$Close, n = 10,maType=\"WMA\") ;RSI10 &lt;- c(NA,RSI10) ;RSI15 &lt;- RSI(XYZQ$Close, n = 15,maType=\"WMA\") ;RSI15 &lt;- c(NA,RSI15) ;RSI30 &lt;- RSI(XYZQ$Close, n = 30,maType=\"WMA\") ;RSI30 &lt;- c(NA,RSI30) ;RSI45 &lt;- RSI(XYZQ$Close, n = 45,maType=\"WMA\") ;RSI45 &lt;- c(NA,RSI45) ;RSI60 &lt;- RSI(XYZQ$Close, n = 60,maType=\"WMA\") ;RSI60 &lt;- c(NA,RSI60) ;ROC5 &lt;- ROC(XYZQ$Close, n = 5,type =\"discrete\")*100 ; ROC5 &lt;- c(NA,ROC5) ;ROC10 &lt;- ROC(XYZQ$Close, n = 10,type =\"discrete\")*100 ; ROC10 &lt;- c(NA,ROC10) ;ROC15 &lt;- ROC(XYZQ$Close, n = 15,type =\"discrete\")*100 ; ROC15 &lt;- c(NA,ROC15) ;ROC30 &lt;- ROC(XYZQ$Close, n = 30,type =\"discrete\")*100 ; ROC30 &lt;- c(NA,ROC30) ;ROC45 &lt;- ROC(XYZQ$Close, n = 45,type =\"discrete\")*100 ; ROC45 &lt;- c(NA,ROC45) ;ROC60 &lt;- ROC(XYZQ$Close, n = 60,type =\"discrete\")*100 ; ROC60 &lt;- c(NA,ROC60) ;MOM5 &lt;- momentum(XYZQ$Close, n = 5, na.pad = TRUE) ; MOM5 &lt;- c(NA,MOM5) ;MOM10 &lt;- momentum(XYZQ$Close, n = 10, na.pad = TRUE) ; MOM10 &lt;- c(NA,MOM10) ;MOM15 &lt;- momentum(XYZQ$Close, n = 15, na.pad = TRUE) ; MOM15 &lt;- c(NA,MOM15) ;MOM30 &lt;- momentum(XYZQ$Close, n = 30, na.pad = TRUE) ; MOM30 &lt;- c(NA,MOM30) ;MOM45 &lt;- momentum(XYZQ$Close, n = 45, na.pad = TRUE) ; MOM45 &lt;- c(NA,MOM45) ;MOM60 &lt;- momentum(XYZQ$Close, n = 60, na.pad = TRUE) ; MOM60 &lt;- c(NA,MOM60) ;ATR5 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 5, maType=\"WMA\")[,1] ; ATR5 &lt;- c(NA,ATR5) ;ATR10 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 10, maType=\"WMA\")[,1]; ATR10 &lt;- c(NA,ATR10) ;ATR15 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 15, maType=\"WMA\")[,1]; ATR15 &lt;- c(NA,ATR15) ;ATR30 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 30, maType=\"WMA\")[,1] ; ATR30 &lt;- c(NA,ATR30) ;ATR45 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 45, maType=\"WMA\")[,1]; ATR45 &lt;- c(NA,ATR45) ;ATR60 &lt;- ATR(XYZQ[,c(\"High\",\"Low\",\"Close\")], n = 60, maType=\"WMA\")[,1]; ATR60 &lt;- c(NA,ATR60) ;Mean5 &lt;- runMean(Cl(XYZQ), n = 5); Mean5 &lt;- c(NA,Mean5) ;Mean10 &lt;- runMean(Cl(XYZQ), n = 10); Mean10 &lt;- c(NA,Mean10) ;Mean15 &lt;- runMean(Cl(XYZQ), n = 15); Mean15 &lt;- c(NA,Mean15) ;Mean30 &lt;- runMean(Cl(XYZQ), n = 30); Mean30 &lt;- c(NA,Mean30) ;Mean45 &lt;- runMean(Cl(XYZQ), n = 45); Mean45 &lt;- c(NA,Mean45) ;Mean60 &lt;- runMean(Cl(XYZQ), n = 60); Mean60 &lt;- c(NA,Mean60) ;SD5 &lt;- runSD(Cl(XYZQ), n = 5); SD5 &lt;- c(NA,SD5) ;SD10 &lt;- runSD(Cl(XYZQ), n = 10); SD10 &lt;- c(NA,SD10) ;SD15 &lt;- runSD(Cl(XYZQ), n = 15); SD15 &lt;- c(NA,SD15) ;SD30 &lt;- runSD(Cl(XYZQ), n = 30); SD30 &lt;- c(NA,SD30) ;SD45 &lt;- runSD(Cl(XYZQ), n = 45); SD45 &lt;- c(NA,SD45) ;SD60 &lt;- runSD(Cl(XYZQ), n = 60); SD60 &lt;- c(NA,SD60) ;# 创建特征数据集mydataset &lt;- data.frame(Close,myATR,mySMI,myADX,myAroon,myBB,myChaikinVol, myCLV,myEMV,myMACD,myMFI,mySAR,myVolat,myCMO,myEMA, forceindex,WillR5,WillR10,WillR15,WillR30,WillR45,WillR60, RSI5,RSI10,RSI15,RSI30,RSI45,RSI60, ROC5,ROC10,ROC15,ROC30,ROC45,ROC60, MOM5,MOM10,MOM15,MOM30,MOM45,MOM60, ATR5,ATR10,ATR15,ATR30,ATR45,ATR60, Mean5,Mean10,Mean15,Mean30,Mean45,Mean60, SD5,SD10,SD15,SD30,SD45,SD60,zhenfu,huanshou)tail(mydataset)Close myATR mySMI myADX myAroon myBB myChaikinVol myCLV2980 1 3.541269 59.81687 28.90271 55 0.7459216 -1.1581303 -0.139450172981 0 3.494036 58.12533 28.46763 55 0.7118186 0.3881411 0.014692292982 1 3.971604 49.35320 27.41221 55 0.3746141 -3.0420371 -0.153050812983 1 3.888633 39.81658 26.43217 55 0.3783665 0.2083101 -0.141483552984 1 3.946588 31.02765 25.88484 55 0.3630058 -0.8824859 -0.075527162985 NA 3.955403 24.34497 24.82386 55 0.4283426 -6.2060752 -0.01131472 myEMV myMACD myMFI mySAR myVolat myCMO myEMA2980 6.188251e-04 3.383335 82.26928 98.32502 0.3562983 45.267176 0.00084080882981 4.658566e-04 3.476127 77.92001 99.45121 0.3493821 46.711260 0.00283687382982 2.671371e-04 3.360701 65.19533 107.71000 0.3824663 9.544950 -0.01532447062983 -3.034233e-05 3.136216 63.25320 107.71000 0.3857953 7.727144 -0.00965686722984 -4.895251e-04 2.853403 55.06603 107.11200 0.4081373 -4.286628 -0.00722574302985 -2.754052e-04 2.573196 52.62331 106.53792 0.3798513 -1.747815 -0.0022594600 forceindex WillR5 WillR10 WillR15 WillR30 WillR45 WillR602980 -49171519 0.9265367 0.4537445 0.2915094 0.2915094 0.2086428 0.20524742981 37598803 0.6693548 0.4055375 0.2580311 0.2349057 0.1681296 0.16539362982 34075913 0.9854772 0.9861478 0.7746114 0.7051887 0.5047265 0.49651282983 49447684 0.8772827 0.8891821 0.8067026 0.6358491 0.4550979 0.44769182984 11659692 0.7752545 0.8206250 0.7857570 0.6193396 0.4432816 0.43606782985 55125928 0.5830420 0.7018750 0.7018750 0.5297170 0.4060014 0.3755853 RSI5 RSI10 RSI15 RSI30 RSI45 RSI60 ROC5 ROC102980 21.975737 51.34959 62.96328 63.85533 63.19654 62.64153 -1.168111 5.8486242981 31.805274 54.59128 64.75479 65.85176 64.42324 63.65843 -3.367510 5.8635612982 8.578284 23.67412 36.13496 46.71875 50.26581 52.31439 -11.866983 -3.7859142983 19.109948 27.06896 38.44325 48.65479 51.67119 53.47462 -11.246115 -2.8957132984 24.012449 25.28028 37.25706 48.82718 51.85549 53.64726 -8.494582 -7.4287952985 45.294925 33.95349 41.28556 51.58082 53.73532 55.13925 -4.973899 -6.083909 ROC15 ROC30 ROC45 ROC60 MOM5 MOM10 MOM15 MOM30 MOM45 MOM602980 13.5809375 9.125107 19.78528 22.63558 -1.20 5.61 12.14 8.49 16.77 18.742981 14.5645143 12.890110 22.32674 23.14793 -3.58 5.69 13.06 11.73 18.75 19.312982 2.8837622 4.837251 11.01005 14.06788 -12.49 -3.65 2.60 4.28 9.20 11.442983 5.4970891 6.752011 12.67488 18.96225 -11.94 -2.81 4.91 5.96 10.60 15.022984 3.3661202 4.014077 13.28303 19.19345 -8.78 -7.59 3.08 3.65 11.09 15.232985 0.5838198 4.404285 18.94957 23.66060 -5.05 -6.25 0.56 4.07 15.37 18.46 ATR5 ATR10 ATR15 ATR30 ATR45 ATR60 Mean5 Mean10 Mean15 Mean30 Mean452980 3.44 3.44 3.44 3.44 3.44 3.44 104.524 101.801 98.30533 94.60333 92.102442981 2.88 2.88 2.88 2.88 2.88 2.88 103.808 102.370 99.17600 94.99433 92.519112982 10.18 10.18 10.18 10.18 10.18 10.18 101.310 102.005 99.34933 95.13700 92.723562983 2.81 2.81 2.81 2.81 2.81 2.81 98.922 101.724 99.67667 95.33567 92.959112984 4.70 4.70 4.70 4.70 4.70 4.70 97.166 100.965 99.88200 95.45733 93.205562985 4.07 4.07 4.07 4.07 4.07 4.07 96.156 100.340 99.91933 95.59300 93.54711 Mean60 SD5 SD10 SD15 SD30 SD45 SD60 zhenfu huanshou2980 89.86567 2.046040 3.789094 6.124109 5.798568 6.304757 6.834129 3.33 0.8612981 90.18750 1.884840 3.402179 5.724033 5.940947 6.374945 6.978591 2.83 0.7742982 90.37817 5.074185 4.212042 5.465018 5.829461 6.226899 6.887901 4.44 4.0102983 90.62850 5.024527 4.650943 4.943558 5.687187 6.073691 6.746620 2.65 1.0902984 90.88233 4.602253 5.161378 4.633946 5.628425 5.903346 6.600000 4.99 1.0302985 91.19000 3.906678 5.300442 4.601892 5.601426 5.625728 6.418002 4.30 0.570print(dim(mydataset))[1] 2985 60# data.test.tmp &lt;- mydataset[nrow(mydataset),]mydataset.tmp &lt;- na.omit(mydataset[1:(nrow(mydataset)-1),])print(dim(mydataset.tmp))[1] 2923 60y = mydataset.tmp$Closecbind(freq=table(y), percentage=prop.table(table(y))*100) freq percentage0 1384 47.348611 1539 52.65139# correlations = cor(mydataset.tmp[,c(2:58)])# print(head(correlations))# corrplot(correlations, method=\"circle\")set.seed(5)weights &lt;- random.forest.importance(Close~., mydataset.tmp, importance.type = 1)# 选取前十个权重较大的特征subset = cutoff.k(weights, 10)print(subset)[1] \"ROC45\" \"huanshou\" \"WillR60\" \"Mean60\" \"SD15\" \"myCLV\" \"zhenfu\" [8] \"MOM60\" \"SD60\" \"myATR\" 构建训练集、测试集用选取出来的十个权重较大的特征构建训练集测试集123456789101112131415dataset_rf = data.frame(Close,mydataset[,subset[1]],mydataset[,subset[2]], mydataset[,subset[3]],mydataset[,subset[4]], mydataset[,subset[5]],mydataset[,subset[6]], mydataset[,subset[7]],mydataset[,subset[8]], mydataset[,subset[9]],mydataset[,subset[10]])# write.csv(na.omit(dataset_rf),\"XYZQ.csv\", quote = F, row.names = F)# 测试集data.test &lt;- dataset_rf[nrow(dataset_rf),]# 训练集dataset_rf &lt;- na.omit(dataset_rf[1:(nrow(dataset_rf)-1),])dataset_rf$Close &lt;- factor(dataset_rf$Close)# 十次交叉验证trainControl = trainControl(method=\"cv\", number=10)metric = \"Accuracy\" 机器学习模型分别以nnet，naive bayes，rpart，pcaNNet，svmRadial，gbm，xgbTree七个模型进行训练测试12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# nnetfit.nnet = train(Close~., data=dataset_rf, method=\"nnet\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# naive bayesfit.nb = train(Close~., data=dataset_rf, method=\"nb\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# rpartfit.rpart = train(Close~., data=dataset_rf, method=\"rpart\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# pcaNNetfit.pcaNNet = train(Close~., data=dataset_rf, method=\"pcaNNet\", metric=metric, preProc=c(\"range\"),trControl=trainControl)# svmRadialfit.svmRadial = train(Close~., data=dataset_rf, method=\"svmRadial\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# gbmfit.gbm = train(Close~., data=dataset_rf, method=\"gbm\", metric=metric,preProc=c(\"range\"),trControl=trainControl)# xgbTreefit.xgbTree = train(Close~., data=dataset_rf, method=\"xgbTree\", metric=metric,preProc=c(\"range\"),trControl=trainControl)## Evaluating the algorithms using the \"Accuracy\" metricresults = resamples(list(nnet=fit.nnet,nb=fit.nb,rpart=fit.rpart, pcaNNet=fit.pcaNNet, svmRadial=fit.svmRadial, gbm=fit.gbm, xgbTree=fit.xgbTree))summary(results)Call:summary.resamples(object = results)Models: nnet, nb, rpart, pcaNNet, svmRadial, gbm, xgbTree Number of resamples: 10 Accuracy Min. 1st Qu. Median Mean 3rd Qu. Max. NA'snnet 0.5119454 0.5278005 0.5350531 0.5374879 0.5448303 0.5807560 0nb 0.4914676 0.5179795 0.5435738 0.5371219 0.5593500 0.5616438 0rpart 0.5034247 0.5239726 0.5299161 0.5289004 0.5376712 0.5426621 0pcaNNet 0.5034247 0.5226314 0.5273973 0.5299406 0.5439741 0.5582192 0svmRadial 0.5034247 0.5106918 0.5188356 0.5264996 0.5445205 0.5631399 0gbm 0.5017065 0.5393836 0.5462329 0.5473935 0.5571672 0.5890411 0xgbTree 0.5119454 0.5397722 0.5487166 0.5545761 0.5758279 0.5972696 0Kappa Min. 1st Qu. Median Mean 3rd Qu. Max. NA'snnet -0.018275937 0.019774954 0.03377084 0.03687602 0.05092740 0.12767212 0nb -0.054848141 -0.001926439 0.05667858 0.04044339 0.08712856 0.09197804 0rpart -0.019553073 0.018301156 0.03064388 0.02607515 0.03487674 0.06389776 0pcaNNet -0.037948617 0.001225074 0.02214420 0.02116072 0.04925753 0.07875171 0svmRadial -0.029068637 -0.010983769 0.00421168 0.01941413 0.05314826 0.09151688 0gbm -0.015573809 0.060255860 0.07418475 0.07746468 0.09846185 0.16428163 0xgbTree 0.004939796 0.062262588 0.07858693 0.09198844 0.13641100 0.17330592 0 dotplot(results) 预测涨跌首先根据每个模型的预测效果的平均数，计算出每个模型的权重，再根据这个权重计算出最终的涨跌概率。有点类似于7个常委进行民主投票进行最终的决策。12345678910111213141516171819202122232425262728293031323334353637383940pres.pcaNNet &lt;- predict(fit.pcaNNet, data.test)pres.nnet &lt;- predict(fit.nnet, data.test)pres.xgbTree &lt;- predict(fit.xgbTree, data.test)pres.svmRadial &lt;- predict(fit.svmRadial, data.test)pres.gbm &lt;- predict(fit.gbm, data.test)pres.nb &lt;- predict(fit.nb, data.test)pres.rpart &lt;- predict(fit.rpart, data.test)pres.pcaNNet &lt;- as.numeric(as.character(pres.pcaNNet))pres.nnet &lt;- as.numeric(as.character(pres.nnet))pres.xgbTree &lt;- as.numeric(as.character(pres.xgbTree))pres.svmRadial &lt;- as.numeric(as.character(pres.svmRadial))pres.gbm &lt;- as.numeric(as.character(pres.gbm))pres.nb &lt;- as.numeric(as.character(pres.nb))pres.rpart &lt;- as.numeric(as.character(pres.rpart))pres &lt;- data.frame(pres.pcaNNet,pres.nnet,pres.xgbTree,pres.svmRadial,pres.gbm,pres.nb,pres.rpart) prespres.pcaNNet pres.nnet pres.xgbTree pres.svmRadial pres.gbm pres.nb pres.rpart1 1 1 0 1 0 0 1# names(getModelInfo())results.acc &lt;- as.data.frame(summary(results)[3]$statistics$Accuracy)mean.sum &lt;- sum(results.acc$Mean)mean.pcaNNet &lt;- results.acc[\"pcaNNet\",\"Mean\"]mean.nnet &lt;- results.acc[\"nnet\",\"Mean\"]mean.xgbTree &lt;- results.acc[\"xgbTree\",\"Mean\"]mean.svmRadial &lt;- results.acc[\"svmRadial\",\"Mean\"]mean.gbm &lt;- results.acc[\"gbm\",\"Mean\"]mean.nb &lt;- results.acc[\"nb\",\"Mean\"]mean.rpart &lt;- results.acc[\"rpart\",\"Mean\"]# 加入权重计算涨跌概率q.pcaNNet &lt;- (mean.pcaNNet / mean.sum) * nrow(results.acc)q.nnet &lt;- (mean.nnet / mean.sum) * nrow(results.acc)q.xgbTree &lt;- (mean.xgbTree / mean.sum) * nrow(results.acc)q.svmRadial &lt;- (mean.svmRadial / mean.sum) * nrow(results.acc)q.gbm &lt;- (mean.gbm / mean.sum) * nrow(results.acc)q.nb &lt;- (mean.nb / mean.sum) * nrow(results.acc)q.rpart &lt;- (mean.rpart / mean.sum) * nrow(results.acc)# 最终的预测结果p.result &lt;- mean(c(pres.pcaNNet*q.pcaNNet,pres.nnet*q.nnet,pres.xgbTree*q.xgbTree,pres.svmRadial*q.svmRadial,pres.gbm*q.gbm,pres.nb*q.nb,pres.rpart*q.rpart))p.result[1] 0.5642939 最终计算出的今日涨跌概率为0.56，超出了0.5，那么可以认为，今天的分众传媒的预测结果为涨。实际上，分众传媒今天上涨了0.45%，这是巧合吗？有待进一步的探索。如设置止损点、挖掘更有效的特征以及配合蒙特卡洛方法。","tags":[{"name":"量化金融","slug":"量化金融","permalink":"http://weibo.com/youthcolor/tags/量化金融/"},{"name":"股票预测","slug":"股票预测","permalink":"http://weibo.com/youthcolor/tags/股票预测/"},{"name":"机器学习","slug":"机器学习","permalink":"http://weibo.com/youthcolor/tags/机器学习/"},{"name":"特征选取","slug":"特征选取","permalink":"http://weibo.com/youthcolor/tags/特征选取/"}]},{"title":"Hello World","date":"2017-07-11T09:35:53.000Z","path":"2017/07/11/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[]}]