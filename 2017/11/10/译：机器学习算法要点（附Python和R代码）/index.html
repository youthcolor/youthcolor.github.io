<!DOCTYPE html><html lang="zh-CN"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content="youthcolor's blog"><title>译：机器学习算法要点（附Python和R代码） | Geekdreams</title><link rel="stylesheet" type="text/css" href="//fonts.css.network/css?family=Source+Code+Pro"><link rel="stylesheet" type="text/css" href="/css/style.css?v=2.0.1"><link rel="stylesheet" type="text/css" href="/css/highlight.css?v=2.0.1"><link rel="Shortcut Icon" href="/favicon.ico"><link rel="bookmark" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">译：机器学习算法要点（附Python和R代码）</h1><a id="logo" href="/.">Geekdreams</a><p class="description">Nothing is difficult to the man who will try.</p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> 首页</i></a><a href="/archives/"><i class="fa fa-archive"> 归档</i></a><a href="/about/"><i class="fa fa-user"> 关于</i></a></div><div id="search-form"><div id="result-mask" class="hide"></div><label><input id="search-key" type="text" autocomplete="off" placeholder="Arama"></label><div id="result-wrap" class="hide"><div id="search-result"></div></div><div class="hide"><template id="search-tpl"><div class="item"><a href="/{path}" title="{title}"><div class="title">{title}</div><div class="time">{date}</div><div class="tags">{tags}</div></a></div></template></div></div></div><div id="layout" class="layout-g"><div class="layout-l"><div class="content_container"><div class="post"><h1 class="post-title">译：机器学习算法要点（附Python和R代码）</h1><div class="post-meta"><a href="/2017/11/10/译：机器学习算法要点（附Python和R代码）/#comments" class="comment-count"><a id="uyan_count_unit" href="/2017/11/10/译：机器学习算法要点（附Python和R代码）/"></a>留言</a><p><span class="date">Nov 10, 2017</span><span><a href="/categories/Python/" class="category">Python</a></span><span><i id="busuanzi_container_page_pv"><i id="busuanzi_value_page_pv"></i><i>点击</i></i></span></p></div><div class="post-content"><p>“谷歌的自动驾驶汽车和机器人受到了很多媒体的关注，但该公司真正的未来在于机器学习，这是一种让计算机变得更智能、更个性化的技术。”<br>                                                ——埃里克.施密特(谷歌董事会主席)</p>
<p>我们可能生活在人类历史上最具决定性的时期。计算从大型主机到pc再到云计算的时代。但它的定义并不是发生了什么，而是未来几年将会发生什么。</p>
<p>对于像我这样的人来说，这段时期令人兴奋的是工具和技术的民主化，这是随着计算机技术的发展而来的。今天，作为一名数据科学家，我可以用复杂的算法构建数据处理机器，每小时几美元。但是，做到这些并不容易！如果我没有没日没夜的付出。<br><a id="more"></a></p>
<h2 id="谁将从这份指南获益匪浅"><a href="#谁将从这份指南获益匪浅" class="headerlink" title="谁将从这份指南获益匪浅"></a>谁将从这份指南获益匪浅</h2><p>创建这个指南的目的是为了简化世界各地有抱负的数据科学家和机器学习爱好者的学习路径。通过本指南，我将使您能够处理机器学习问题并从经验中获益。我对各种机器学习算法以及R和Python代码进行了高水平的理解，并运行它们。这些应该足够让你的手忙碌了。</p>
<p><img src="http://wx2.sinaimg.cn/large/e621a9abgy1flb2wio330j20dr09eq4k.jpg" alt="image"></p>
<p>我故意跳过了这些技术背后的统计知识，因为您在开始时不需要了解它们。所以，如果你在寻找对这些算法的统计理解，你应该去别处看看。但是，如果你想要装备自己来开始建造机器学习项目，你完全可以信任它。</p>
<h2 id="总的来说，有三种类型的机器学习算法"><a href="#总的来说，有三种类型的机器学习算法" class="headerlink" title="总的来说，有三种类型的机器学习算法"></a>总的来说，有三种类型的机器学习算法</h2><ol>
<li>监督学习</li>
</ol>
<p>它是如何工作的:这个算法由一个变量/因变量(或依赖变量)组成，这个变量是由一个给定的预测集(独立变量)来预测的。使用这组变量，我们生成一个将输入映射到所需输出的函数。培训过程将继续，直到模型达到训练数据所需的精确程度。监督学习的例子:线性回归、决策树、随机森林、最近邻（KNN)、逻辑回归等。</p>
<ol>
<li>无监督学习</li>
</ol>
<p>它是如何工作的:在这个算法中，我们没有任何目标或结果变量来预测/估计。它适用于不同组的聚类，广泛用于细分不同群体，以进行特定的干预。无监督学习的例子:关联算法，k-均值。</p>
<ol>
<li>强化学习</li>
</ol>
<p>它是如何工作的:使用这个算法，机器被训练来做出具体的决定。它的工作原理是这样的:机器被暴露在一个环境中，在这个环境中，它不断地通过试错来训练自己。这台机器从过去的经验中学习，并试图获取最好的知识以做出准确的决策。强化学习的例子:马尔科夫决策过程。</p>
<h2 id="一般机器学习算法列表"><a href="#一般机器学习算法列表" class="headerlink" title="一般机器学习算法列表"></a>一般机器学习算法列表</h2><ul>
<li>线性回归</li>
<li>逻辑回归</li>
<li>决策树</li>
<li>支持向量机</li>
<li>朴素贝叶斯</li>
<li>最近邻（KNN）</li>
<li>K-均值</li>
<li>随机森林</li>
<li>降维算法</li>
<li>梯度下降算法<ol>
<li>GBM</li>
<li>XGBoost</li>
<li>LightGBM</li>
<li>CatBoost</li>
</ol>
</li>
</ul>
<h2 id="1-线性回归"><a href="#1-线性回归" class="headerlink" title="1.线性回归"></a>1.线性回归</h2><p>它用于估计基于连续变量(s)的实际值(房子的成本，电话的数量，总销售额等)。在这里，我们通过拟合最佳直线，建立了独立变量和依赖变量之间的关系。这条最佳拟合线被称为回归线，由线性方程Y=aX+b表示。</p>
<p>理解线性回归最好的方法就是重温童年的经历。比如说，你让一个五年级的孩子通过体重递增方式排列他的同学，而不问他们的具体重量！你认为孩子会做什么?他/她可能会(在视觉上分析)身高和身材，并利用这些可见的参数来排列他们。这是现实生活中的线性回归！这个孩子实际上已经知道了身高与体重之间的关联，这看起来就像上面的等式。</p>
<p>在这个方程中:</p>
<p>Y -因变量</p>
<p>a -斜率</p>
<p>X -独立变量</p>
<p>b -截距</p>
<p>这些系数a和b是基于最小化数据点和回归线之间距离的平方差的。</p>
<p>请看下面的例子。这里我们已经确定了线性方程y=0.2811x+13.9的最佳拟合线。利用这个方程，我们可以计算出一个人的身高。</p>
<p><img src="http://wx4.sinaimg.cn/mw690/e621a9abgy1flb2x6zbpxj20d807vjr9.jpg" alt="image"></p>
<p>线性回归主要有两种类型:一元线性回归和多元线性回归。一元线性回归具有一个独立变量的特征。并且，多元线性回归(如名字所示)具有多个(2个以上)独立变量的特征。在寻找最佳拟合线时，你可以选择一个多项式或曲线回归。这些被称为多项式或曲线线性回归。</p>
<p>Python Code</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</div><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train=input_variables_values_training_datasets</div><div class="line">y_train=target_variables_values_training_datasets</div><div class="line">x_test=input_variables_values_test_datasets</div><div class="line"><span class="comment"># Create linear regression object</span></div><div class="line">linear = linear_model.LinearRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear.fit(x_train, y_train)</div><div class="line">linear.score(x_train, y_train)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, linear.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, linear.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= linear.predict(x_test)</div></pre></td></tr></table></figure>
<p>R Code</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Load Train and Test datasets</span></div><div class="line"><span class="comment">#Identify feature and response variable(s) and values must be numeric and numpy arrays</span></div><div class="line">x_train &lt;- input_variables_values_training_datasets</div><div class="line">y_train &lt;- target_variables_values_training_datasets</div><div class="line">x_test &lt;- input_variables_values_test_datasets</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">linear &lt;- lm(y_train ~ ., data = x)</div><div class="line">summary(linear)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(linear,x_test)</div></pre></td></tr></table></figure>
<h2 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2.逻辑回归"></a>2.逻辑回归</h2><p>别被它的名字搞糊涂了！它是一种分类而不是回归算法。它用于估计离散值(二进制值，如0/1、yes/no、true/false)，基于给定的独立变量(s)。简单地说，它通过将数据匹配到logit函数来预测事件发生的可能性。因此，它也被称为logit回归。因为它预测了概率，它的输出值在0到1之间(如预期的那样)。</p>
<p>让我们再通过一个简单的例子来理解这个问题。</p>
<p>假设你的朋友给了你出了一道题。只有两种结果——要么你解决了，要么你没有。现在想象一下，你被给予了各种各样的智力测验/小测验，试着去理解你擅长的科目。这个研究的结果是这样的-如果你有一个基于十年级的三角测量法，你有70%的可能会解决这个问题。另一方面，如果这是第五次历史问题，得到答案的概率只有30%。这就是逻辑回归所提供的。</p>
<p>在数学计算中，结果的对数概率被建模为预测变量的线性组合。</p>
<figure class="highlight erlang"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">odds= p/ (<span class="number">1</span>-p) = probability <span class="keyword">of</span> event occurrence / probability <span class="keyword">of</span> <span class="keyword">not</span> event occurrence</div><div class="line"><span class="function"><span class="title">ln</span><span class="params">(odds)</span> = <span class="title">ln</span><span class="params">(p/(<span class="number">1</span>-p)</span>)</span></div><div class="line"><span class="title">logit</span><span class="params">(p)</span> = <span class="title">ln</span><span class="params">(p/(<span class="number">1</span>-p)</span>) = <span class="title">b0</span>+<span class="title">b1X1</span>+<span class="title">b2X2</span>+<span class="title">b3X3</span>....+<span class="title">bkXk</span></div></pre></td></tr></table></figure>
<p>上面，p是我们感兴趣的特征的概率。它选择参数来最大化观察样本值的可能性，而不是最小化平方误差的总和(就像普通的回归一样)。</p>
<p>现在，你可能会问，为什么要写一个日志呢?为了简单起见，我们假设这是复制一个阶跃函数的最好的数学方法之一。我可以详细介绍，但这将超出本文的目的。</p>
<p><img src="http://wx1.sinaimg.cn/mw690/e621a9abgy1flbgv1oc3ij20et0bawer.jpg" alt="image"></p>
<p>Python Code</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create logistic regression object</span></div><div class="line">model = LogisticRegression()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Equation coefficient and Intercept</span></div><div class="line">print(<span class="string">'Coefficient: \n'</span>, model.coef_)</div><div class="line">print(<span class="string">'Intercept: \n'</span>, model.intercept_)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<p>R Code</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">logistic &lt;- glm(y_train ~ ., data = x,family=<span class="string">'binomial'</span>)</div><div class="line">summary(logistic)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= predict(logistic,x_test)</div></pre></td></tr></table></figure>
<p>另外，如果想提高改进模型，有许多不同的方法可以尝试:</p>
<ul>
<li>包括相互作用项</li>
<li>删减特征</li>
<li>正则化技术</li>
<li>使用非线性模型<h2 id="3-决策树"><a href="#3-决策树" class="headerlink" title="3.决策树"></a>3.决策树</h2>这是我最喜欢的算法之一，我经常使用它。它是一种被监督的学习算法，主要用于分类问题。令人惊讶的是，它适用于分类和连续相关变量。使用这个算法，我们可以把种群分成两个或更多的同类集合。它基于最重要的属性/独立变量，以使其尽可能地成为不同的组。要了解更多细节，您可以阅读有关决策树的建模实现过程。</li>
</ul>
<p><img src="http://wx1.sinaimg.cn/mw690/e621a9abgy1flbo75c9onj20g30bumyh.jpg" alt="image"></p>
<p>在上面的图片中，你可以看到，根据不同的属性，人们被划分为四个不同的组，以确定“他们是否会出去玩”。为了将人口分成不同的不同的群体，它使用了各种各样的技术，比如基尼系数、信息增益、卡方、熵。</p>
<p>要理解决策树的工作原理，最好的方法是玩Jezzball——这是微软的一款经典游戏(下图)。基本上，你有一间有移动墙的房间，你需要创建墙壁，这样没有球的最大区域就会被清理。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/e621a9abgy1flboei37ijj208204uwei.jpg" alt="image"></p>
<p>所以，每次你用一面墙把房间分成两部分，你就会在同一个房间里创造两种不同的人群。决策树以非常相似的方式工作，将一个种群划分为不同的组。</p>
<p>Python Code</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="comment">#Import other necessary libraries like pandas, numpy...</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> tree</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create tree object </span></div><div class="line">model = tree.DecisionTreeClassifier(criterion=<span class="string">'gini'</span>) <span class="comment"># for classification, here you can change the algorithm as gini or entropy (information gain) by default it is gini  </span></div><div class="line"><span class="comment"># model = tree.DecisionTreeRegressor() for regression</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure>
<p>R Code</p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(rpart)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># grow tree </span></div><div class="line">fit &lt;- rpart(y_train ~ ., data = x,method=<span class="string">"class"</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure>
<h2 id="4-SVM-支持向量机"><a href="#4-SVM-支持向量机" class="headerlink" title="4. SVM (支持向量机)"></a>4. SVM (支持向量机)</h2><p>这是一种分类方法。在这个算法中，我们将每个数据项绘制成n维空间中的一个点(其中n是您拥有的特性的数量)，每个特性的值都是一个特定坐标的值。</p>
<p>例如，如果我们只有两个特征，比如身高和头发的长度，我们首先会在二维空间中绘制这两个变量，其中每个点都有两个坐标(这些坐标被称为支持向量)。</p>
<p><img src="http://wx4.sinaimg.cn/mw690/e621a9abgy1flboor8ne8j20nm0fz76m.jpg" alt="image"></p>
<p>现在，我们将找到一些在两种不同类别的数据之间分割的线。这条线是这样的，这两个簇两个最近点之间是最远的。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/e621a9abgy1flboowe1szj20nm0g2gpo.jpg" alt="image"></p>
<p>在上面的示例中，将数据划分为两个不同的分组的那条线是黑线，因为这两个最接近的点离黑线最远。这条线是我们的分类器。然后，根据测试数据位置在这条线的哪一边，对新数据进行分类。</p>
<p>如果把这个算法看作是在n维空间中玩JezzBall。游戏的微调是:</p>
<ul>
<li><p>你可以在任何角度画线/平面(而不是像传统游戏那样水平或垂直)。</p>
</li>
<li><p>游戏的目的是在不同的房间中分离不同颜色的球。</p>
</li>
<li><p>球是固定不动的。</p>
</li>
</ul>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object </span></div><div class="line">model = svm.svc() <span class="comment"># there is various option associated with it, this is simple for classification. You can refer link, for mo# re detail.</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line">model.score(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-svm(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h2 id="5-朴素贝叶斯"><a href="#5-朴素贝叶斯" class="headerlink" title="5. 朴素贝叶斯"></a>5. 朴素贝叶斯</h2><p>这是一种基于贝叶斯定理的分类技术，前提基于预测者之间的独立性假设。简单地讲，朴素贝叶斯的分类器假设一个类的特定特性与其他任何特性的存在无关。例如，如果水果是红色的，圆形的，直径大约3英寸，就可以认为它是苹果。即使这些特征相互依赖，或者其他特征的存在，一个朴素贝叶斯分类器也会考虑所有这些特性各自独立地为这个水果是苹果的概率可能性做出的贡献。</p>
<p>简单的贝叶斯模型很容易构建，对于非常大的数据集尤其有用。除了简洁性，朴素贝叶斯也被认为比高度复杂的分类方法表现出色。</p>
<p>贝叶斯定理提供了一种由P(c)、P(x)和P(x|c)中计算出概率P(c|x)的方法。请看下面的等式:</p>
<p><img src="http://wx1.sinaimg.cn/mw690/e621a9abgy1flc14kmymtj20ct07c744.jpg" alt="image"></p>
<p>这里，</p>
<ul>
<li>P(c|x)是分类(目标)的后验概率(属性)。</li>
<li>P(c)是分类的先验概率。</li>
<li>P(x|c)是预测给定分类的概率可能性。</li>
<li>P(x)是预测的先验概率。</li>
</ul>
<p>例子:让我们用一个例子来理解它。下面我有一组天气预报和相应的目标变量“Play”。现在，我们需要根据天气情况对玩家是否会出去玩进行分类。让我们按照下面的步骤来执行它。</p>
<p>步骤1:将数据集转换为频率表</p>
<p>第2步:通过寻找可能性的概率来创建可能性表，例如:阴天概率=0.29，而玩的概率是0.64。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/e621a9abgy1flc1ezajkzj20nm08m0us.jpg" alt="image"></p>
<p>第三步:用简单的贝叶斯方程来计算每个类的后验概率。具有最高后验概率的类是预测的结果。</p>
<p>问题:如果天气晴朗，球员会出去玩，这句话是正确的吗?</p>
<p>我们可以用上面讨论的方法来解它，所以 P(Yes | Sunny) = P( Sunny | Yes) * P(Yes) / P (Sunny)</p>
<p>这里P (Sunny |Yes) = 3/9 = 0.33, P(Sunny) = 5/14 = 0.36, P( Yes)= 9/14 = 0.64</p>
<p>现在，P (Yes | Sunny) = 0.33 * 0.64 / 0.36 = 0.60，这有更高的概率。</p>
<p>Naive Bayes使用了一种类似的方法来预测不同类型的不同属性的可能性。该算法主要用于文本分类和有多个类的问题。</p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create SVM classification object model = GaussianNB() # there is other distribution for multinomial classes like Bernoulli Naive Bayes</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(e1071)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-naiveBayes(y_train ~ ., data = x)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h2 id="6-KNN-K-最近邻"><a href="#6-KNN-K-最近邻" class="headerlink" title="6. KNN (K-最近邻)"></a>6. KNN (K-最近邻)</h2><p>它可以用于分类和回归问题。然而，它更广泛地应用于工业中的分类问题。K最近邻是一个简单的算法，它可以存储所有可用的情况，并通过它的K个邻居的多数投票来对新情况进行分类。被分配到一个类的实例在它的K最近的邻居中也是最常见的，它是由一个距离函数测量的。</p>
<p>这些距离函数可以是欧氏、曼哈顿、闵可夫斯基和海明距离。第3个函数用于连续函数，第4个(海明)用于分类变量。如果K=1,那么，这个实例被简单地分配给它最近的邻居。很多时候，在使用KNN建模时，对于K的选择往往是一个挑战。</p>
<p><img src="http://wx3.sinaimg.cn/mw690/e621a9abgy1flc1xehlh9j20jv08p742.jpg" alt="image"></p>
<p>KNN很容易被映射到我们的真实生活中。如果你想了解一个你没有任何信息的人，你可能会想了解他的亲密朋友和他的圈子，并由此获得他/她的信息！</p>
<p>在选择KNN之前要考虑的事情:</p>
<ul>
<li>KNN在计算上代价是昂贵的</li>
<li>变量应该被规范化否则更高范围的变量会导致偏差</li>
<li>在使用KNN之前进行预处理十分重要，比如离群值，消除噪音</li>
</ul>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">KNeighborsClassifier(n_neighbors=<span class="number">6</span>) <span class="comment"># default value for n_neighbors is 5</span></div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicte</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(knn)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;-knn(y_train ~ ., data = x,k=<span class="number">5</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h2 id="7-K-均值"><a href="#7-K-均值" class="headerlink" title="7. K-均值"></a>7. K-均值</h2><p>它是一种不受监督的算法，解决了聚类的问题。它的过程遵循一种简单而容易的方法，通过一定数量的簇(假设k个簇)对给定的数据集进行分类。簇中的数据点是同类的，异构的，对等的。</p>
<p>还记得从墨水污渍中找出形状吗?k均值与其有点类似。你看一下这个形状，然后扩散开来，看看有多少不同的簇出现！</p>
<p><img src="http://wx1.sinaimg.cn/mw690/e621a9abgy1flc2c9mzbbj207w08cq2y.jpg" alt="image"></p>
<p>k-均值聚簇方法:</p>
<ul>
<li>k-均值为每个中心点挑选出K个点。</li>
<li>每个数据点形成一个具有最接近的中心点的簇，即k个簇。</li>
<li>根据现有的簇成员查找每个簇的中心点。这里有新的中心点。</li>
<li>当我们有新的中心点时，重复第2步和第3步。从新的中心找到每个数据点的最近距离，并与新的k个簇联系起来。重复这个过程直到收敛发生，即中心点不会改变。</li>
</ul>
<p>如何确定K的值:</p>
<p>在k-均值中，我们有这样的簇，每个簇都有自己的中心点。中心点和一个簇中的数据点之间的平方和构成了该集群的平方值的总和。此外，当添加了所有簇的平方总和时，它就变成了簇解决方案的平方总和。</p>
<p>我们知道，随着集群的数量增加，这个值一直在减少但如果你画出结果，你会发现，距离的平方和k的值会急剧下降，之后会更慢。我们可以以此找到最优簇个数K。</p>
<p><img src="http://wx2.sinaimg.cn/mw690/e621a9abgy1flc2o0yhv0j20nm0bxab6.jpg" alt="image"></p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</div><div class="line"><span class="comment">#Assumed you have, X (attributes) for training data set and x_test(attributes) of test_dataset</span></div><div class="line"><span class="comment"># Create KNeighbors classifier object model </span></div><div class="line">k_means = KMeans(n_clusters=<span class="number">3</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(cluster)</div><div class="line">fit &lt;- kmeans(X, <span class="number">3</span>) <span class="comment"># 3 cluster solution</span></div></pre></td></tr></table></figure></p>
<h2 id="8-随机森林"><a href="#8-随机森林" class="headerlink" title="8. 随机森林"></a>8. 随机森林</h2><p>随机森林是一组决策树的标术语。在随机森林中，我们收集了决策树(也就是所谓的“森林”)。为了对基于属性的新对象进行分类，每棵树都给出一个分类，我们说这个树给类进行“投票”。森林选择了拥有最多选票的分类。</p>
<p>每棵树都是种下并生长的:</p>
<ul>
<li>如果训练集的实例数是N，则随机可替换抽取这N个实例的样本。这个样本将作为树生长的训练集。</li>
<li>如果有M个输入变量，那么在每个节点上都指定了一个m值的M变量，m变量是在M中中随机选择的，在这些m中最好的分割是去用来分割节点的。在森林生长过程中，m的值保持不变。</li>
<li>每棵树都在尽可能大的范围内生长。没有修剪。</li>
</ul>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Random Forest object</span></div><div class="line">model= RandomForestClassifier()</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(randomForest)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fit &lt;- randomForest(Species ~ ., x,ntree=<span class="number">500</span>)</div><div class="line">summary(fit)</div><div class="line"><span class="comment">#Predict Output </span></div><div class="line">predicted= predict(fit,x_test)</div></pre></td></tr></table></figure></p>
<h2 id="9-降维算法"><a href="#9-降维算法" class="headerlink" title="9. 降维算法"></a>9. 降维算法</h2><p>在过去的4-5年中，在每个可能的阶段，数据收集都呈指数级增长。企业/政府机构/研究机构不仅提供了新的信息来源，而且还在详细地收集数据。</p>
<p>例如:电子商务公司正在获取更多关于客户的细节信息，比如他们的人口统计数据、网络爬行历史、他们喜欢或不喜欢的东西、购买历史记录、反馈信息等等，这些都比你最近的杂货店老板更能给他们个性化的关注。</p>
<p>作为一名数据科学家，我们提供的数据也包含许多特性，这对于构建良好的健壮模型来说是很好的，但也有一个挑战。你如何从1000或2000变量中识别出高度显著的变量(s)?在这种情况下，降维算法可以帮助我们，与决策树、随机森林、主成分分析、因子分析等其他算法相结合，以关联矩阵、缺失值比例等为基础进行识别。</p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> decomposition</div><div class="line"><span class="comment">#Assumed you have training and test data set as train and test</span></div><div class="line"><span class="comment"># Create PCA obeject pca= decomposition.PCA(n_components=k) #default value of k =min(n_sample, n_features)</span></div><div class="line"><span class="comment"># For Factor analysis</span></div><div class="line"><span class="comment">#fa= decomposition.FactorAnalysis()</span></div><div class="line"><span class="comment"># Reduced the dimension of training dataset using PCA</span></div><div class="line">train_reduced = pca.fit_transform(train)</div><div class="line"><span class="comment">#Reduced the dimension of test dataset</span></div><div class="line">test_reduced = pca.transform(test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(stats)</div><div class="line">pca &lt;- princomp(train, cor = <span class="literal">TRUE</span>)</div><div class="line">train_reduced  &lt;- predict(pca,train)</div><div class="line">test_reduced  &lt;- predict(pca,test)</div></pre></td></tr></table></figure></p>
<h2 id="10-梯度下降算法"><a href="#10-梯度下降算法" class="headerlink" title="10. 梯度下降算法"></a>10. 梯度下降算法</h2><blockquote>
<p> 10.1. GBM</p>
</blockquote>
<p>GBM是一种增强算法，当我们以高预测能力处理大量数据时，它是一种增强算法。增强实际上是一套学习算法的集合，它结合了几个基本估计量的预测，以提高对单一估计量的鲁棒性。它将多个弱或平均的预测因子结合构建到一个强的预测器中。这些增强算法在诸如Kaggle、AV Hackathon、CrowdAnalytix等数据科学竞赛中都很有效。</p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#Import Library</span></div><div class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingClassifier</div><div class="line"><span class="comment">#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset</span></div><div class="line"><span class="comment"># Create Gradient Boosting Classifier object</span></div><div class="line">model= GradientBoostingClassifier(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">1.0</span>, max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>)</div><div class="line"><span class="comment"># Train the model using the training sets and check score</span></div><div class="line">model.fit(X, y)</div><div class="line"><span class="comment">#Predict Output</span></div><div class="line">predicted= model.predict(x_test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(caret)</div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"><span class="comment"># Fitting model</span></div><div class="line">fitControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = <span class="number">4</span>, repeats = <span class="number">4</span>)</div><div class="line">fit &lt;- train(y ~ ., data = x, method = <span class="string">"gbm"</span>, trControl = fitControl,verbose = <span class="literal">FALSE</span>)</div><div class="line">predicted= predict(fit,x_test,type= <span class="string">"prob"</span>)[,<span class="number">2</span>]</div></pre></td></tr></table></figure></p>
<blockquote>
<p>10.2. XGBoost</p>
</blockquote>
<p>另一种经典的梯度下降算法，它被认为是在某些Kaggle比赛中获胜与失败之间的决定性选择。</p>
<p>XGBoost具有非常高的预测能力，这使它在预测模型中成为最佳的选择，因为它既具有线性模型又有树学习算法，使得算法比现有的梯度下降技术快了10倍。</p>
<p>它可以支持的功能，包括回归、分类和排名。</p>
<p>XGBoost最有趣的一点是它也被称为规则化的增强技术。这有助于减少过度拟合的建模，并对诸如Scala、Java、R、Python、Julia和C++等一系列语言提供了大量支持。</p>
<p>它也支持分布式大规模训练，包括GCE、AWS、Azure和Yarn 集群。XGBoost也可以与Spark、Flink和其他云数据系统集成在一起，在每次迭代的过程中都建立了交叉验证。</p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> xgboost <span class="keyword">import</span> XGBClassifier</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</div><div class="line">X = dataset[:,<span class="number">0</span>:<span class="number">10</span>]</div><div class="line">Y = dataset[:,<span class="number">10</span>:]</div><div class="line">seed = <span class="number">1</span></div><div class="line"></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=<span class="number">0.33</span>, random_state=seed)</div><div class="line"></div><div class="line">model = XGBClassifier()</div><div class="line"></div><div class="line">model.fit(X_train, y_train)</div><div class="line"></div><div class="line"><span class="comment">#Make predictions for test data</span></div><div class="line">y_pred = model.predict(X_test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">require</span>(caret)</div><div class="line"></div><div class="line">x &lt;- cbind(x_train,y_train)</div><div class="line"></div><div class="line"><span class="comment"># Fitting model</span></div><div class="line"></div><div class="line">TrainControl &lt;- trainControl( method = <span class="string">"repeatedcv"</span>, number = <span class="number">10</span>, repeats = <span class="number">4</span>)</div><div class="line"></div><div class="line">model&lt;- train(y ~ ., data = x, method = <span class="string">"xgbLinear"</span>, trControl = TrainControl,verbose = <span class="literal">FALSE</span>)</div><div class="line"></div><div class="line">OR </div><div class="line"></div><div class="line">model&lt;- train(y ~ ., data = x, method = <span class="string">"xgbTree"</span>, trControl = TrainControl,verbose = <span class="literal">FALSE</span>)</div><div class="line"></div><div class="line">predicted &lt;- predict(model, x_test)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>10.3. LightGBM</p>
</blockquote>
<p>LightGBM是一个使用基于树的学习算法的梯度增强框架。它的设计是分布式的和高效的，有以下优点:</p>
<ul>
<li>更快的训练速度和更高的效率</li>
<li>降低内存使用</li>
<li>更好的精度</li>
<li>并行和GPU学习支持</li>
<li>能够处理大规模数据</li>
</ul>
<p>该框架是一种基于决策树算法的快速和高性能梯度增强，用于排序、分类和许多其他机器学习任务。它是在微软的分布式机器学习工具包项目下开发的。</p>
<p>由于LightGBM是基于决策树算法的，它将树叶和最适合的树分割开，而其他的提升算法将树的深度和水平都分割开，而不是叶节点。因此，当LightGBM在相同的叶子生长时，叶节点的智慧算法能够比水平的算法减少更多的损失，从而产生更好的精度，而这在现有的增强算法中几乎是不可能实现的。</p>
<p>而且，它的速度惊人的快，因此用了“光”这个词。</p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">data = np.random.rand(<span class="number">500</span>, <span class="number">10</span>) <span class="comment"># 500 entities, each contains 10 features</span></div><div class="line">label = np.random.randint(<span class="number">2</span>, size=<span class="number">500</span>) <span class="comment"># binary target</span></div><div class="line"></div><div class="line">train_data = lgb.Dataset(data, label=label)</div><div class="line">test_data = train_data.create_valid(<span class="string">'test.svm'</span>)</div><div class="line"></div><div class="line">param = &#123;<span class="string">'num_leaves'</span>:<span class="number">31</span>, <span class="string">'num_trees'</span>:<span class="number">100</span>, <span class="string">'objective'</span>:<span class="string">'binary'</span>&#125;</div><div class="line">param[<span class="string">'metric'</span>] = <span class="string">'auc'</span></div><div class="line"></div><div class="line">num_round = <span class="number">10</span></div><div class="line">bst = lgb.train(param, train_data, num_round, valid_sets=[test_data])</div><div class="line"></div><div class="line">bst.save_model(<span class="string">'model.txt'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 7 entities, each contains 10 features</span></div><div class="line">data = np.random.rand(<span class="number">7</span>, <span class="number">10</span>)</div><div class="line">ypred = bst.predict(data)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(RLightGBM)</div><div class="line">data(example.binary)</div><div class="line"><span class="comment">#Parameters</span></div><div class="line"></div><div class="line">num_iterations &lt;- <span class="number">100</span></div><div class="line">config &lt;- list(objective = <span class="string">"binary"</span>,  metric=<span class="string">"binary_logloss,auc"</span>, learning_rate = <span class="number">0.1</span>, num_leaves = <span class="number">63</span>, tree_learner = <span class="string">"serial"</span>, feature_fraction = <span class="number">0.8</span>, bagging_freq = <span class="number">5</span>, bagging_fraction = <span class="number">0.8</span>, min_data_in_leaf = <span class="number">50</span>, min_sum_hessian_in_leaf = <span class="number">5.0</span>)</div><div class="line"></div><div class="line"><span class="comment">#Create data handle and booster</span></div><div class="line">handle.data &lt;- lgbm.data.create(x)</div><div class="line"></div><div class="line">lgbm.data.setField(handle.data, <span class="string">"label"</span>, y)</div><div class="line"></div><div class="line">handle.booster &lt;- lgbm.booster.create(handle.data, lapply(config, as.character))</div><div class="line"></div><div class="line"><span class="comment">#Train for num_iterations iterations and eval every 5 steps</span></div><div class="line"></div><div class="line">lgbm.booster.train(handle.booster, num_iterations, <span class="number">5</span>)</div><div class="line"></div><div class="line"><span class="comment">#Predict</span></div><div class="line">pred &lt;- lgbm.booster.predict(handle.booster, x.test)</div><div class="line"></div><div class="line"><span class="comment">#Test accuracy</span></div><div class="line">sum(y.test == (y.pred &gt; <span class="number">0.5</span>)) / length(y.test)</div><div class="line"></div><div class="line"><span class="comment">#Save model (can be loaded again via lgbm.booster.load(filename))</span></div><div class="line">lgbm.booster.save(handle.booster, filename = <span class="string">"/tmp/model.txt"</span>)</div></pre></td></tr></table></figure></p>
<p>如果您熟悉R中的Caret包，下面是实现LightGBM的另一种方式。<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">require</span>(caret)</div><div class="line"><span class="keyword">require</span>(RLightGBM)</div><div class="line">data(iris)</div><div class="line"></div><div class="line">model &lt;-caretModel.LGBM()</div><div class="line"></div><div class="line">fit &lt;- train(Species ~ ., data = iris, method=model, verbosity = <span class="number">0</span>)</div><div class="line">print(fit)</div><div class="line">y.pred &lt;- predict(fit, iris[,<span class="number">1</span>:<span class="number">4</span>])</div><div class="line"></div><div class="line"><span class="keyword">library</span>(Matrix)</div><div class="line">model.sparse &lt;- caretModel.LGBM.sparse()</div><div class="line"></div><div class="line"><span class="comment">#Generate a sparse matrix</span></div><div class="line">mat &lt;- Matrix(as.matrix(iris[,<span class="number">1</span>:<span class="number">4</span>]), sparse = <span class="literal">T</span>)</div><div class="line">fit &lt;- train(data.frame(idx = <span class="number">1</span>:nrow(iris)), iris$Species, method = model.sparse, matrix = mat, verbosity = <span class="number">0</span>)</div><div class="line">print(fit)</div></pre></td></tr></table></figure></p>
<blockquote>
<p>10.4. Catboost</p>
</blockquote>
<p>CatBoost是最近开源的一款来自Yandex的机器学习算法。它可以很容易地与谷歌的TensorFlow和苹果的Core ML等深度学习框架集成。</p>
<p>关于CatBoost的最好的地方是它不需要像其他ML模型那样进行大量的数据进行训练，并且可以处理各种数据格式，也不会影响它的鲁棒性。</p>
<p>在执行算法之前，请确保您已对缺失的数据进行了处理。</p>
<p>Catboost可以自动处理分类变量，而不显示类型转换错误，这有助于您集中精力优化您的模型，而不是处理一些琐碎的错误。</p>
<p>Python Code<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressor</div><div class="line"></div><div class="line"><span class="comment">#Read training and testing files</span></div><div class="line">train = pd.read_csv(<span class="string">"train.csv"</span>)</div><div class="line">test = pd.read_csv(<span class="string">"test.csv"</span>)</div><div class="line"></div><div class="line"><span class="comment">#Imputing missing values for both train and test</span></div><div class="line">train.fillna(<span class="number">-999</span>, inplace=<span class="keyword">True</span>)</div><div class="line">test.fillna(<span class="number">-999</span>,inplace=<span class="keyword">True</span>)</div><div class="line"></div><div class="line"><span class="comment">#Creating a training set for modeling and validation set to check model performance</span></div><div class="line">X = train.drop([<span class="string">'Item_Outlet_Sales'</span>], axis=<span class="number">1</span>)</div><div class="line">y = train.Item_Outlet_Sales</div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line">X_train, X_validation, y_train, y_validation = train_test_split(X, y, train_size=<span class="number">0.7</span>, random_state=<span class="number">1234</span>)</div><div class="line">categorical_features_indices = np.where(X.dtypes != np.float)[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment">#importing library and building model</span></div><div class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressormodel=CatBoostRegressor(iterations=<span class="number">50</span>, depth=<span class="number">3</span>, learning_rate=<span class="number">0.1</span>, loss_function=<span class="string">'RMSE'</span>)</div><div class="line"></div><div class="line">model.fit(X_train, y_train,cat_features=categorical_features_indices,eval_set=(X_validation, y_validation),plot=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">submission = pd.DataFrame()</div><div class="line"></div><div class="line">submission[<span class="string">'Item_Identifier'</span>] = test[<span class="string">'Item_Identifier'</span>]</div><div class="line">submission[<span class="string">'Outlet_Identifier'</span>] = test[<span class="string">'Outlet_Identifier'</span>]</div><div class="line">submission[<span class="string">'Item_Outlet_Sales'</span>] = model.predict(test)</div></pre></td></tr></table></figure></p>
<p>R Code<br><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">set.seed(<span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="keyword">require</span>(titanic)</div><div class="line"></div><div class="line"><span class="keyword">require</span>(caret)</div><div class="line"></div><div class="line"><span class="keyword">require</span>(catboost)</div><div class="line"></div><div class="line">tt &lt;- titanic::titanic_train[complete.cases(titanic::titanic_train),]</div><div class="line"></div><div class="line">data &lt;- as.data.frame(as.matrix(tt), stringsAsFactors = <span class="literal">TRUE</span>)</div><div class="line"></div><div class="line">drop_columns = c(<span class="string">"PassengerId"</span>, <span class="string">"Survived"</span>, <span class="string">"Name"</span>, <span class="string">"Ticket"</span>, <span class="string">"Cabin"</span>)</div><div class="line"></div><div class="line">x &lt;- data[,!(names(data) %<span class="keyword">in</span>% drop_columns)]y &lt;- data[,c(<span class="string">"Survived"</span>)]</div><div class="line"></div><div class="line">fit_control &lt;- trainControl(method = <span class="string">"cv"</span>, number = <span class="number">4</span>,classProbs = <span class="literal">TRUE</span>)</div><div class="line"></div><div class="line">grid &lt;- expand.grid(depth = c(<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>),learning_rate = <span class="number">0.1</span>,iterations = <span class="number">100</span>, l2_leaf_reg = <span class="number">1e-3</span>,            rsm = <span class="number">0.95</span>, border_count = <span class="number">64</span>)</div><div class="line"></div><div class="line">report &lt;- train(x, as.factor(make.names(y)),method = catboost.caret,verbose = <span class="literal">TRUE</span>, preProc = <span class="literal">NULL</span>,tuneGrid = grid, trControl = fit_control)</div><div class="line"></div><div class="line">print(report)</div><div class="line"></div><div class="line">importance &lt;- varImp(report, scale = <span class="literal">FALSE</span>)</div><div class="line"></div><div class="line">print(importance)</div></pre></td></tr></table></figure></p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>现在，我敢肯定，你会有一种大体如何使用机器学习算法的想法。我写这篇文章并提供R和Python代码的唯一目的就是让你马上开始。如果你想要精通机器学习，那就马上开始吧。解决问题，不断加强对过程的物理理解，应用这些代码，并发现乐趣！</p>
<p>译自：<a href="https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/" target="_blank" rel="external">https://www.analyticsvidhya.com/blog/2017/09/common-machine-learning-algorithms/</a></p>
</div><div class="tags"><a href="/tags/线性回归/">线性回归</a><a href="/tags/机器学习/">机器学习</a><a href="/tags/逻辑回归/">逻辑回归</a><a href="/tags/决策树/">决策树</a><a href="/tags/支持向量机/">支持向量机</a><a href="/tags/朴素贝叶斯/">朴素贝叶斯</a><a href="/tags/最近邻（KNN）/">最近邻（KNN）</a><a href="/tags/K-均值/">K-均值</a><a href="/tags/随机森林/">随机森林</a><a href="/tags/降维算法/">降维算法</a><a href="/tags/梯度下降算法/">梯度下降算法</a><a href="/tags/GBM/">GBM</a><a href="/tags/XGBoost/">XGBoost</a><a href="/tags/LightGBM/">LightGBM</a><a href="/tags/CatBoost/">CatBoost</a></div><div class="post-share"><div class="jiathis_style"><span class="jiathis_txt">分享到：</span><a class="jiathis_button_tsina"></a><a class="jiathis_button_qzone"></a><a class="jiathis_button_weixin"></a><a class="jiathis_button_fb"></a><a class="jiathis_button_linkedin"></a><a class="jiathis_button_twitter"></a><a class="jiathis_button_ydnote"></a><a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis"></a><a class="jiathis_counter_style"></a></div></div><div class="post-nav"><a href="/2017/08/23/侦测欺诈交易/" class="next">侦测欺诈交易</a></div><div id="comments"><div id="uyan_frame"></div></div></div></div></div><div class="layout-r"><div id="sidebar"><div class="search-pla"></div><div id="toc" class="widget"><div class="widget-title"><i class="fa fa-fei">文章目录</i></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#谁将从这份指南获益匪浅"><span class="toc-text">谁将从这份指南获益匪浅</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总的来说，有三种类型的机器学习算法"><span class="toc-text">总的来说，有三种类型的机器学习算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#一般机器学习算法列表"><span class="toc-text">一般机器学习算法列表</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-线性回归"><span class="toc-text">1.线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-逻辑回归"><span class="toc-text">2.逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-决策树"><span class="toc-text">3.决策树</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-SVM-支持向量机"><span class="toc-text">4. SVM (支持向量机)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-朴素贝叶斯"><span class="toc-text">5. 朴素贝叶斯</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-KNN-K-最近邻"><span class="toc-text">6. KNN (K-最近邻)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-K-均值"><span class="toc-text">7. K-均值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-随机森林"><span class="toc-text">8. 随机森林</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#9-降维算法"><span class="toc-text">9. 降维算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#10-梯度下降算法"><span class="toc-text">10. 梯度下降算法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#写在最后"><span class="toc-text">写在最后</span></a></li></ol></div><div class="widget"><div class="widget-title"><i class="fa fa-xie"> 最新文章</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2017/11/10/译：机器学习算法要点（附Python和R代码）/">译：机器学习算法要点（附Python和R代码）</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/23/侦测欺诈交易/">侦测欺诈交易</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/18/购物篮关联规则分析/">购物篮关联规则分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/08/01/R中如何对量化策略进行回测/">R中如何对量化策略进行回测</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/11/R中的大数据分析/">R中的大数据分析</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/11/R机器学习之股票预测初探/">R机器学习之股票预测初探</a></li><li class="post-list-item"><a class="post-list-link" href="/2017/07/11/hello-world/">Hello World</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-gui"> 分类</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Python/">Python</a><span class="category-list-count">1</span></li><li class="category-list-item"><a class="category-list-link" href="/categories/R/">R</a><span class="category-list-count">5</span></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-biao"> 标签</i></div><div class="tagcloud"><a href="/tags/离群值侦测排序/" style="font-size: 15px;">离群值侦测排序</a> <a href="/tags/量化金融/" style="font-size: 15px;">量化金融</a> <a href="/tags/大数据/" style="font-size: 15px;">大数据</a> <a href="/tags/聚类/" style="font-size: 15px;">聚类</a> <a href="/tags/线性回归/" style="font-size: 15px;">线性回归</a> <a href="/tags/股票预测/" style="font-size: 15px;">股票预测</a> <a href="/tags/机器学习/" style="font-size: 15px;">机器学习</a> <a href="/tags/特征选取/" style="font-size: 15px;">特征选取</a> <a href="/tags/欺诈侦测/" style="font-size: 15px;">欺诈侦测</a> <a href="/tags/半监督分类/" style="font-size: 15px;">半监督分类</a> <a href="/tags/类失衡/" style="font-size: 15px;">类失衡</a> <a href="/tags/贝叶斯分类器/" style="font-size: 15px;">贝叶斯分类器</a> <a href="/tags/AdaBoost分类器/" style="font-size: 15px;">AdaBoost分类器</a> <a href="/tags/决策精确度/" style="font-size: 15px;">决策精确度</a> <a href="/tags/回溯精确度/" style="font-size: 15px;">回溯精确度</a> <a href="/tags/交叉验证/" style="font-size: 15px;">交叉验证</a> <a href="/tags/回测/" style="font-size: 15px;">回测</a> <a href="/tags/逻辑回归/" style="font-size: 15px;">逻辑回归</a> <a href="/tags/决策树/" style="font-size: 15px;">决策树</a> <a href="/tags/支持向量机/" style="font-size: 15px;">支持向量机</a> <a href="/tags/朴素贝叶斯/" style="font-size: 15px;">朴素贝叶斯</a> <a href="/tags/最近邻（KNN）/" style="font-size: 15px;">最近邻（KNN）</a> <a href="/tags/K-均值/" style="font-size: 15px;">K-均值</a> <a href="/tags/随机森林/" style="font-size: 15px;">随机森林</a> <a href="/tags/降维算法/" style="font-size: 15px;">降维算法</a> <a href="/tags/梯度下降算法/" style="font-size: 15px;">梯度下降算法</a> <a href="/tags/GBM/" style="font-size: 15px;">GBM</a> <a href="/tags/XGBoost/" style="font-size: 15px;">XGBoost</a> <a href="/tags/LightGBM/" style="font-size: 15px;">LightGBM</a> <a href="/tags/CatBoost/" style="font-size: 15px;">CatBoost</a> <a href="/tags/购物篮/" style="font-size: 15px;">购物篮</a> <a href="/tags/关联规则/" style="font-size: 15px;">关联规则</a> <a href="/tags/推荐算法/" style="font-size: 15px;">推荐算法</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-archive"> 归档</i></div><ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">十一月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/08/">八月 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/07/">七月 2017</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-you"> 友情链接</i></div><ul></ul><a href="http://weibo.com/youthcolor" title="Geekdreams" target="_blank">Geekdreams</a></div></div></div></div><a id="totop" href="#top"></a><div id="footer"><div class="footer-info"><p>本站总访问量：<i id="busuanzi_container_site_pv"><i id="busuanzi_value_site_pv"></i></i>次</p><p><span> Copyright &copy;<a href="/." rel="nofollow">youthcolor.</a></span></p></div></div></div><script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><script type="text/javascript" src="/js/search.json.js?v=2.0.1"></script><script type="text/javascript" src="/js/toctotop.js?v=2.0.1" async></script><script>var jiathis_config={
    data_track_clickback:true,
    summary:"",
    shortUrl:true,
    hideMore:false
}</script><script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script><script src="http://v2.uyan.cc/code/uyan.js?uid=2136627"></script></body></html>